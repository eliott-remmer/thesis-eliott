{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric, set_caching_enabled\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    PrinterCallback,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "from transformers.trainer_utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meliottr\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "2022-04-27 20:04:01.131409: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220427_200359-3e8cbx8t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/workspace-notebooks/runs/3e8cbx8t\" target=\"_blank\">efficient-violet-6</a></strong> to <a href=\"https://wandb.ai/eliottr/workspace-notebooks\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: tjsd3ws1\n",
      "Sweep URL: https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\n"
     ]
    }
   ],
   "source": [
    "wandb.init()\n",
    "\n",
    "sweep_config = {\n",
    "  \"name\" : \"my-sweep\",\n",
    "  \"method\" : \"bayes\",\n",
    "    \"metric\": {\"name\": \"eval/f1\", \"goal\": \"maximize\"},\n",
    "  \"parameters\" : {\n",
    "    \"learning_rate\" :{\n",
    "      \"distribution\": \"uniform\",\n",
    "      \"min\": 0.0000001,\n",
    "      \"max\": 0.00001\n",
    "    },\n",
    "    \"num_train_epochs\" :{\n",
    "      \"distribution\": \"int_uniform\",\n",
    "      \"min\": 1,\n",
    "      \"max\": 3\n",
    "    },\n",
    "    \"per_device_train_batch_size\" :{\n",
    "      \"distribution\": \"int_uniform\",\n",
    "      \"min\": 8,\n",
    "      \"max\": 32\n",
    "    },\n",
    "    \"weight_decay\":{\n",
    "      \"max\": 0.02,\n",
    "      \"min\": 0.005,\n",
    "      \"distribution\": \"uniform\",\n",
    "    },\n",
    "    \"warmup_ratio\":{ \n",
    "      \"max\": 0.1,\n",
    "      \"min\": 0.01,\n",
    "      \"distribution\": \"uniform\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "METRIC_NAME = \"f1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "MODEL_DIR = \"/workspace/models\"\n",
    "DATA_DIR = \"/workspace/data\"\n",
    "CACHE_DIR = \"/.cache/huggingface\"\n",
    "LOG_DIR = \"/workspace/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3, cache_dir=CACHE_DIR)\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=70)  # max length 70 for twitter\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = load_metric(METRIC_NAME)\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop():\n",
    "    with wandb.init() as run:\n",
    "        # Define model and data\n",
    "\n",
    "        # DATASET = \"movies\"\n",
    "        DATASET = \"tweets\"\n",
    "        set_caching_enabled(True)\n",
    "        set_seed(42)\n",
    "        NUM_LABELS = 3\n",
    "        dataset_raw = load_dataset(\n",
    "                DATA_DIR + \"/tweet-sentiment-extraction\",\n",
    "                data_files={\"train\": \"train_for_fine_tune.csv\", \"test\": \"test_for_fine_tune.csv\"},\n",
    "            )\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS, cache_dir=CACHE_DIR)\n",
    "        global tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "        global metric\n",
    "        metric = load_metric(METRIC_NAME)\n",
    "\n",
    "        # Prepate the data\n",
    "        dataset = dataset_raw.map(tokenize_fn, batched=True)\n",
    "        data_collator = default_data_collator\n",
    "\n",
    "        train_dataset = dataset[\"train\"]\n",
    "        eval_dataset = dataset[\"test\"]\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        training_args = TrainingArguments(\n",
    "            f\"{MODEL_DIR}/{MODEL_NAME}-finetuned-\" + DATASET,\n",
    "            per_device_train_batch_size=wandb.config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=wandb.config.per_device_train_batch_size,\n",
    "            learning_rate=wandb.config.learning_rate,\n",
    "            num_train_epochs=wandb.config.num_train_epochs,\n",
    "            logging_dir=LOG_DIR,\n",
    "            warmup_ratio=wandb.config.warmup_ratio,\n",
    "            weight_decay=wandb.config.weight_decay,\n",
    "            metric_for_best_model=METRIC_NAME,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            logging_steps=50,\n",
    "            #save_steps=50,\n",
    "            load_best_model_at_end=True\n",
    "            # lr_scheduler_type='cosine_with_restarts'\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            # optimizers=[AdamW(model.parameters(), lr=2e-5), get_cosine_with_hard_restarts_schedule_with_warmup(optimizer=AdamW(model.parameters(), lr=2e-5), num_warmup_steps=300, num_training_steps=3436, num_cycles=3)]\n",
    "        )\n",
    "\n",
    "        result_train = trainer.train()\n",
    "        result_eval = trainer.evaluate()\n",
    "        trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pgbqf04v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.340994941331681e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05049849975627213\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.016067169519792812\n",
      "2022-04-27 20:04:12.381465: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220427_200411-pgbqf04v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/pgbqf04v\" target=\"_blank\">absurd-sweep-1</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb60ee7ac35646548572d6f7f182b562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 20\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 20\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4122\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4122' max='4122' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4122/4122 16:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.108200</td>\n",
       "      <td>1.095607</td>\n",
       "      <td>0.312542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.093300</td>\n",
       "      <td>1.061980</td>\n",
       "      <td>0.433047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.053400</td>\n",
       "      <td>1.008510</td>\n",
       "      <td>0.503555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.982800</td>\n",
       "      <td>0.900186</td>\n",
       "      <td>0.602116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.879400</td>\n",
       "      <td>0.785755</td>\n",
       "      <td>0.670767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.756500</td>\n",
       "      <td>0.681085</td>\n",
       "      <td>0.732422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.707800</td>\n",
       "      <td>0.642355</td>\n",
       "      <td>0.738141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.639500</td>\n",
       "      <td>0.616598</td>\n",
       "      <td>0.751331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.656100</td>\n",
       "      <td>0.595296</td>\n",
       "      <td>0.764201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.614000</td>\n",
       "      <td>0.597090</td>\n",
       "      <td>0.756402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.621200</td>\n",
       "      <td>0.580965</td>\n",
       "      <td>0.763006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.584700</td>\n",
       "      <td>0.575122</td>\n",
       "      <td>0.767848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.582500</td>\n",
       "      <td>0.561386</td>\n",
       "      <td>0.776214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.592700</td>\n",
       "      <td>0.571025</td>\n",
       "      <td>0.766975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.572600</td>\n",
       "      <td>0.589995</td>\n",
       "      <td>0.757726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.632900</td>\n",
       "      <td>0.550255</td>\n",
       "      <td>0.773371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.561500</td>\n",
       "      <td>0.551955</td>\n",
       "      <td>0.773168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.543868</td>\n",
       "      <td>0.782054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.572600</td>\n",
       "      <td>0.546304</td>\n",
       "      <td>0.771351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.578400</td>\n",
       "      <td>0.533340</td>\n",
       "      <td>0.778983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.569900</td>\n",
       "      <td>0.560766</td>\n",
       "      <td>0.771662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.581200</td>\n",
       "      <td>0.529244</td>\n",
       "      <td>0.783193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>0.539206</td>\n",
       "      <td>0.777542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.525700</td>\n",
       "      <td>0.539754</td>\n",
       "      <td>0.781785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.562000</td>\n",
       "      <td>0.529490</td>\n",
       "      <td>0.779304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.477900</td>\n",
       "      <td>0.534008</td>\n",
       "      <td>0.781741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.525200</td>\n",
       "      <td>0.529417</td>\n",
       "      <td>0.788262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.507800</td>\n",
       "      <td>0.518628</td>\n",
       "      <td>0.782554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.451400</td>\n",
       "      <td>0.538456</td>\n",
       "      <td>0.784789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.495300</td>\n",
       "      <td>0.530850</td>\n",
       "      <td>0.781710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.512900</td>\n",
       "      <td>0.519857</td>\n",
       "      <td>0.789386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.507700</td>\n",
       "      <td>0.515427</td>\n",
       "      <td>0.790279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.488900</td>\n",
       "      <td>0.518633</td>\n",
       "      <td>0.788789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.539127</td>\n",
       "      <td>0.789966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.509200</td>\n",
       "      <td>0.521472</td>\n",
       "      <td>0.788146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.460200</td>\n",
       "      <td>0.524888</td>\n",
       "      <td>0.792546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.497700</td>\n",
       "      <td>0.524942</td>\n",
       "      <td>0.789300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.499200</td>\n",
       "      <td>0.520724</td>\n",
       "      <td>0.789853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.443400</td>\n",
       "      <td>0.528952</td>\n",
       "      <td>0.787102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.517400</td>\n",
       "      <td>0.506604</td>\n",
       "      <td>0.789208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.485300</td>\n",
       "      <td>0.532637</td>\n",
       "      <td>0.786689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.454900</td>\n",
       "      <td>0.524683</td>\n",
       "      <td>0.790538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.474500</td>\n",
       "      <td>0.514857</td>\n",
       "      <td>0.794027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.479200</td>\n",
       "      <td>0.508012</td>\n",
       "      <td>0.794173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.482000</td>\n",
       "      <td>0.519137</td>\n",
       "      <td>0.792029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.452800</td>\n",
       "      <td>0.525761</td>\n",
       "      <td>0.789430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.455900</td>\n",
       "      <td>0.520614</td>\n",
       "      <td>0.795368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.489900</td>\n",
       "      <td>0.525867</td>\n",
       "      <td>0.791952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.486100</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.795948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.476100</td>\n",
       "      <td>0.515631</td>\n",
       "      <td>0.794770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.446600</td>\n",
       "      <td>0.508264</td>\n",
       "      <td>0.795124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.439500</td>\n",
       "      <td>0.521916</td>\n",
       "      <td>0.791989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.468000</td>\n",
       "      <td>0.516194</td>\n",
       "      <td>0.794662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.532000</td>\n",
       "      <td>0.508317</td>\n",
       "      <td>0.794066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.509300</td>\n",
       "      <td>0.519003</td>\n",
       "      <td>0.791451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.403600</td>\n",
       "      <td>0.519218</td>\n",
       "      <td>0.792629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.432700</td>\n",
       "      <td>0.523468</td>\n",
       "      <td>0.791521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.445700</td>\n",
       "      <td>0.523910</td>\n",
       "      <td>0.791451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.433400</td>\n",
       "      <td>0.520651</td>\n",
       "      <td>0.792824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.382400</td>\n",
       "      <td>0.542086</td>\n",
       "      <td>0.789709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.432000</td>\n",
       "      <td>0.527242</td>\n",
       "      <td>0.792092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.445900</td>\n",
       "      <td>0.521309</td>\n",
       "      <td>0.789166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.436700</td>\n",
       "      <td>0.523984</td>\n",
       "      <td>0.795005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.385700</td>\n",
       "      <td>0.527456</td>\n",
       "      <td>0.790918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.425400</td>\n",
       "      <td>0.520806</td>\n",
       "      <td>0.792553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>0.526264</td>\n",
       "      <td>0.792612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.414000</td>\n",
       "      <td>0.522758</td>\n",
       "      <td>0.793137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>0.520154</td>\n",
       "      <td>0.796220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>0.518551</td>\n",
       "      <td>0.793164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.440600</td>\n",
       "      <td>0.524321</td>\n",
       "      <td>0.792406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.430400</td>\n",
       "      <td>0.521780</td>\n",
       "      <td>0.792651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.431300</td>\n",
       "      <td>0.517686</td>\n",
       "      <td>0.793469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.441800</td>\n",
       "      <td>0.515674</td>\n",
       "      <td>0.796542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.404600</td>\n",
       "      <td>0.516438</td>\n",
       "      <td>0.797736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>0.517362</td>\n",
       "      <td>0.794793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.468100</td>\n",
       "      <td>0.519597</td>\n",
       "      <td>0.794051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.418900</td>\n",
       "      <td>0.515742</td>\n",
       "      <td>0.795985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.427000</td>\n",
       "      <td>0.518318</td>\n",
       "      <td>0.795658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.423200</td>\n",
       "      <td>0.515971</td>\n",
       "      <td>0.794329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.414000</td>\n",
       "      <td>0.516929</td>\n",
       "      <td>0.795104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>0.517709</td>\n",
       "      <td>0.794453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.443200</td>\n",
       "      <td>0.517741</td>\n",
       "      <td>0.793850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000 (score: 0.7951039795616293).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='177' max='177' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [177/177 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▄▆▇█▇█▇████████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▇▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▄▂▂▃▂▃▄█▄▃▃▃▃▃▃▇▃▄▄▄▄▄▄▄▃▃▄▆▃▄▄▄▃▄▄▄▄▄▄</td></tr><tr><td>eval/samples_per_second</td><td>█▅▇▇▆▇▆▅▁▅▆▆▆▆▆▆▂▆▄▅▅▅▅▅▅▆▆▅▃▆▅▅▅▆▅▅▅▅▅▅</td></tr><tr><td>eval/steps_per_second</td><td>█▅▇▇▆▇▆▅▁▅▆▆▆▆▆▆▂▆▄▅▅▅▅▅▅▆▆▅▃▆▅▅▅▆▅▅▅▅▅▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▃▆███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▄▃▃▃▃▂▃▃▂▂▂▂▂▁▁▂▂▁▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.7951</td></tr><tr><td>eval/loss</td><td>0.51693</td></tr><tr><td>eval/runtime</td><td>5.8898</td></tr><tr><td>eval/samples_per_second</td><td>600.02</td></tr><tr><td>eval/steps_per_second</td><td>30.052</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>4122</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4432</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.52609</td></tr><tr><td>train/train_runtime</td><td>966.849</td></tr><tr><td>train/train_samples_per_second</td><td>85.267</td></tr><tr><td>train/train_steps_per_second</td><td>4.263</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">absurd-sweep-1</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/pgbqf04v\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/pgbqf04v</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220427_200411-pgbqf04v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: adkk6dr0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.831443119263844e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.01960502604771487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.019650452308337615\n",
      "2022-04-27 20:21:00.663377: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220427_202059-adkk6dr0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/adkk6dr0\" target=\"_blank\">driven-sweep-2</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7de3b5098cf48fc9a1381c847d5f647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5496\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5496' max='5496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5496/5496 18:15, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>1.104203</td>\n",
       "      <td>0.274477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.103500</td>\n",
       "      <td>1.084078</td>\n",
       "      <td>0.364890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.085700</td>\n",
       "      <td>1.067239</td>\n",
       "      <td>0.419955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.076800</td>\n",
       "      <td>1.053738</td>\n",
       "      <td>0.451662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.062200</td>\n",
       "      <td>1.037582</td>\n",
       "      <td>0.471164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.032700</td>\n",
       "      <td>1.014960</td>\n",
       "      <td>0.506948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.015200</td>\n",
       "      <td>0.998850</td>\n",
       "      <td>0.533227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.990200</td>\n",
       "      <td>0.966087</td>\n",
       "      <td>0.528131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.937247</td>\n",
       "      <td>0.569954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.939600</td>\n",
       "      <td>0.907394</td>\n",
       "      <td>0.575436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.921800</td>\n",
       "      <td>0.878440</td>\n",
       "      <td>0.615586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.877900</td>\n",
       "      <td>0.852438</td>\n",
       "      <td>0.651279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.909200</td>\n",
       "      <td>0.832767</td>\n",
       "      <td>0.658033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.838400</td>\n",
       "      <td>0.817095</td>\n",
       "      <td>0.686161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.823200</td>\n",
       "      <td>0.796604</td>\n",
       "      <td>0.691067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.807800</td>\n",
       "      <td>0.778425</td>\n",
       "      <td>0.697494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.783700</td>\n",
       "      <td>0.761071</td>\n",
       "      <td>0.702278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.777600</td>\n",
       "      <td>0.746681</td>\n",
       "      <td>0.708627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.732730</td>\n",
       "      <td>0.705755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.742500</td>\n",
       "      <td>0.719563</td>\n",
       "      <td>0.708600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.760700</td>\n",
       "      <td>0.706693</td>\n",
       "      <td>0.714267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.725400</td>\n",
       "      <td>0.695858</td>\n",
       "      <td>0.721996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.734500</td>\n",
       "      <td>0.684500</td>\n",
       "      <td>0.726317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>0.682452</td>\n",
       "      <td>0.726777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.685900</td>\n",
       "      <td>0.665837</td>\n",
       "      <td>0.737434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.669100</td>\n",
       "      <td>0.661656</td>\n",
       "      <td>0.730901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.637900</td>\n",
       "      <td>0.651247</td>\n",
       "      <td>0.741694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.710300</td>\n",
       "      <td>0.648084</td>\n",
       "      <td>0.736757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.683400</td>\n",
       "      <td>0.644032</td>\n",
       "      <td>0.738620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.630300</td>\n",
       "      <td>0.659125</td>\n",
       "      <td>0.724325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.723300</td>\n",
       "      <td>0.639774</td>\n",
       "      <td>0.740702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.651500</td>\n",
       "      <td>0.637292</td>\n",
       "      <td>0.733202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.659500</td>\n",
       "      <td>0.629444</td>\n",
       "      <td>0.744816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.643400</td>\n",
       "      <td>0.625806</td>\n",
       "      <td>0.745005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.670200</td>\n",
       "      <td>0.622865</td>\n",
       "      <td>0.748651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.621327</td>\n",
       "      <td>0.748018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.656700</td>\n",
       "      <td>0.624428</td>\n",
       "      <td>0.742585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.628500</td>\n",
       "      <td>0.616189</td>\n",
       "      <td>0.749449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.650500</td>\n",
       "      <td>0.614875</td>\n",
       "      <td>0.752572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.658700</td>\n",
       "      <td>0.609676</td>\n",
       "      <td>0.753809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.616000</td>\n",
       "      <td>0.610992</td>\n",
       "      <td>0.752754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.673500</td>\n",
       "      <td>0.618696</td>\n",
       "      <td>0.745291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.674700</td>\n",
       "      <td>0.612201</td>\n",
       "      <td>0.745832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.655900</td>\n",
       "      <td>0.605321</td>\n",
       "      <td>0.752927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.637800</td>\n",
       "      <td>0.612025</td>\n",
       "      <td>0.748103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.597300</td>\n",
       "      <td>0.601385</td>\n",
       "      <td>0.755751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.578500</td>\n",
       "      <td>0.603776</td>\n",
       "      <td>0.754818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.618100</td>\n",
       "      <td>0.600788</td>\n",
       "      <td>0.754442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.653100</td>\n",
       "      <td>0.595856</td>\n",
       "      <td>0.759230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.610800</td>\n",
       "      <td>0.596432</td>\n",
       "      <td>0.756789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.555300</td>\n",
       "      <td>0.597534</td>\n",
       "      <td>0.756013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.575700</td>\n",
       "      <td>0.594905</td>\n",
       "      <td>0.758106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.583100</td>\n",
       "      <td>0.591371</td>\n",
       "      <td>0.762514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.637200</td>\n",
       "      <td>0.592065</td>\n",
       "      <td>0.759099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.589867</td>\n",
       "      <td>0.759012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.570100</td>\n",
       "      <td>0.587410</td>\n",
       "      <td>0.762636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.580800</td>\n",
       "      <td>0.592037</td>\n",
       "      <td>0.759870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.543800</td>\n",
       "      <td>0.588144</td>\n",
       "      <td>0.759830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.579800</td>\n",
       "      <td>0.585713</td>\n",
       "      <td>0.762015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.631800</td>\n",
       "      <td>0.586535</td>\n",
       "      <td>0.761156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.587580</td>\n",
       "      <td>0.758122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.584300</td>\n",
       "      <td>0.586561</td>\n",
       "      <td>0.760442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.558200</td>\n",
       "      <td>0.592025</td>\n",
       "      <td>0.759205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.574600</td>\n",
       "      <td>0.586131</td>\n",
       "      <td>0.760742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.598800</td>\n",
       "      <td>0.582923</td>\n",
       "      <td>0.762245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.566400</td>\n",
       "      <td>0.582157</td>\n",
       "      <td>0.765680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.585200</td>\n",
       "      <td>0.579687</td>\n",
       "      <td>0.769320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.517000</td>\n",
       "      <td>0.580897</td>\n",
       "      <td>0.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.574500</td>\n",
       "      <td>0.577483</td>\n",
       "      <td>0.767690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.620900</td>\n",
       "      <td>0.580437</td>\n",
       "      <td>0.764992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.566000</td>\n",
       "      <td>0.576487</td>\n",
       "      <td>0.768930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.569400</td>\n",
       "      <td>0.575589</td>\n",
       "      <td>0.769061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.609500</td>\n",
       "      <td>0.584014</td>\n",
       "      <td>0.763671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.601800</td>\n",
       "      <td>0.573439</td>\n",
       "      <td>0.770540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.602800</td>\n",
       "      <td>0.576162</td>\n",
       "      <td>0.765549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.600500</td>\n",
       "      <td>0.574449</td>\n",
       "      <td>0.767783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.552300</td>\n",
       "      <td>0.575926</td>\n",
       "      <td>0.764600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.553000</td>\n",
       "      <td>0.573860</td>\n",
       "      <td>0.767707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.583400</td>\n",
       "      <td>0.571849</td>\n",
       "      <td>0.769080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.622400</td>\n",
       "      <td>0.571065</td>\n",
       "      <td>0.771345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.599400</td>\n",
       "      <td>0.570275</td>\n",
       "      <td>0.771520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.550800</td>\n",
       "      <td>0.574621</td>\n",
       "      <td>0.763659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.584100</td>\n",
       "      <td>0.570074</td>\n",
       "      <td>0.770822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.568700</td>\n",
       "      <td>0.572480</td>\n",
       "      <td>0.769480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.572500</td>\n",
       "      <td>0.571587</td>\n",
       "      <td>0.770063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.604700</td>\n",
       "      <td>0.569200</td>\n",
       "      <td>0.771157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>0.569363</td>\n",
       "      <td>0.770951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.619600</td>\n",
       "      <td>0.568337</td>\n",
       "      <td>0.772370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.539900</td>\n",
       "      <td>0.568467</td>\n",
       "      <td>0.771308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.590800</td>\n",
       "      <td>0.568361</td>\n",
       "      <td>0.772057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.516700</td>\n",
       "      <td>0.567467</td>\n",
       "      <td>0.772880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.588900</td>\n",
       "      <td>0.568580</td>\n",
       "      <td>0.771420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.546900</td>\n",
       "      <td>0.567799</td>\n",
       "      <td>0.773059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.545400</td>\n",
       "      <td>0.566549</td>\n",
       "      <td>0.776462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.608100</td>\n",
       "      <td>0.566332</td>\n",
       "      <td>0.773294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.555600</td>\n",
       "      <td>0.567826</td>\n",
       "      <td>0.772137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.592200</td>\n",
       "      <td>0.567978</td>\n",
       "      <td>0.770534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.586000</td>\n",
       "      <td>0.566899</td>\n",
       "      <td>0.772399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.573700</td>\n",
       "      <td>0.566462</td>\n",
       "      <td>0.775349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.537700</td>\n",
       "      <td>0.566814</td>\n",
       "      <td>0.775060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.528100</td>\n",
       "      <td>0.565937</td>\n",
       "      <td>0.773242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.569400</td>\n",
       "      <td>0.565806</td>\n",
       "      <td>0.774450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>0.565805</td>\n",
       "      <td>0.774499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.603700</td>\n",
       "      <td>0.566167</td>\n",
       "      <td>0.773518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.531200</td>\n",
       "      <td>0.566129</td>\n",
       "      <td>0.773531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.589500</td>\n",
       "      <td>0.566414</td>\n",
       "      <td>0.771971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.645300</td>\n",
       "      <td>0.566495</td>\n",
       "      <td>0.772145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.575500</td>\n",
       "      <td>0.566377</td>\n",
       "      <td>0.771613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.579000</td>\n",
       "      <td>0.566458</td>\n",
       "      <td>0.771584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000 (score: 0.7750602175516776).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='354' max='354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [354/354 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▃▄▅▆▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>eval/loss</td><td>██▇▆▅▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▃▃▄▃▃▄▄▂▂▂█▂▂▂▃▁▂▄▁▂▃▂▅▃▃▄▃▄▅▄▂▁▃▂▄▃▄▃▆</td></tr><tr><td>eval/samples_per_second</td><td>█▆▆▅▆▆▅▅▆▇▇▁▇▇▇▆▇▇▅█▇▆▇▄▆▆▅▆▅▄▅▇█▆▇▅▆▅▆▃</td></tr><tr><td>eval/steps_per_second</td><td>█▆▆▅▆▆▅▅▆▇▇▁▇▇▇▆▇▇▅█▇▆▇▄▆▆▅▆▅▄▅▇█▆▇▅▆▅▆▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▄███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▅▅▄▄▄▃▄▄▃▃▃▃▃▃▂▂▂▂▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.77506</td></tr><tr><td>eval/loss</td><td>0.56681</td></tr><tr><td>eval/runtime</td><td>6.3752</td></tr><tr><td>eval/samples_per_second</td><td>554.337</td></tr><tr><td>eval/steps_per_second</td><td>55.528</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>5496</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.579</td></tr><tr><td>train/total_flos</td><td>1977050665396800.0</td></tr><tr><td>train/train_loss</td><td>0.66491</td></tr><tr><td>train/train_runtime</td><td>1095.4304</td></tr><tr><td>train/train_samples_per_second</td><td>50.172</td></tr><tr><td>train/train_steps_per_second</td><td>5.017</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">driven-sweep-2</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/adkk6dr0\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/adkk6dr0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220427_202059-adkk6dr0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vxedv13w with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4.209517912375305e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.039171741820167194\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0057429187590040745\n",
      "2022-04-27 20:39:58.594960: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220427_203957-vxedv13w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/vxedv13w\" target=\"_blank\">breezy-sweep-3</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7449e3211246b0a221345a3d0ee23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 17\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 17\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1617\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1617' max='1617' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1617/1617 06:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.103500</td>\n",
       "      <td>1.082274</td>\n",
       "      <td>0.368325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.071900</td>\n",
       "      <td>1.031448</td>\n",
       "      <td>0.482275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.019000</td>\n",
       "      <td>0.978162</td>\n",
       "      <td>0.571756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.894350</td>\n",
       "      <td>0.646219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.887100</td>\n",
       "      <td>0.822410</td>\n",
       "      <td>0.672516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.824600</td>\n",
       "      <td>0.775568</td>\n",
       "      <td>0.687915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.765800</td>\n",
       "      <td>0.722904</td>\n",
       "      <td>0.709282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.755400</td>\n",
       "      <td>0.688513</td>\n",
       "      <td>0.718245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.703300</td>\n",
       "      <td>0.658545</td>\n",
       "      <td>0.731143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.666800</td>\n",
       "      <td>0.637904</td>\n",
       "      <td>0.742153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.663400</td>\n",
       "      <td>0.631797</td>\n",
       "      <td>0.745602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.646800</td>\n",
       "      <td>0.615125</td>\n",
       "      <td>0.750573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.646000</td>\n",
       "      <td>0.609071</td>\n",
       "      <td>0.752034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.612100</td>\n",
       "      <td>0.608840</td>\n",
       "      <td>0.754037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>0.597457</td>\n",
       "      <td>0.761615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.592000</td>\n",
       "      <td>0.596236</td>\n",
       "      <td>0.757550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.631400</td>\n",
       "      <td>0.598922</td>\n",
       "      <td>0.754601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.639200</td>\n",
       "      <td>0.599852</td>\n",
       "      <td>0.749799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.615600</td>\n",
       "      <td>0.588480</td>\n",
       "      <td>0.760460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.591600</td>\n",
       "      <td>0.588710</td>\n",
       "      <td>0.757427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.642900</td>\n",
       "      <td>0.578980</td>\n",
       "      <td>0.767755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.617200</td>\n",
       "      <td>0.581728</td>\n",
       "      <td>0.762377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>0.577725</td>\n",
       "      <td>0.767582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.608500</td>\n",
       "      <td>0.579078</td>\n",
       "      <td>0.763966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.634700</td>\n",
       "      <td>0.580727</td>\n",
       "      <td>0.757975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.618500</td>\n",
       "      <td>0.572637</td>\n",
       "      <td>0.767915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.575008</td>\n",
       "      <td>0.765292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.574500</td>\n",
       "      <td>0.574908</td>\n",
       "      <td>0.762929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.630900</td>\n",
       "      <td>0.569862</td>\n",
       "      <td>0.770609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.552400</td>\n",
       "      <td>0.573064</td>\n",
       "      <td>0.763447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.573900</td>\n",
       "      <td>0.570219</td>\n",
       "      <td>0.768551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.584400</td>\n",
       "      <td>0.570279</td>\n",
       "      <td>0.767076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500 (score: 0.7634472202131296).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='208' max='208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [208/208 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▃▅▆▆▇▇▇▇████████████████████████</td></tr><tr><td>eval/loss</td><td>█▇▇▅▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▁▂▂▂▂█▂▃▃▂▂▂▃▂▃▂▂▃▃▃▂▃▂▃▃▃▃▂▃▃▃▅</td></tr><tr><td>eval/samples_per_second</td><td>██▇▇▇▇▁▇▆▆▇▇▇▆▇▆▇▇▆▆▆▇▆▇▆▆▆▆▇▆▆▆▄</td></tr><tr><td>eval/steps_per_second</td><td>██▇▇▇▇▁▇▆▆▇▇▇▆▇▆▇▇▆▆▆▇▆▇▆▆▆▆▇▆▆▆▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▇███▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▇▆▅▄▄▄▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▁▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.76345</td></tr><tr><td>eval/loss</td><td>0.57306</td></tr><tr><td>eval/runtime</td><td>6.2733</td></tr><tr><td>eval/samples_per_second</td><td>563.343</td></tr><tr><td>eval/steps_per_second</td><td>33.157</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>1617</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5844</td></tr><tr><td>train/total_flos</td><td>988525332698400.0</td></tr><tr><td>train/train_loss</td><td>0.69402</td></tr><tr><td>train/train_runtime</td><td>374.4248</td></tr><tr><td>train/train_samples_per_second</td><td>73.393</td></tr><tr><td>train/train_steps_per_second</td><td>4.319</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">breezy-sweep-3</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/vxedv13w\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/vxedv13w</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220427_203957-vxedv13w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3rg1eo05 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.3361077839598998e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.03999780827110677\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01590829122600427\n",
      "2022-04-27 20:46:47.159316: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220427_204645-3rg1eo05</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/3rg1eo05\" target=\"_blank\">mild-sweep-4</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1c34253ebf416e80b95747eadbe31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 21\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 21\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1309\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1309' max='1309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1309/1309 05:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.106400</td>\n",
       "      <td>1.097227</td>\n",
       "      <td>0.304043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.090200</td>\n",
       "      <td>1.076743</td>\n",
       "      <td>0.395271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.073100</td>\n",
       "      <td>1.059649</td>\n",
       "      <td>0.433491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.060000</td>\n",
       "      <td>1.041295</td>\n",
       "      <td>0.456028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.043400</td>\n",
       "      <td>1.022982</td>\n",
       "      <td>0.452570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.033000</td>\n",
       "      <td>1.011738</td>\n",
       "      <td>0.456209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.020300</td>\n",
       "      <td>0.986842</td>\n",
       "      <td>0.494751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>0.968425</td>\n",
       "      <td>0.508333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.966900</td>\n",
       "      <td>0.944446</td>\n",
       "      <td>0.505488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.965500</td>\n",
       "      <td>0.924907</td>\n",
       "      <td>0.524708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.947400</td>\n",
       "      <td>0.906511</td>\n",
       "      <td>0.555703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.921400</td>\n",
       "      <td>0.891441</td>\n",
       "      <td>0.581264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.906700</td>\n",
       "      <td>0.874684</td>\n",
       "      <td>0.582881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.896300</td>\n",
       "      <td>0.861850</td>\n",
       "      <td>0.619079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.850713</td>\n",
       "      <td>0.642397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.863800</td>\n",
       "      <td>0.840929</td>\n",
       "      <td>0.645418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.868500</td>\n",
       "      <td>0.831831</td>\n",
       "      <td>0.651891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.844800</td>\n",
       "      <td>0.823928</td>\n",
       "      <td>0.668731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.861000</td>\n",
       "      <td>0.817257</td>\n",
       "      <td>0.658356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.850800</td>\n",
       "      <td>0.810849</td>\n",
       "      <td>0.674817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.839900</td>\n",
       "      <td>0.805995</td>\n",
       "      <td>0.677713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.820300</td>\n",
       "      <td>0.801798</td>\n",
       "      <td>0.681498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.829400</td>\n",
       "      <td>0.798455</td>\n",
       "      <td>0.688469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.815100</td>\n",
       "      <td>0.796247</td>\n",
       "      <td>0.690906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.799400</td>\n",
       "      <td>0.794825</td>\n",
       "      <td>0.690892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.820800</td>\n",
       "      <td>0.794160</td>\n",
       "      <td>0.691484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000 (score: 0.6748165030664601).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='169' max='169' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [169/169 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇█▇████████</td></tr><tr><td>eval/loss</td><td>██▇▇▆▆▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▃▂▁▁▁▃▃▃▄▁▃▃▂▂▂▅▃▃▂▃▃▃█▃▂▂</td></tr><tr><td>eval/samples_per_second</td><td>█▆▇███▆▆▆▅█▆▆▇▇▇▄▆▆▇▆▆▅▁▆▆▇</td></tr><tr><td>eval/steps_per_second</td><td>█▆▇███▆▆▆▅█▆▆▇▇▇▄▆▆▇▆▆▅▁▆▆▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▆▆▆▅▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▆▆▅▅▅▄▄▃▃▃▂▃▂▂▂▂▁▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.67482</td></tr><tr><td>eval/loss</td><td>0.81085</td></tr><tr><td>eval/runtime</td><td>5.7133</td></tr><tr><td>eval/samples_per_second</td><td>618.553</td></tr><tr><td>eval/steps_per_second</td><td>29.58</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>1309</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.8208</td></tr><tr><td>train/total_flos</td><td>988525332698400.0</td></tr><tr><td>train/train_loss</td><td>0.92646</td></tr><tr><td>train/train_runtime</td><td>304.499</td></tr><tr><td>train/train_samples_per_second</td><td>90.247</td></tr><tr><td>train/train_steps_per_second</td><td>4.299</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">mild-sweep-4</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/3rg1eo05\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/3rg1eo05</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220427_204645-3rg1eo05/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d1k38tqj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 7.459411257403022e-07\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.02884344730004397\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01576994972645896\n",
      "2022-04-27 20:52:32.881701: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220427_205231-d1k38tqj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/d1k38tqj\" target=\"_blank\">worthy-sweep-5</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17314c0ad7474a2594c4c07d1841bdda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 9\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 9\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6108\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6108' max='6108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6108/6108 20:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.117800</td>\n",
       "      <td>1.108986</td>\n",
       "      <td>0.250013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.103300</td>\n",
       "      <td>1.104588</td>\n",
       "      <td>0.269751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.111300</td>\n",
       "      <td>1.096929</td>\n",
       "      <td>0.311158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.086000</td>\n",
       "      <td>1.090233</td>\n",
       "      <td>0.346631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.094100</td>\n",
       "      <td>1.080619</td>\n",
       "      <td>0.381067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.083400</td>\n",
       "      <td>1.074741</td>\n",
       "      <td>0.404105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.067600</td>\n",
       "      <td>1.068388</td>\n",
       "      <td>0.411187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.068700</td>\n",
       "      <td>1.063285</td>\n",
       "      <td>0.424525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.067600</td>\n",
       "      <td>1.055563</td>\n",
       "      <td>0.429722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.065500</td>\n",
       "      <td>1.048942</td>\n",
       "      <td>0.439175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.061900</td>\n",
       "      <td>1.039343</td>\n",
       "      <td>0.434928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.044400</td>\n",
       "      <td>1.033083</td>\n",
       "      <td>0.429319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>1.028038</td>\n",
       "      <td>0.428612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.039200</td>\n",
       "      <td>1.025361</td>\n",
       "      <td>0.423421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.042200</td>\n",
       "      <td>1.013017</td>\n",
       "      <td>0.465020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.008100</td>\n",
       "      <td>1.002923</td>\n",
       "      <td>0.477372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.013900</td>\n",
       "      <td>0.995849</td>\n",
       "      <td>0.483835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.999500</td>\n",
       "      <td>0.984627</td>\n",
       "      <td>0.500923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.973636</td>\n",
       "      <td>0.509429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.978400</td>\n",
       "      <td>0.963414</td>\n",
       "      <td>0.504898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.965200</td>\n",
       "      <td>0.953525</td>\n",
       "      <td>0.501996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.977100</td>\n",
       "      <td>0.943443</td>\n",
       "      <td>0.500446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.970600</td>\n",
       "      <td>0.934136</td>\n",
       "      <td>0.536033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.941900</td>\n",
       "      <td>0.924058</td>\n",
       "      <td>0.524543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.947200</td>\n",
       "      <td>0.912651</td>\n",
       "      <td>0.552728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.931900</td>\n",
       "      <td>0.903730</td>\n",
       "      <td>0.567343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.893990</td>\n",
       "      <td>0.594249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.899800</td>\n",
       "      <td>0.882127</td>\n",
       "      <td>0.600062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.885100</td>\n",
       "      <td>0.872906</td>\n",
       "      <td>0.600260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.875700</td>\n",
       "      <td>0.863422</td>\n",
       "      <td>0.615013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.884600</td>\n",
       "      <td>0.855605</td>\n",
       "      <td>0.626366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.895500</td>\n",
       "      <td>0.848332</td>\n",
       "      <td>0.640627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.856200</td>\n",
       "      <td>0.840935</td>\n",
       "      <td>0.662238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.878600</td>\n",
       "      <td>0.833705</td>\n",
       "      <td>0.663145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.836400</td>\n",
       "      <td>0.826827</td>\n",
       "      <td>0.660796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.838600</td>\n",
       "      <td>0.819728</td>\n",
       "      <td>0.667976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.842300</td>\n",
       "      <td>0.811993</td>\n",
       "      <td>0.682052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.814100</td>\n",
       "      <td>0.806246</td>\n",
       "      <td>0.684235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.831000</td>\n",
       "      <td>0.799697</td>\n",
       "      <td>0.686798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.835700</td>\n",
       "      <td>0.794327</td>\n",
       "      <td>0.687307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.823400</td>\n",
       "      <td>0.788960</td>\n",
       "      <td>0.688505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.789500</td>\n",
       "      <td>0.784168</td>\n",
       "      <td>0.686484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.818200</td>\n",
       "      <td>0.779715</td>\n",
       "      <td>0.685602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.823900</td>\n",
       "      <td>0.771193</td>\n",
       "      <td>0.699795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.788300</td>\n",
       "      <td>0.766276</td>\n",
       "      <td>0.695061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.813300</td>\n",
       "      <td>0.759299</td>\n",
       "      <td>0.706224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.793800</td>\n",
       "      <td>0.755224</td>\n",
       "      <td>0.702239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.794100</td>\n",
       "      <td>0.750269</td>\n",
       "      <td>0.704178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.770700</td>\n",
       "      <td>0.744646</td>\n",
       "      <td>0.708241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.741190</td>\n",
       "      <td>0.705347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>0.735218</td>\n",
       "      <td>0.714186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.747600</td>\n",
       "      <td>0.730843</td>\n",
       "      <td>0.711224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.733800</td>\n",
       "      <td>0.726002</td>\n",
       "      <td>0.715896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.760700</td>\n",
       "      <td>0.721877</td>\n",
       "      <td>0.718723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.778400</td>\n",
       "      <td>0.718009</td>\n",
       "      <td>0.719875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>0.714585</td>\n",
       "      <td>0.715765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.709400</td>\n",
       "      <td>0.712041</td>\n",
       "      <td>0.712777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.709700</td>\n",
       "      <td>0.709377</td>\n",
       "      <td>0.717703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.737600</td>\n",
       "      <td>0.704320</td>\n",
       "      <td>0.721350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.723500</td>\n",
       "      <td>0.701235</td>\n",
       "      <td>0.721507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.724800</td>\n",
       "      <td>0.698426</td>\n",
       "      <td>0.719597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.706200</td>\n",
       "      <td>0.695670</td>\n",
       "      <td>0.723373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.718100</td>\n",
       "      <td>0.693128</td>\n",
       "      <td>0.721902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.722400</td>\n",
       "      <td>0.690358</td>\n",
       "      <td>0.722009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.688700</td>\n",
       "      <td>0.686302</td>\n",
       "      <td>0.727874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.716900</td>\n",
       "      <td>0.683468</td>\n",
       "      <td>0.729593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.732300</td>\n",
       "      <td>0.682523</td>\n",
       "      <td>0.728127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.706200</td>\n",
       "      <td>0.680486</td>\n",
       "      <td>0.729302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.689400</td>\n",
       "      <td>0.678655</td>\n",
       "      <td>0.724995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.670900</td>\n",
       "      <td>0.676898</td>\n",
       "      <td>0.727424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.673509</td>\n",
       "      <td>0.729318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.696800</td>\n",
       "      <td>0.672181</td>\n",
       "      <td>0.729509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.700700</td>\n",
       "      <td>0.672060</td>\n",
       "      <td>0.725141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.681300</td>\n",
       "      <td>0.668998</td>\n",
       "      <td>0.729321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.704100</td>\n",
       "      <td>0.667837</td>\n",
       "      <td>0.727381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.642200</td>\n",
       "      <td>0.665537</td>\n",
       "      <td>0.726920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.703200</td>\n",
       "      <td>0.662978</td>\n",
       "      <td>0.733203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.700200</td>\n",
       "      <td>0.661823</td>\n",
       "      <td>0.733377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.674800</td>\n",
       "      <td>0.659931</td>\n",
       "      <td>0.734753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.688600</td>\n",
       "      <td>0.658642</td>\n",
       "      <td>0.735252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.706600</td>\n",
       "      <td>0.660132</td>\n",
       "      <td>0.732371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.676400</td>\n",
       "      <td>0.655945</td>\n",
       "      <td>0.735777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.684900</td>\n",
       "      <td>0.655204</td>\n",
       "      <td>0.733354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.696500</td>\n",
       "      <td>0.654494</td>\n",
       "      <td>0.733312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>0.653866</td>\n",
       "      <td>0.731679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.656200</td>\n",
       "      <td>0.653914</td>\n",
       "      <td>0.733449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.682200</td>\n",
       "      <td>0.652245</td>\n",
       "      <td>0.733117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.650312</td>\n",
       "      <td>0.737493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.668700</td>\n",
       "      <td>0.648627</td>\n",
       "      <td>0.737594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.689300</td>\n",
       "      <td>0.649731</td>\n",
       "      <td>0.734280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.643800</td>\n",
       "      <td>0.651634</td>\n",
       "      <td>0.730269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.693200</td>\n",
       "      <td>0.648841</td>\n",
       "      <td>0.732757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.659700</td>\n",
       "      <td>0.646623</td>\n",
       "      <td>0.738746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.684700</td>\n",
       "      <td>0.645636</td>\n",
       "      <td>0.739403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.633300</td>\n",
       "      <td>0.644778</td>\n",
       "      <td>0.739458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.666800</td>\n",
       "      <td>0.644694</td>\n",
       "      <td>0.739852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.656800</td>\n",
       "      <td>0.644715</td>\n",
       "      <td>0.735459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.677800</td>\n",
       "      <td>0.644113</td>\n",
       "      <td>0.736415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.664900</td>\n",
       "      <td>0.644265</td>\n",
       "      <td>0.735220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.677100</td>\n",
       "      <td>0.643406</td>\n",
       "      <td>0.735120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.608600</td>\n",
       "      <td>0.642197</td>\n",
       "      <td>0.736055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.688700</td>\n",
       "      <td>0.641765</td>\n",
       "      <td>0.737453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.667300</td>\n",
       "      <td>0.641336</td>\n",
       "      <td>0.736963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.663500</td>\n",
       "      <td>0.639972</td>\n",
       "      <td>0.740607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.642700</td>\n",
       "      <td>0.639928</td>\n",
       "      <td>0.738266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.667100</td>\n",
       "      <td>0.640104</td>\n",
       "      <td>0.738521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.672300</td>\n",
       "      <td>0.639596</td>\n",
       "      <td>0.737829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.668100</td>\n",
       "      <td>0.638618</td>\n",
       "      <td>0.739674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.657300</td>\n",
       "      <td>0.638159</td>\n",
       "      <td>0.741158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.651400</td>\n",
       "      <td>0.637689</td>\n",
       "      <td>0.741477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.629400</td>\n",
       "      <td>0.637745</td>\n",
       "      <td>0.740004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.672800</td>\n",
       "      <td>0.637578</td>\n",
       "      <td>0.739507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.666600</td>\n",
       "      <td>0.637466</td>\n",
       "      <td>0.740060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.586400</td>\n",
       "      <td>0.637433</td>\n",
       "      <td>0.741655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.643800</td>\n",
       "      <td>0.636977</td>\n",
       "      <td>0.741354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.666400</td>\n",
       "      <td>0.636847</td>\n",
       "      <td>0.740983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.628100</td>\n",
       "      <td>0.636386</td>\n",
       "      <td>0.741797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.726300</td>\n",
       "      <td>0.636304</td>\n",
       "      <td>0.741237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.693700</td>\n",
       "      <td>0.636251</td>\n",
       "      <td>0.741223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.639600</td>\n",
       "      <td>0.636164</td>\n",
       "      <td>0.741529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.676500</td>\n",
       "      <td>0.636158</td>\n",
       "      <td>0.741529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.674900</td>\n",
       "      <td>0.636152</td>\n",
       "      <td>0.741529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000 (score: 0.7415287690409653).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='393' max='393' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [393/393 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▂▃▄▄▄▅▅▆▆▇▇▇▇▇▇████████████████████████</td></tr><tr><td>eval/loss</td><td>██▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▂▂▃▁▂▂▄▁▃▃▂▆▄▄▃▁█▂▄▂▃▅▆▅▃▂▄▄▁▃▅▂▃▅▅▅▅▅▁</td></tr><tr><td>eval/samples_per_second</td><td>█▇▇▆█▇▇▅█▆▆▇▃▅▅▆█▁▇▅▇▆▄▃▄▆▇▄▅█▆▄▇▆▄▄▄▄▄█</td></tr><tr><td>eval/steps_per_second</td><td>█▇▇▆█▇▇▅█▆▆▇▃▅▅▆█▁▇▅▇▆▄▃▄▆▇▄▅█▆▄▇▆▄▄▄▄▄█</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▃███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.74153</td></tr><tr><td>eval/loss</td><td>0.63616</td></tr><tr><td>eval/runtime</td><td>6.5423</td></tr><tr><td>eval/samples_per_second</td><td>540.177</td></tr><tr><td>eval/steps_per_second</td><td>60.071</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>6108</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6749</td></tr><tr><td>train/total_flos</td><td>1977050665396800.0</td></tr><tr><td>train/train_loss</td><td>0.78796</td></tr><tr><td>train/train_runtime</td><td>1249.3988</td></tr><tr><td>train/train_samples_per_second</td><td>43.989</td></tr><tr><td>train/train_steps_per_second</td><td>4.889</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">worthy-sweep-5</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/d1k38tqj\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/d1k38tqj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220427_205231-d1k38tqj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6yq3s8r9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4.884897966099726e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 27\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.0780993679477932\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.013225093538967454\n",
      "2022-04-27 21:13:55.850772: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220427_211354-6yq3s8r9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/6yq3s8r9\" target=\"_blank\">earnest-sweep-6</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ff34f5498e4243a1d371e2d41a9fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 27\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 27\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2036\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2036' max='2036' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2036/2036 08:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.113200</td>\n",
       "      <td>1.094276</td>\n",
       "      <td>0.323873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.083500</td>\n",
       "      <td>1.058808</td>\n",
       "      <td>0.434881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.041000</td>\n",
       "      <td>0.992354</td>\n",
       "      <td>0.461499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.963100</td>\n",
       "      <td>0.884291</td>\n",
       "      <td>0.604797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.879700</td>\n",
       "      <td>0.806252</td>\n",
       "      <td>0.659191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.778200</td>\n",
       "      <td>0.724762</td>\n",
       "      <td>0.715782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.718100</td>\n",
       "      <td>0.670915</td>\n",
       "      <td>0.736031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.656561</td>\n",
       "      <td>0.734274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.621756</td>\n",
       "      <td>0.749739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.604700</td>\n",
       "      <td>0.608493</td>\n",
       "      <td>0.755411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.614790</td>\n",
       "      <td>0.747957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.657000</td>\n",
       "      <td>0.595054</td>\n",
       "      <td>0.757162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.606200</td>\n",
       "      <td>0.587887</td>\n",
       "      <td>0.763204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.634700</td>\n",
       "      <td>0.583430</td>\n",
       "      <td>0.759958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.617900</td>\n",
       "      <td>0.575676</td>\n",
       "      <td>0.766621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.604200</td>\n",
       "      <td>0.569024</td>\n",
       "      <td>0.768808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.583200</td>\n",
       "      <td>0.571650</td>\n",
       "      <td>0.767886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.578700</td>\n",
       "      <td>0.558017</td>\n",
       "      <td>0.775350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.570700</td>\n",
       "      <td>0.561899</td>\n",
       "      <td>0.767012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.561200</td>\n",
       "      <td>0.556159</td>\n",
       "      <td>0.772626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.528300</td>\n",
       "      <td>0.555708</td>\n",
       "      <td>0.774133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.527200</td>\n",
       "      <td>0.548738</td>\n",
       "      <td>0.779405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.566000</td>\n",
       "      <td>0.550932</td>\n",
       "      <td>0.774507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>0.544732</td>\n",
       "      <td>0.778580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.517100</td>\n",
       "      <td>0.546700</td>\n",
       "      <td>0.776648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.533500</td>\n",
       "      <td>0.540183</td>\n",
       "      <td>0.779368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.522200</td>\n",
       "      <td>0.544999</td>\n",
       "      <td>0.781362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.540200</td>\n",
       "      <td>0.541747</td>\n",
       "      <td>0.779887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.512800</td>\n",
       "      <td>0.539962</td>\n",
       "      <td>0.784249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.534300</td>\n",
       "      <td>0.533395</td>\n",
       "      <td>0.781083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.505300</td>\n",
       "      <td>0.533471</td>\n",
       "      <td>0.785259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.509300</td>\n",
       "      <td>0.534817</td>\n",
       "      <td>0.782956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.523200</td>\n",
       "      <td>0.531518</td>\n",
       "      <td>0.783850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.534947</td>\n",
       "      <td>0.780539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.501900</td>\n",
       "      <td>0.532073</td>\n",
       "      <td>0.782030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.529200</td>\n",
       "      <td>0.532742</td>\n",
       "      <td>0.781572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.512900</td>\n",
       "      <td>0.531629</td>\n",
       "      <td>0.781311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.497200</td>\n",
       "      <td>0.530758</td>\n",
       "      <td>0.781424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.492600</td>\n",
       "      <td>0.531647</td>\n",
       "      <td>0.781819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.564400</td>\n",
       "      <td>0.531331</td>\n",
       "      <td>0.780963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500 (score: 0.7810832312136304).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='131' max='131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [131/131 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▃▃▅▆▇▇▇▇█▇█████████████████████████████</td></tr><tr><td>eval/loss</td><td>██▇▅▄▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▂▂▁▃▅▃▃▂▃▂▃▃▃▇▃▃▄▃▂▃▃▅▃▃▄▃▂▃▃▂▃▃▃▃▃▂█▃▃</td></tr><tr><td>eval/samples_per_second</td><td>█▇▇█▆▄▆▆▇▅▇▆▆▆▂▆▆▅▆▇▆▅▄▆▆▅▆▆▆▆▇▆▆▆▆▆▇▁▆▆</td></tr><tr><td>eval/steps_per_second</td><td>█▇▇█▆▄▆▆▇▅▇▆▆▆▂▆▆▅▆▇▆▅▄▆▆▅▆▆▆▆▇▆▆▆▆▆▇▁▆▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▃▅████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▇▆▅▄▄▃▃▂▃▃▂▃▂▂▂▂▂▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.78108</td></tr><tr><td>eval/loss</td><td>0.53339</td></tr><tr><td>eval/runtime</td><td>5.8671</td></tr><tr><td>eval/samples_per_second</td><td>602.346</td></tr><tr><td>eval/steps_per_second</td><td>22.328</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>2036</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5644</td></tr><tr><td>train/total_flos</td><td>1977050665396800.0</td></tr><tr><td>train/train_loss</td><td>0.6245</td></tr><tr><td>train/train_runtime</td><td>529.3739</td></tr><tr><td>train/train_samples_per_second</td><td>103.821</td></tr><tr><td>train/train_steps_per_second</td><td>3.846</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">earnest-sweep-6</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/6yq3s8r9\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/6yq3s8r9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220427_211354-6yq3s8r9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0nbu0cj6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 7.067731308485416e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05533789415539891\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.017223270797022638\n",
      "2022-04-27 21:23:34.351527: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220427_212332-0nbu0cj6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/0nbu0cj6\" target=\"_blank\">youthful-sweep-7</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c7066ccc1444f48a26e28c22e309f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3436\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3436' max='3436' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3436/3436 12:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.103900</td>\n",
       "      <td>1.093734</td>\n",
       "      <td>0.329838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.087200</td>\n",
       "      <td>1.059183</td>\n",
       "      <td>0.415110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.056000</td>\n",
       "      <td>1.010814</td>\n",
       "      <td>0.523738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.977500</td>\n",
       "      <td>0.896946</td>\n",
       "      <td>0.656552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.845700</td>\n",
       "      <td>0.760457</td>\n",
       "      <td>0.712290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.773500</td>\n",
       "      <td>0.682052</td>\n",
       "      <td>0.718927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.703700</td>\n",
       "      <td>0.642530</td>\n",
       "      <td>0.741818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.665500</td>\n",
       "      <td>0.622186</td>\n",
       "      <td>0.745837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.657300</td>\n",
       "      <td>0.606250</td>\n",
       "      <td>0.758592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.603700</td>\n",
       "      <td>0.604603</td>\n",
       "      <td>0.758089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.637100</td>\n",
       "      <td>0.581929</td>\n",
       "      <td>0.768177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>0.578134</td>\n",
       "      <td>0.763297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.623600</td>\n",
       "      <td>0.575813</td>\n",
       "      <td>0.761480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.578100</td>\n",
       "      <td>0.589473</td>\n",
       "      <td>0.755229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.560900</td>\n",
       "      <td>0.557985</td>\n",
       "      <td>0.778133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.593100</td>\n",
       "      <td>0.552913</td>\n",
       "      <td>0.777790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.547200</td>\n",
       "      <td>0.548283</td>\n",
       "      <td>0.777767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.585100</td>\n",
       "      <td>0.560055</td>\n",
       "      <td>0.772627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.584200</td>\n",
       "      <td>0.570022</td>\n",
       "      <td>0.769072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.618500</td>\n",
       "      <td>0.546314</td>\n",
       "      <td>0.775679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.530600</td>\n",
       "      <td>0.542040</td>\n",
       "      <td>0.781564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.565400</td>\n",
       "      <td>0.543940</td>\n",
       "      <td>0.781992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.596400</td>\n",
       "      <td>0.531815</td>\n",
       "      <td>0.783182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.575200</td>\n",
       "      <td>0.538096</td>\n",
       "      <td>0.781280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.570300</td>\n",
       "      <td>0.528975</td>\n",
       "      <td>0.779928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.544700</td>\n",
       "      <td>0.544106</td>\n",
       "      <td>0.783610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.572000</td>\n",
       "      <td>0.525716</td>\n",
       "      <td>0.786821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.572600</td>\n",
       "      <td>0.547486</td>\n",
       "      <td>0.779065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.515500</td>\n",
       "      <td>0.551406</td>\n",
       "      <td>0.774020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.540700</td>\n",
       "      <td>0.531958</td>\n",
       "      <td>0.785754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.588200</td>\n",
       "      <td>0.517833</td>\n",
       "      <td>0.786049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.480400</td>\n",
       "      <td>0.526536</td>\n",
       "      <td>0.783072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.495900</td>\n",
       "      <td>0.525813</td>\n",
       "      <td>0.787599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.523285</td>\n",
       "      <td>0.787391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.476200</td>\n",
       "      <td>0.527296</td>\n",
       "      <td>0.786554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.444100</td>\n",
       "      <td>0.545934</td>\n",
       "      <td>0.785551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>0.525693</td>\n",
       "      <td>0.788197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.539700</td>\n",
       "      <td>0.528348</td>\n",
       "      <td>0.787342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.477100</td>\n",
       "      <td>0.535055</td>\n",
       "      <td>0.786053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.474400</td>\n",
       "      <td>0.520726</td>\n",
       "      <td>0.793186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.497800</td>\n",
       "      <td>0.550597</td>\n",
       "      <td>0.781768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.460700</td>\n",
       "      <td>0.531197</td>\n",
       "      <td>0.790324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.456600</td>\n",
       "      <td>0.524024</td>\n",
       "      <td>0.793250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.494400</td>\n",
       "      <td>0.524567</td>\n",
       "      <td>0.791094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.440600</td>\n",
       "      <td>0.529009</td>\n",
       "      <td>0.793548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.488400</td>\n",
       "      <td>0.526870</td>\n",
       "      <td>0.794326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.472300</td>\n",
       "      <td>0.531037</td>\n",
       "      <td>0.790348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>0.545223</td>\n",
       "      <td>0.789204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.443400</td>\n",
       "      <td>0.524986</td>\n",
       "      <td>0.794060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.500200</td>\n",
       "      <td>0.509410</td>\n",
       "      <td>0.794124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.432100</td>\n",
       "      <td>0.539161</td>\n",
       "      <td>0.784313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.465900</td>\n",
       "      <td>0.520691</td>\n",
       "      <td>0.794244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.445700</td>\n",
       "      <td>0.529420</td>\n",
       "      <td>0.791015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.490100</td>\n",
       "      <td>0.519414</td>\n",
       "      <td>0.793861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.468900</td>\n",
       "      <td>0.517470</td>\n",
       "      <td>0.794219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.440700</td>\n",
       "      <td>0.530688</td>\n",
       "      <td>0.791357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.442100</td>\n",
       "      <td>0.524710</td>\n",
       "      <td>0.790252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.467900</td>\n",
       "      <td>0.523603</td>\n",
       "      <td>0.790644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.457400</td>\n",
       "      <td>0.519786</td>\n",
       "      <td>0.792614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.470400</td>\n",
       "      <td>0.531736</td>\n",
       "      <td>0.789836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.482000</td>\n",
       "      <td>0.518143</td>\n",
       "      <td>0.793570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.461700</td>\n",
       "      <td>0.519141</td>\n",
       "      <td>0.793636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.448100</td>\n",
       "      <td>0.521721</td>\n",
       "      <td>0.793237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.444000</td>\n",
       "      <td>0.519049</td>\n",
       "      <td>0.794907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.434700</td>\n",
       "      <td>0.520853</td>\n",
       "      <td>0.793744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.466900</td>\n",
       "      <td>0.521094</td>\n",
       "      <td>0.793090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.533500</td>\n",
       "      <td>0.520053</td>\n",
       "      <td>0.792740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.477800</td>\n",
       "      <td>0.520140</td>\n",
       "      <td>0.792805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500 (score: 0.7941237375843542).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='221' max='221' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [221/221 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▂▆▇▇▇██▇███████████████████████████████</td></tr><tr><td>eval/loss</td><td>██▆▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▁▃▂▄▁▃▇▃▃▃▃▇▃▄▃▄▃▂▄▃▄▄▃▄▅▄▄█▄▂▄▄▅▄▄▄▄▄▃</td></tr><tr><td>eval/samples_per_second</td><td>██▆▇▅█▆▂▆▆▆▆▂▆▅▆▅▅▇▅▅▅▅▆▅▄▅▅▁▅▆▅▅▄▅▅▅▅▅▆</td></tr><tr><td>eval/steps_per_second</td><td>██▆▇▅█▆▂▆▆▆▆▂▆▅▆▅▅▇▅▅▅▅▆▅▄▅▅▁▅▆▅▅▄▅▅▅▅▅▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▃▅███▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▇▅▄▃▃▃▂▃▃▃▂▃▂▂▂▂▃▂▁▁▂▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79412</td></tr><tr><td>eval/loss</td><td>0.50941</td></tr><tr><td>eval/runtime</td><td>5.8561</td></tr><tr><td>eval/samples_per_second</td><td>603.472</td></tr><tr><td>eval/steps_per_second</td><td>37.738</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>3436</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4778</td></tr><tr><td>train/total_flos</td><td>1977050665396800.0</td></tr><tr><td>train/train_loss</td><td>0.55805</td></tr><tr><td>train/train_runtime</td><td>751.4778</td></tr><tr><td>train/train_samples_per_second</td><td>73.136</td></tr><tr><td>train/train_steps_per_second</td><td>4.572</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">youthful-sweep-7</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/0nbu0cj6\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/0nbu0cj6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220427_212332-0nbu0cj6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ucn2og5g with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.745413609717602e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05604512442645369\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0174168727361444\n",
      "2022-04-27 21:36:47.582630: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220427_213646-ucn2og5g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/ucn2og5g\" target=\"_blank\">valiant-sweep-8</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088d07d029994cfb85db5c8ea9e1cd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 18\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 18\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4581\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4581' max='4581' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4581/4581 17:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.111800</td>\n",
       "      <td>1.097173</td>\n",
       "      <td>0.307231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.095700</td>\n",
       "      <td>1.072085</td>\n",
       "      <td>0.400466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.066200</td>\n",
       "      <td>1.030728</td>\n",
       "      <td>0.486810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.020700</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.577739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.937600</td>\n",
       "      <td>0.845433</td>\n",
       "      <td>0.658936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>0.751379</td>\n",
       "      <td>0.691291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.747600</td>\n",
       "      <td>0.670720</td>\n",
       "      <td>0.727335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.698100</td>\n",
       "      <td>0.637225</td>\n",
       "      <td>0.749691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.648100</td>\n",
       "      <td>0.609788</td>\n",
       "      <td>0.757532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.593820</td>\n",
       "      <td>0.759619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.612000</td>\n",
       "      <td>0.609270</td>\n",
       "      <td>0.744877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.619200</td>\n",
       "      <td>0.582410</td>\n",
       "      <td>0.767619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.585400</td>\n",
       "      <td>0.578787</td>\n",
       "      <td>0.765020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.603000</td>\n",
       "      <td>0.567583</td>\n",
       "      <td>0.767545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.541600</td>\n",
       "      <td>0.560792</td>\n",
       "      <td>0.779494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.628300</td>\n",
       "      <td>0.560472</td>\n",
       "      <td>0.762762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.603100</td>\n",
       "      <td>0.563810</td>\n",
       "      <td>0.762958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.597400</td>\n",
       "      <td>0.547722</td>\n",
       "      <td>0.773930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.553800</td>\n",
       "      <td>0.550176</td>\n",
       "      <td>0.768361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>0.541831</td>\n",
       "      <td>0.772749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.556600</td>\n",
       "      <td>0.544190</td>\n",
       "      <td>0.772041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.590400</td>\n",
       "      <td>0.548898</td>\n",
       "      <td>0.773892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.557400</td>\n",
       "      <td>0.549048</td>\n",
       "      <td>0.774905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.572600</td>\n",
       "      <td>0.533034</td>\n",
       "      <td>0.785632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.569000</td>\n",
       "      <td>0.552000</td>\n",
       "      <td>0.771260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.520900</td>\n",
       "      <td>0.542159</td>\n",
       "      <td>0.771741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.550600</td>\n",
       "      <td>0.530731</td>\n",
       "      <td>0.784156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.575200</td>\n",
       "      <td>0.546902</td>\n",
       "      <td>0.769256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.526418</td>\n",
       "      <td>0.781929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.525600</td>\n",
       "      <td>0.525931</td>\n",
       "      <td>0.785788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.495300</td>\n",
       "      <td>0.523541</td>\n",
       "      <td>0.778230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.455900</td>\n",
       "      <td>0.538338</td>\n",
       "      <td>0.786434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.461200</td>\n",
       "      <td>0.534176</td>\n",
       "      <td>0.779873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.540300</td>\n",
       "      <td>0.526415</td>\n",
       "      <td>0.781647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.495700</td>\n",
       "      <td>0.537505</td>\n",
       "      <td>0.779717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.493400</td>\n",
       "      <td>0.535860</td>\n",
       "      <td>0.782829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.471600</td>\n",
       "      <td>0.522390</td>\n",
       "      <td>0.789265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.452700</td>\n",
       "      <td>0.529423</td>\n",
       "      <td>0.788554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.511500</td>\n",
       "      <td>0.526224</td>\n",
       "      <td>0.788952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.464000</td>\n",
       "      <td>0.528062</td>\n",
       "      <td>0.792013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.529108</td>\n",
       "      <td>0.788037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.498800</td>\n",
       "      <td>0.525189</td>\n",
       "      <td>0.787426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.445500</td>\n",
       "      <td>0.530896</td>\n",
       "      <td>0.788813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.516338</td>\n",
       "      <td>0.790425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.504400</td>\n",
       "      <td>0.544146</td>\n",
       "      <td>0.783768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.459800</td>\n",
       "      <td>0.515203</td>\n",
       "      <td>0.798691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.528531</td>\n",
       "      <td>0.788649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.463000</td>\n",
       "      <td>0.517999</td>\n",
       "      <td>0.793940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.465800</td>\n",
       "      <td>0.511889</td>\n",
       "      <td>0.791682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.474100</td>\n",
       "      <td>0.521719</td>\n",
       "      <td>0.793175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.451400</td>\n",
       "      <td>0.528758</td>\n",
       "      <td>0.789405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.462800</td>\n",
       "      <td>0.519060</td>\n",
       "      <td>0.796057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.457300</td>\n",
       "      <td>0.521611</td>\n",
       "      <td>0.790277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.475700</td>\n",
       "      <td>0.515386</td>\n",
       "      <td>0.795439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.505900</td>\n",
       "      <td>0.509763</td>\n",
       "      <td>0.795711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.438200</td>\n",
       "      <td>0.515231</td>\n",
       "      <td>0.795381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.433800</td>\n",
       "      <td>0.507723</td>\n",
       "      <td>0.794152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.457600</td>\n",
       "      <td>0.524030</td>\n",
       "      <td>0.792139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.483900</td>\n",
       "      <td>0.512368</td>\n",
       "      <td>0.791772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.538500</td>\n",
       "      <td>0.503568</td>\n",
       "      <td>0.792418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.489200</td>\n",
       "      <td>0.518479</td>\n",
       "      <td>0.790025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.398500</td>\n",
       "      <td>0.523715</td>\n",
       "      <td>0.792376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.418300</td>\n",
       "      <td>0.522566</td>\n",
       "      <td>0.794938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.420300</td>\n",
       "      <td>0.527774</td>\n",
       "      <td>0.792922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.454800</td>\n",
       "      <td>0.514578</td>\n",
       "      <td>0.793946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>0.530299</td>\n",
       "      <td>0.791598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.409200</td>\n",
       "      <td>0.538587</td>\n",
       "      <td>0.789149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.414500</td>\n",
       "      <td>0.527477</td>\n",
       "      <td>0.791357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.441400</td>\n",
       "      <td>0.524357</td>\n",
       "      <td>0.791808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.421900</td>\n",
       "      <td>0.521873</td>\n",
       "      <td>0.796113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.396100</td>\n",
       "      <td>0.529988</td>\n",
       "      <td>0.793331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.424500</td>\n",
       "      <td>0.528957</td>\n",
       "      <td>0.794031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.451400</td>\n",
       "      <td>0.525024</td>\n",
       "      <td>0.791292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.419300</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.795784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.383900</td>\n",
       "      <td>0.533952</td>\n",
       "      <td>0.790589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.432400</td>\n",
       "      <td>0.522171</td>\n",
       "      <td>0.798765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.455600</td>\n",
       "      <td>0.528980</td>\n",
       "      <td>0.790689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.444800</td>\n",
       "      <td>0.524768</td>\n",
       "      <td>0.793136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.417700</td>\n",
       "      <td>0.525992</td>\n",
       "      <td>0.794831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.417900</td>\n",
       "      <td>0.521904</td>\n",
       "      <td>0.792573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.435500</td>\n",
       "      <td>0.516548</td>\n",
       "      <td>0.797757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.402400</td>\n",
       "      <td>0.516960</td>\n",
       "      <td>0.798185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.519908</td>\n",
       "      <td>0.794283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.433900</td>\n",
       "      <td>0.519651</td>\n",
       "      <td>0.795876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.395100</td>\n",
       "      <td>0.521920</td>\n",
       "      <td>0.795403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.432100</td>\n",
       "      <td>0.523048</td>\n",
       "      <td>0.795550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.423900</td>\n",
       "      <td>0.521408</td>\n",
       "      <td>0.794661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.407400</td>\n",
       "      <td>0.521189</td>\n",
       "      <td>0.795328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.419700</td>\n",
       "      <td>0.521401</td>\n",
       "      <td>0.795924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.406700</td>\n",
       "      <td>0.521831</td>\n",
       "      <td>0.794926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.462100</td>\n",
       "      <td>0.521759</td>\n",
       "      <td>0.793497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500 (score: 0.7961133263069914).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='197' max='197' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [197/197 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▄▆▇▇██▇████████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▇▅▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▂▃▃▃▃▃▄▃▄▄▄▄▄▄▃▄▃▃▄▅▃▄▄▃▃▄▅▄▃▄▄▃▄▄█▃▄▃▄</td></tr><tr><td>eval/samples_per_second</td><td>█▇▆▆▆▆▆▅▆▅▅▅▅▅▅▆▅▆▆▅▄▆▅▅▅▆▅▄▅▆▅▅▆▅▅▁▆▅▆▅</td></tr><tr><td>eval/steps_per_second</td><td>█▇▆▆▆▆▆▅▆▅▅▅▅▅▅▆▅▆▆▅▄▆▅▅▅▆▅▄▅▆▅▅▆▅▅▁▆▅▆▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▂▅████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▆▄▄▃▃▃▂▂▃▂▃▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79611</td></tr><tr><td>eval/loss</td><td>0.52187</td></tr><tr><td>eval/runtime</td><td>6.043</td></tr><tr><td>eval/samples_per_second</td><td>584.81</td></tr><tr><td>eval/steps_per_second</td><td>32.6</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>4581</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4621</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.52287</td></tr><tr><td>train/train_runtime</td><td>1067.03</td></tr><tr><td>train/train_samples_per_second</td><td>77.261</td></tr><tr><td>train/train_steps_per_second</td><td>4.293</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">valiant-sweep-8</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/ucn2og5g\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/ucn2og5g</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220427_213646-ucn2og5g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pxnw0vfv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.018328700784296e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05425467692768531\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0142138125444001\n",
      "2022-04-27 21:55:04.891894: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220427_215503-pxnw0vfv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/pxnw0vfv\" target=\"_blank\">bright-sweep-9</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b8de4fd12e42b99ab28e18a26e8cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 13\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 13\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6342\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6342' max='6342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6342/6342 22:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.096900</td>\n",
       "      <td>1.098272</td>\n",
       "      <td>0.305536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.092600</td>\n",
       "      <td>1.072134</td>\n",
       "      <td>0.404144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.075600</td>\n",
       "      <td>1.041770</td>\n",
       "      <td>0.468694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.034900</td>\n",
       "      <td>0.978495</td>\n",
       "      <td>0.562186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.945500</td>\n",
       "      <td>0.869553</td>\n",
       "      <td>0.661335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.832900</td>\n",
       "      <td>0.776484</td>\n",
       "      <td>0.674271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.819600</td>\n",
       "      <td>0.709000</td>\n",
       "      <td>0.692298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.727300</td>\n",
       "      <td>0.658379</td>\n",
       "      <td>0.726923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.652700</td>\n",
       "      <td>0.629732</td>\n",
       "      <td>0.746297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.668700</td>\n",
       "      <td>0.618813</td>\n",
       "      <td>0.745005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.658300</td>\n",
       "      <td>0.592574</td>\n",
       "      <td>0.763029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.605800</td>\n",
       "      <td>0.605832</td>\n",
       "      <td>0.749518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.629400</td>\n",
       "      <td>0.593596</td>\n",
       "      <td>0.754788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.644600</td>\n",
       "      <td>0.562006</td>\n",
       "      <td>0.775316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.600200</td>\n",
       "      <td>0.572477</td>\n",
       "      <td>0.768118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.599600</td>\n",
       "      <td>0.566876</td>\n",
       "      <td>0.770315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.555300</td>\n",
       "      <td>0.562694</td>\n",
       "      <td>0.765975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.578600</td>\n",
       "      <td>0.562007</td>\n",
       "      <td>0.770489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.569600</td>\n",
       "      <td>0.545807</td>\n",
       "      <td>0.782952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.564500</td>\n",
       "      <td>0.547474</td>\n",
       "      <td>0.781059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.563800</td>\n",
       "      <td>0.551853</td>\n",
       "      <td>0.778757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.614200</td>\n",
       "      <td>0.546115</td>\n",
       "      <td>0.776146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.520200</td>\n",
       "      <td>0.573345</td>\n",
       "      <td>0.767469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.650600</td>\n",
       "      <td>0.533257</td>\n",
       "      <td>0.783470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.571900</td>\n",
       "      <td>0.528934</td>\n",
       "      <td>0.783622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.496500</td>\n",
       "      <td>0.545111</td>\n",
       "      <td>0.781521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.555300</td>\n",
       "      <td>0.540597</td>\n",
       "      <td>0.786798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.604000</td>\n",
       "      <td>0.526602</td>\n",
       "      <td>0.788123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.555500</td>\n",
       "      <td>0.545707</td>\n",
       "      <td>0.775636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.576900</td>\n",
       "      <td>0.521861</td>\n",
       "      <td>0.787624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.546400</td>\n",
       "      <td>0.526359</td>\n",
       "      <td>0.784292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.534400</td>\n",
       "      <td>0.551264</td>\n",
       "      <td>0.781793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.524600</td>\n",
       "      <td>0.524313</td>\n",
       "      <td>0.789587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.596600</td>\n",
       "      <td>0.527072</td>\n",
       "      <td>0.786127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.527200</td>\n",
       "      <td>0.538232</td>\n",
       "      <td>0.785787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.519200</td>\n",
       "      <td>0.544174</td>\n",
       "      <td>0.776237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.556000</td>\n",
       "      <td>0.514206</td>\n",
       "      <td>0.791453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.573000</td>\n",
       "      <td>0.507234</td>\n",
       "      <td>0.792247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.504700</td>\n",
       "      <td>0.547265</td>\n",
       "      <td>0.772968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.481700</td>\n",
       "      <td>0.518290</td>\n",
       "      <td>0.787597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.494500</td>\n",
       "      <td>0.525405</td>\n",
       "      <td>0.792510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.504400</td>\n",
       "      <td>0.519327</td>\n",
       "      <td>0.795142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.484400</td>\n",
       "      <td>0.521292</td>\n",
       "      <td>0.791212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.408300</td>\n",
       "      <td>0.540326</td>\n",
       "      <td>0.792372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.424800</td>\n",
       "      <td>0.528255</td>\n",
       "      <td>0.794425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.456800</td>\n",
       "      <td>0.535445</td>\n",
       "      <td>0.792050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.498200</td>\n",
       "      <td>0.527863</td>\n",
       "      <td>0.794786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.461300</td>\n",
       "      <td>0.529555</td>\n",
       "      <td>0.788854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.473500</td>\n",
       "      <td>0.530538</td>\n",
       "      <td>0.793423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.475300</td>\n",
       "      <td>0.538404</td>\n",
       "      <td>0.785117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.526457</td>\n",
       "      <td>0.798211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.425900</td>\n",
       "      <td>0.563072</td>\n",
       "      <td>0.787350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.444600</td>\n",
       "      <td>0.540666</td>\n",
       "      <td>0.795737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.480800</td>\n",
       "      <td>0.530095</td>\n",
       "      <td>0.782915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.443600</td>\n",
       "      <td>0.521054</td>\n",
       "      <td>0.797610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.457600</td>\n",
       "      <td>0.532317</td>\n",
       "      <td>0.791912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.452100</td>\n",
       "      <td>0.518006</td>\n",
       "      <td>0.796814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.456500</td>\n",
       "      <td>0.525929</td>\n",
       "      <td>0.801463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.417100</td>\n",
       "      <td>0.553364</td>\n",
       "      <td>0.786424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.436700</td>\n",
       "      <td>0.527054</td>\n",
       "      <td>0.793092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.455900</td>\n",
       "      <td>0.513061</td>\n",
       "      <td>0.793280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.502200</td>\n",
       "      <td>0.529539</td>\n",
       "      <td>0.791788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.409000</td>\n",
       "      <td>0.542898</td>\n",
       "      <td>0.791354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.460700</td>\n",
       "      <td>0.519845</td>\n",
       "      <td>0.797862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.441100</td>\n",
       "      <td>0.533729</td>\n",
       "      <td>0.798113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.448700</td>\n",
       "      <td>0.530497</td>\n",
       "      <td>0.793866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.433800</td>\n",
       "      <td>0.535339</td>\n",
       "      <td>0.793593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.478200</td>\n",
       "      <td>0.512876</td>\n",
       "      <td>0.797901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.471000</td>\n",
       "      <td>0.513822</td>\n",
       "      <td>0.794971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.402900</td>\n",
       "      <td>0.523189</td>\n",
       "      <td>0.794352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.459300</td>\n",
       "      <td>0.532542</td>\n",
       "      <td>0.792340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.443100</td>\n",
       "      <td>0.523494</td>\n",
       "      <td>0.801704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.435600</td>\n",
       "      <td>0.534821</td>\n",
       "      <td>0.792194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.446000</td>\n",
       "      <td>0.530928</td>\n",
       "      <td>0.797490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.456500</td>\n",
       "      <td>0.520530</td>\n",
       "      <td>0.799042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.495700</td>\n",
       "      <td>0.508186</td>\n",
       "      <td>0.802821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.418800</td>\n",
       "      <td>0.522615</td>\n",
       "      <td>0.798505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.393600</td>\n",
       "      <td>0.529082</td>\n",
       "      <td>0.799118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.414500</td>\n",
       "      <td>0.513669</td>\n",
       "      <td>0.800990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.456900</td>\n",
       "      <td>0.524975</td>\n",
       "      <td>0.800792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.420400</td>\n",
       "      <td>0.512952</td>\n",
       "      <td>0.797009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.492400</td>\n",
       "      <td>0.507269</td>\n",
       "      <td>0.792539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.512200</td>\n",
       "      <td>0.503152</td>\n",
       "      <td>0.800238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.467500</td>\n",
       "      <td>0.518396</td>\n",
       "      <td>0.800732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.401200</td>\n",
       "      <td>0.516420</td>\n",
       "      <td>0.801477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.540690</td>\n",
       "      <td>0.805474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.547339</td>\n",
       "      <td>0.796305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>0.553958</td>\n",
       "      <td>0.794015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.381400</td>\n",
       "      <td>0.555581</td>\n",
       "      <td>0.796719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.535112</td>\n",
       "      <td>0.800438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.561047</td>\n",
       "      <td>0.797422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.317200</td>\n",
       "      <td>0.571379</td>\n",
       "      <td>0.792887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.367900</td>\n",
       "      <td>0.561156</td>\n",
       "      <td>0.793005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.344500</td>\n",
       "      <td>0.560700</td>\n",
       "      <td>0.798696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.403900</td>\n",
       "      <td>0.556304</td>\n",
       "      <td>0.793794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>0.564031</td>\n",
       "      <td>0.791957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.411200</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.794912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.331200</td>\n",
       "      <td>0.553839</td>\n",
       "      <td>0.791901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.381400</td>\n",
       "      <td>0.555495</td>\n",
       "      <td>0.797670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.373600</td>\n",
       "      <td>0.557788</td>\n",
       "      <td>0.797064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.430500</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.797704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.368900</td>\n",
       "      <td>0.543241</td>\n",
       "      <td>0.800816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.346800</td>\n",
       "      <td>0.551717</td>\n",
       "      <td>0.794480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.344300</td>\n",
       "      <td>0.566638</td>\n",
       "      <td>0.792829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.392400</td>\n",
       "      <td>0.547643</td>\n",
       "      <td>0.796364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.414700</td>\n",
       "      <td>0.544956</td>\n",
       "      <td>0.795447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.351800</td>\n",
       "      <td>0.558451</td>\n",
       "      <td>0.792088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.403100</td>\n",
       "      <td>0.548433</td>\n",
       "      <td>0.795351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.555670</td>\n",
       "      <td>0.793798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.411800</td>\n",
       "      <td>0.538898</td>\n",
       "      <td>0.798038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.362600</td>\n",
       "      <td>0.544026</td>\n",
       "      <td>0.796190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.388700</td>\n",
       "      <td>0.540583</td>\n",
       "      <td>0.797111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.363500</td>\n",
       "      <td>0.542805</td>\n",
       "      <td>0.796215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.350100</td>\n",
       "      <td>0.545067</td>\n",
       "      <td>0.798042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.372400</td>\n",
       "      <td>0.550917</td>\n",
       "      <td>0.798253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.359700</td>\n",
       "      <td>0.555180</td>\n",
       "      <td>0.798449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.437600</td>\n",
       "      <td>0.556106</td>\n",
       "      <td>0.794315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.373500</td>\n",
       "      <td>0.554944</td>\n",
       "      <td>0.796722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.376100</td>\n",
       "      <td>0.557344</td>\n",
       "      <td>0.797321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.346300</td>\n",
       "      <td>0.556109</td>\n",
       "      <td>0.798318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.387100</td>\n",
       "      <td>0.554680</td>\n",
       "      <td>0.798332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.335200</td>\n",
       "      <td>0.556967</td>\n",
       "      <td>0.798603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.388700</td>\n",
       "      <td>0.555103</td>\n",
       "      <td>0.798676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.333400</td>\n",
       "      <td>0.554244</td>\n",
       "      <td>0.799000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.419400</td>\n",
       "      <td>0.555231</td>\n",
       "      <td>0.797147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.378900</td>\n",
       "      <td>0.554614</td>\n",
       "      <td>0.797748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000 (score: 0.800792460758458).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='272' max='272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [272/272 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▅▆▇▇▇██████████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▇▃▂▂▂▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▂▂▁▁▂▂▂▁</td></tr><tr><td>eval/runtime</td><td>▂▄▂▂▃▃▃▄▂▃▃▃▁█▂▁▂▃▂▂▁▂▄▃▄▂▃▄▁▂▃▃▄▂▂▄▂▂▄█</td></tr><tr><td>eval/samples_per_second</td><td>▇▅▇▇▆▆▆▅▇▅▆▆█▁▇█▇▆▇▇█▇▅▆▅▇▆▅█▇▆▅▅▇▇▅▇▇▄▁</td></tr><tr><td>eval/steps_per_second</td><td>▇▅▇▇▆▆▆▅▇▅▆▆█▁▇█▇▆▇▇█▇▅▆▅▇▆▅█▇▆▅▅▇▇▅▇▇▄▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▂▅███▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▅▄▄▃▃▃▂▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.80079</td></tr><tr><td>eval/loss</td><td>0.52497</td></tr><tr><td>eval/runtime</td><td>6.3616</td></tr><tr><td>eval/samples_per_second</td><td>555.521</td></tr><tr><td>eval/steps_per_second</td><td>42.757</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>6342</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3789</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.4886</td></tr><tr><td>train/train_runtime</td><td>1372.6193</td></tr><tr><td>train/train_samples_per_second</td><td>60.06</td></tr><tr><td>train/train_steps_per_second</td><td>4.62</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">bright-sweep-9</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/pxnw0vfv\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/pxnw0vfv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220427_215503-pxnw0vfv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ffbnsspr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.039492714858574e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.07515335370495126\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.013953418890929535\n",
      "2022-04-27 22:18:28.443157: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220427_221827-ffbnsspr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/ffbnsspr\" target=\"_blank\">solar-sweep-10</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d8c1325f1e4198a387ba0c3e81a929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 22\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 22\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3750\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 16:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.106900</td>\n",
       "      <td>1.093102</td>\n",
       "      <td>0.328439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.091900</td>\n",
       "      <td>1.057577</td>\n",
       "      <td>0.448442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.043700</td>\n",
       "      <td>1.005191</td>\n",
       "      <td>0.497628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.983200</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.620364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.835400</td>\n",
       "      <td>0.750027</td>\n",
       "      <td>0.707329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.658488</td>\n",
       "      <td>0.723202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.660600</td>\n",
       "      <td>0.640073</td>\n",
       "      <td>0.731874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.585786</td>\n",
       "      <td>0.766606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.603700</td>\n",
       "      <td>0.604809</td>\n",
       "      <td>0.747001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.603200</td>\n",
       "      <td>0.568566</td>\n",
       "      <td>0.768983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.567900</td>\n",
       "      <td>0.569365</td>\n",
       "      <td>0.778667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.577200</td>\n",
       "      <td>0.565275</td>\n",
       "      <td>0.770535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.574800</td>\n",
       "      <td>0.566001</td>\n",
       "      <td>0.765062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.599900</td>\n",
       "      <td>0.549578</td>\n",
       "      <td>0.773396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.559000</td>\n",
       "      <td>0.538187</td>\n",
       "      <td>0.783434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.545600</td>\n",
       "      <td>0.536358</td>\n",
       "      <td>0.779001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.590100</td>\n",
       "      <td>0.529120</td>\n",
       "      <td>0.783090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.560100</td>\n",
       "      <td>0.537804</td>\n",
       "      <td>0.777496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.548000</td>\n",
       "      <td>0.546546</td>\n",
       "      <td>0.780147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.561100</td>\n",
       "      <td>0.541168</td>\n",
       "      <td>0.778873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.526800</td>\n",
       "      <td>0.542656</td>\n",
       "      <td>0.776433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.536300</td>\n",
       "      <td>0.519089</td>\n",
       "      <td>0.787268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.537900</td>\n",
       "      <td>0.545860</td>\n",
       "      <td>0.774593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.490400</td>\n",
       "      <td>0.524984</td>\n",
       "      <td>0.789133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.529200</td>\n",
       "      <td>0.530775</td>\n",
       "      <td>0.783579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.435900</td>\n",
       "      <td>0.535549</td>\n",
       "      <td>0.786759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.450100</td>\n",
       "      <td>0.529622</td>\n",
       "      <td>0.788242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.529100</td>\n",
       "      <td>0.520059</td>\n",
       "      <td>0.786602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.466300</td>\n",
       "      <td>0.515602</td>\n",
       "      <td>0.791784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.467000</td>\n",
       "      <td>0.513788</td>\n",
       "      <td>0.790511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.525787</td>\n",
       "      <td>0.794332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.494200</td>\n",
       "      <td>0.509810</td>\n",
       "      <td>0.794087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.443400</td>\n",
       "      <td>0.525485</td>\n",
       "      <td>0.789465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.475700</td>\n",
       "      <td>0.525863</td>\n",
       "      <td>0.782554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.449500</td>\n",
       "      <td>0.546872</td>\n",
       "      <td>0.786988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.441000</td>\n",
       "      <td>0.516500</td>\n",
       "      <td>0.789367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.487400</td>\n",
       "      <td>0.546671</td>\n",
       "      <td>0.780070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>0.520870</td>\n",
       "      <td>0.792811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.511116</td>\n",
       "      <td>0.796633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.470300</td>\n",
       "      <td>0.506163</td>\n",
       "      <td>0.797303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>0.512954</td>\n",
       "      <td>0.793382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>0.514991</td>\n",
       "      <td>0.794025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.452400</td>\n",
       "      <td>0.516295</td>\n",
       "      <td>0.790783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.466100</td>\n",
       "      <td>0.510138</td>\n",
       "      <td>0.796956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.480100</td>\n",
       "      <td>0.504583</td>\n",
       "      <td>0.795544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.419900</td>\n",
       "      <td>0.506995</td>\n",
       "      <td>0.799448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.429300</td>\n",
       "      <td>0.509801</td>\n",
       "      <td>0.801025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>0.509789</td>\n",
       "      <td>0.795334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.507700</td>\n",
       "      <td>0.491661</td>\n",
       "      <td>0.799973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.481700</td>\n",
       "      <td>0.514818</td>\n",
       "      <td>0.791105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.375800</td>\n",
       "      <td>0.521589</td>\n",
       "      <td>0.800866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.405100</td>\n",
       "      <td>0.523802</td>\n",
       "      <td>0.796999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.394100</td>\n",
       "      <td>0.513817</td>\n",
       "      <td>0.802317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.524470</td>\n",
       "      <td>0.798185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.371500</td>\n",
       "      <td>0.530150</td>\n",
       "      <td>0.795572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.401400</td>\n",
       "      <td>0.528176</td>\n",
       "      <td>0.796046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.388700</td>\n",
       "      <td>0.527940</td>\n",
       "      <td>0.796929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.527352</td>\n",
       "      <td>0.794278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.521320</td>\n",
       "      <td>0.796476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.414100</td>\n",
       "      <td>0.519418</td>\n",
       "      <td>0.793211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.375100</td>\n",
       "      <td>0.526850</td>\n",
       "      <td>0.794076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.389900</td>\n",
       "      <td>0.523003</td>\n",
       "      <td>0.798674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.418400</td>\n",
       "      <td>0.521901</td>\n",
       "      <td>0.795404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.400500</td>\n",
       "      <td>0.524880</td>\n",
       "      <td>0.789898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.401300</td>\n",
       "      <td>0.517632</td>\n",
       "      <td>0.798444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.387800</td>\n",
       "      <td>0.518430</td>\n",
       "      <td>0.795147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.388500</td>\n",
       "      <td>0.518222</td>\n",
       "      <td>0.798484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.388000</td>\n",
       "      <td>0.521129</td>\n",
       "      <td>0.796433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.402800</td>\n",
       "      <td>0.523567</td>\n",
       "      <td>0.790627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.396100</td>\n",
       "      <td>0.521888</td>\n",
       "      <td>0.798008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.391600</td>\n",
       "      <td>0.520334</td>\n",
       "      <td>0.798237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>0.520656</td>\n",
       "      <td>0.795713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.520981</td>\n",
       "      <td>0.797955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.400100</td>\n",
       "      <td>0.521745</td>\n",
       "      <td>0.797285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.369100</td>\n",
       "      <td>0.521227</td>\n",
       "      <td>0.797273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500 (score: 0.7980075513745665).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='161' max='161' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [161/161 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▃▅▇▇███████████████████████████████████</td></tr><tr><td>eval/loss</td><td>██▆▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▅▃▅▆▅▄▄▅▄▃▄▆▄▅▅▃▄▆▃▅▆▃▄█▅▅▃▆▆▄▄▃▄▃▆▅▄▄▄</td></tr><tr><td>eval/samples_per_second</td><td>█▄▆▄▃▄▅▅▄▅▆▅▃▅▄▄▆▅▃▅▄▃▆▅▁▄▃▆▃▃▅▅▆▅▆▃▄▅▅▅</td></tr><tr><td>eval/steps_per_second</td><td>█▄▆▄▃▄▅▅▄▅▆▅▃▅▄▄▆▅▃▅▄▃▆▅▁▄▃▆▃▃▅▅▆▅▆▃▄▅▅▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▂▃▆███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▇▄▄▃▃▃▃▃▃▂▃▃▂▂▁▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79801</td></tr><tr><td>eval/loss</td><td>0.52189</td></tr><tr><td>eval/runtime</td><td>6.623</td></tr><tr><td>eval/samples_per_second</td><td>533.595</td></tr><tr><td>eval/steps_per_second</td><td>24.309</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>3750</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3691</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.50619</td></tr><tr><td>train/train_runtime</td><td>995.9014</td></tr><tr><td>train/train_samples_per_second</td><td>82.779</td></tr><tr><td>train/train_steps_per_second</td><td>3.765</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">solar-sweep-10</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/ffbnsspr\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/ffbnsspr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220427_221827-ffbnsspr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kmcnpgr1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.3386953053127e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.04160310660485734\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01664969329186402\n",
      "2022-04-27 22:35:46.066860: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220427_223544-kmcnpgr1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/kmcnpgr1\" target=\"_blank\">blooming-sweep-11</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fd301e6c4d464a9681bb6fb8aa26d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 21\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 21\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3927\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3927' max='3927' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3927/3927 15:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.102800</td>\n",
       "      <td>1.082577</td>\n",
       "      <td>0.372160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.068400</td>\n",
       "      <td>1.028374</td>\n",
       "      <td>0.503480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.991700</td>\n",
       "      <td>0.899063</td>\n",
       "      <td>0.637488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.849200</td>\n",
       "      <td>0.735387</td>\n",
       "      <td>0.717955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.730600</td>\n",
       "      <td>0.669372</td>\n",
       "      <td>0.725931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.652400</td>\n",
       "      <td>0.625006</td>\n",
       "      <td>0.750279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.596810</td>\n",
       "      <td>0.758182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.614300</td>\n",
       "      <td>0.571887</td>\n",
       "      <td>0.771679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.618600</td>\n",
       "      <td>0.566374</td>\n",
       "      <td>0.771540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.597200</td>\n",
       "      <td>0.570030</td>\n",
       "      <td>0.768496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.580800</td>\n",
       "      <td>0.553241</td>\n",
       "      <td>0.775792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.573900</td>\n",
       "      <td>0.540444</td>\n",
       "      <td>0.784461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.559100</td>\n",
       "      <td>0.534857</td>\n",
       "      <td>0.786350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.555600</td>\n",
       "      <td>0.566049</td>\n",
       "      <td>0.766879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.616000</td>\n",
       "      <td>0.527815</td>\n",
       "      <td>0.786609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>0.542992</td>\n",
       "      <td>0.779742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.580900</td>\n",
       "      <td>0.529556</td>\n",
       "      <td>0.787794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.547800</td>\n",
       "      <td>0.528448</td>\n",
       "      <td>0.787279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.562400</td>\n",
       "      <td>0.516240</td>\n",
       "      <td>0.790427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.556000</td>\n",
       "      <td>0.534177</td>\n",
       "      <td>0.783669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.553700</td>\n",
       "      <td>0.541224</td>\n",
       "      <td>0.780936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.516700</td>\n",
       "      <td>0.530292</td>\n",
       "      <td>0.783758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.543400</td>\n",
       "      <td>0.509891</td>\n",
       "      <td>0.789754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.540300</td>\n",
       "      <td>0.521470</td>\n",
       "      <td>0.788646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.467400</td>\n",
       "      <td>0.512219</td>\n",
       "      <td>0.793922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.525500</td>\n",
       "      <td>0.514376</td>\n",
       "      <td>0.797296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.465500</td>\n",
       "      <td>0.523010</td>\n",
       "      <td>0.792444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.430400</td>\n",
       "      <td>0.512055</td>\n",
       "      <td>0.798623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.504400</td>\n",
       "      <td>0.526084</td>\n",
       "      <td>0.797551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.466500</td>\n",
       "      <td>0.509756</td>\n",
       "      <td>0.792319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.462100</td>\n",
       "      <td>0.536229</td>\n",
       "      <td>0.786681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.452500</td>\n",
       "      <td>0.533258</td>\n",
       "      <td>0.793691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.513411</td>\n",
       "      <td>0.802901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.474800</td>\n",
       "      <td>0.508975</td>\n",
       "      <td>0.800070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.464600</td>\n",
       "      <td>0.516883</td>\n",
       "      <td>0.797061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.475300</td>\n",
       "      <td>0.522576</td>\n",
       "      <td>0.792936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>0.511508</td>\n",
       "      <td>0.798905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.483800</td>\n",
       "      <td>0.499006</td>\n",
       "      <td>0.797710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.450600</td>\n",
       "      <td>0.523117</td>\n",
       "      <td>0.794377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.439300</td>\n",
       "      <td>0.513325</td>\n",
       "      <td>0.799884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.455000</td>\n",
       "      <td>0.511850</td>\n",
       "      <td>0.797315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.438000</td>\n",
       "      <td>0.501339</td>\n",
       "      <td>0.800146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.443400</td>\n",
       "      <td>0.518045</td>\n",
       "      <td>0.793146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>0.524666</td>\n",
       "      <td>0.799790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.444600</td>\n",
       "      <td>0.515935</td>\n",
       "      <td>0.794743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.457800</td>\n",
       "      <td>0.515400</td>\n",
       "      <td>0.798298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.492800</td>\n",
       "      <td>0.493199</td>\n",
       "      <td>0.801974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.425300</td>\n",
       "      <td>0.501063</td>\n",
       "      <td>0.800517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>0.502077</td>\n",
       "      <td>0.796386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.442000</td>\n",
       "      <td>0.511305</td>\n",
       "      <td>0.802027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.532600</td>\n",
       "      <td>0.494555</td>\n",
       "      <td>0.796099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.478300</td>\n",
       "      <td>0.506246</td>\n",
       "      <td>0.798507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.402300</td>\n",
       "      <td>0.512634</td>\n",
       "      <td>0.799733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.517908</td>\n",
       "      <td>0.803495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.387600</td>\n",
       "      <td>0.531313</td>\n",
       "      <td>0.800607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.419500</td>\n",
       "      <td>0.519068</td>\n",
       "      <td>0.803094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.323500</td>\n",
       "      <td>0.538553</td>\n",
       "      <td>0.800303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.397000</td>\n",
       "      <td>0.529404</td>\n",
       "      <td>0.802120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.408900</td>\n",
       "      <td>0.523975</td>\n",
       "      <td>0.800425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.394900</td>\n",
       "      <td>0.528144</td>\n",
       "      <td>0.800499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.366100</td>\n",
       "      <td>0.525007</td>\n",
       "      <td>0.796999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.391900</td>\n",
       "      <td>0.525974</td>\n",
       "      <td>0.798476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.418800</td>\n",
       "      <td>0.517455</td>\n",
       "      <td>0.798833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>0.528984</td>\n",
       "      <td>0.798388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.417800</td>\n",
       "      <td>0.517942</td>\n",
       "      <td>0.802788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.429900</td>\n",
       "      <td>0.521928</td>\n",
       "      <td>0.800260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.396900</td>\n",
       "      <td>0.519858</td>\n",
       "      <td>0.798726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.408800</td>\n",
       "      <td>0.513865</td>\n",
       "      <td>0.802179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.413300</td>\n",
       "      <td>0.514167</td>\n",
       "      <td>0.800170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.378600</td>\n",
       "      <td>0.511438</td>\n",
       "      <td>0.804220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.368100</td>\n",
       "      <td>0.515338</td>\n",
       "      <td>0.803092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.517227</td>\n",
       "      <td>0.800468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.385400</td>\n",
       "      <td>0.518637</td>\n",
       "      <td>0.800975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.377500</td>\n",
       "      <td>0.520320</td>\n",
       "      <td>0.801586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.404500</td>\n",
       "      <td>0.517623</td>\n",
       "      <td>0.803432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.379500</td>\n",
       "      <td>0.518090</td>\n",
       "      <td>0.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.354700</td>\n",
       "      <td>0.518575</td>\n",
       "      <td>0.803660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.411900</td>\n",
       "      <td>0.518774</td>\n",
       "      <td>0.802505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500 (score: 0.8042200756788643).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 21\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='169' max='169' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [169/169 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▅▇▇▇███████████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▆▃▂▂▂▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▂▂▃▁▁▆▂▂▃▂▄▂▂▂▂▂▂▂█▃▂▃▃▂▁▂▃▂▂▂▁▅▃▃▂▂▂▂▃</td></tr><tr><td>eval/samples_per_second</td><td>█▇▇▆██▃▇▆▆▇▅▇▇▇▆▇▇▇▁▆▇▅▆▇█▇▆▆▇▇█▄▆▆▇▇▆▇▆</td></tr><tr><td>eval/steps_per_second</td><td>█▇▇▆██▃▇▆▆▇▅▇▇▇▆▇▇▇▁▆▇▅▆▇█▇▆▆▇▇█▄▆▆▇▇▆▇▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▃▅███▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▆▄▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.80422</td></tr><tr><td>eval/loss</td><td>0.51144</td></tr><tr><td>eval/runtime</td><td>5.7689</td></tr><tr><td>eval/samples_per_second</td><td>612.595</td></tr><tr><td>eval/steps_per_second</td><td>29.295</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>3927</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4119</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.49596</td></tr><tr><td>train/train_runtime</td><td>924.4763</td></tr><tr><td>train/train_samples_per_second</td><td>89.175</td></tr><tr><td>train/train_steps_per_second</td><td>4.248</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">blooming-sweep-11</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/kmcnpgr1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/kmcnpgr1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220427_223544-kmcnpgr1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y8u2frff with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.828077271706537e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 13\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.012952487355606376\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.016366189106043087\n",
      "2022-04-27 22:51:42.113924: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220427_225140-y8u2frff</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/y8u2frff\" target=\"_blank\">different-sweep-12</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e030a5057b48497e9515f635c141637f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 13\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 13\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6342\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6342' max='6342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6342/6342 22:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.087000</td>\n",
       "      <td>1.065450</td>\n",
       "      <td>0.418331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.040400</td>\n",
       "      <td>0.969051</td>\n",
       "      <td>0.521171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.935800</td>\n",
       "      <td>0.850970</td>\n",
       "      <td>0.576797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.827300</td>\n",
       "      <td>0.838489</td>\n",
       "      <td>0.594271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.763900</td>\n",
       "      <td>0.693429</td>\n",
       "      <td>0.719470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.693300</td>\n",
       "      <td>0.647055</td>\n",
       "      <td>0.734767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.715700</td>\n",
       "      <td>0.674990</td>\n",
       "      <td>0.710836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.608232</td>\n",
       "      <td>0.749307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.616300</td>\n",
       "      <td>0.612552</td>\n",
       "      <td>0.747283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.645900</td>\n",
       "      <td>0.598676</td>\n",
       "      <td>0.754649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.632900</td>\n",
       "      <td>0.576258</td>\n",
       "      <td>0.772011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.592200</td>\n",
       "      <td>0.576094</td>\n",
       "      <td>0.760268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.608200</td>\n",
       "      <td>0.561863</td>\n",
       "      <td>0.769756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.622800</td>\n",
       "      <td>0.537828</td>\n",
       "      <td>0.779701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.579000</td>\n",
       "      <td>0.548815</td>\n",
       "      <td>0.773054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.588700</td>\n",
       "      <td>0.551865</td>\n",
       "      <td>0.772857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.543500</td>\n",
       "      <td>0.556770</td>\n",
       "      <td>0.773035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.560200</td>\n",
       "      <td>0.551390</td>\n",
       "      <td>0.776740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>0.534446</td>\n",
       "      <td>0.786207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.554100</td>\n",
       "      <td>0.526354</td>\n",
       "      <td>0.787265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.540200</td>\n",
       "      <td>0.526662</td>\n",
       "      <td>0.783811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.576900</td>\n",
       "      <td>0.537251</td>\n",
       "      <td>0.774119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.517800</td>\n",
       "      <td>0.557529</td>\n",
       "      <td>0.776755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.628000</td>\n",
       "      <td>0.523190</td>\n",
       "      <td>0.786359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.560900</td>\n",
       "      <td>0.519775</td>\n",
       "      <td>0.790711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.484200</td>\n",
       "      <td>0.534206</td>\n",
       "      <td>0.790405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.535987</td>\n",
       "      <td>0.789194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.595800</td>\n",
       "      <td>0.520375</td>\n",
       "      <td>0.793134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.539200</td>\n",
       "      <td>0.531482</td>\n",
       "      <td>0.786753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.578100</td>\n",
       "      <td>0.512016</td>\n",
       "      <td>0.789748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.536100</td>\n",
       "      <td>0.518489</td>\n",
       "      <td>0.788871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.538100</td>\n",
       "      <td>0.535879</td>\n",
       "      <td>0.790227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.513400</td>\n",
       "      <td>0.524252</td>\n",
       "      <td>0.790730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.588200</td>\n",
       "      <td>0.523298</td>\n",
       "      <td>0.787427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.521300</td>\n",
       "      <td>0.526256</td>\n",
       "      <td>0.789394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.537997</td>\n",
       "      <td>0.776965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.550200</td>\n",
       "      <td>0.508709</td>\n",
       "      <td>0.794765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.570700</td>\n",
       "      <td>0.508353</td>\n",
       "      <td>0.792418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.499600</td>\n",
       "      <td>0.541601</td>\n",
       "      <td>0.775237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.470800</td>\n",
       "      <td>0.512883</td>\n",
       "      <td>0.787028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.491600</td>\n",
       "      <td>0.522015</td>\n",
       "      <td>0.791788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.503500</td>\n",
       "      <td>0.517605</td>\n",
       "      <td>0.791867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.466800</td>\n",
       "      <td>0.515858</td>\n",
       "      <td>0.791513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.407200</td>\n",
       "      <td>0.544315</td>\n",
       "      <td>0.793145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.429700</td>\n",
       "      <td>0.513049</td>\n",
       "      <td>0.795672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.449700</td>\n",
       "      <td>0.538881</td>\n",
       "      <td>0.793849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.497000</td>\n",
       "      <td>0.529296</td>\n",
       "      <td>0.794449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.461900</td>\n",
       "      <td>0.513860</td>\n",
       "      <td>0.789766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.454900</td>\n",
       "      <td>0.519843</td>\n",
       "      <td>0.796132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.458700</td>\n",
       "      <td>0.539671</td>\n",
       "      <td>0.786307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.430200</td>\n",
       "      <td>0.525367</td>\n",
       "      <td>0.796546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.421600</td>\n",
       "      <td>0.550777</td>\n",
       "      <td>0.793013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.435800</td>\n",
       "      <td>0.537746</td>\n",
       "      <td>0.800453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.479100</td>\n",
       "      <td>0.523453</td>\n",
       "      <td>0.787570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.436100</td>\n",
       "      <td>0.521686</td>\n",
       "      <td>0.796823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.442800</td>\n",
       "      <td>0.524444</td>\n",
       "      <td>0.796430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.431800</td>\n",
       "      <td>0.514494</td>\n",
       "      <td>0.799252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.441500</td>\n",
       "      <td>0.523981</td>\n",
       "      <td>0.801348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.407700</td>\n",
       "      <td>0.559074</td>\n",
       "      <td>0.791946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.429000</td>\n",
       "      <td>0.517125</td>\n",
       "      <td>0.798703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.456900</td>\n",
       "      <td>0.508018</td>\n",
       "      <td>0.795216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.491500</td>\n",
       "      <td>0.510905</td>\n",
       "      <td>0.793981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.401800</td>\n",
       "      <td>0.544726</td>\n",
       "      <td>0.794806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.444700</td>\n",
       "      <td>0.517870</td>\n",
       "      <td>0.801057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.428900</td>\n",
       "      <td>0.530955</td>\n",
       "      <td>0.799774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.442100</td>\n",
       "      <td>0.518106</td>\n",
       "      <td>0.800405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.412900</td>\n",
       "      <td>0.537409</td>\n",
       "      <td>0.794712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.473000</td>\n",
       "      <td>0.506925</td>\n",
       "      <td>0.802696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.452400</td>\n",
       "      <td>0.517242</td>\n",
       "      <td>0.792405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.391800</td>\n",
       "      <td>0.525897</td>\n",
       "      <td>0.796650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.446900</td>\n",
       "      <td>0.534157</td>\n",
       "      <td>0.795604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.445400</td>\n",
       "      <td>0.520079</td>\n",
       "      <td>0.806023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.422600</td>\n",
       "      <td>0.529662</td>\n",
       "      <td>0.791607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.438300</td>\n",
       "      <td>0.527092</td>\n",
       "      <td>0.799986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.447200</td>\n",
       "      <td>0.512598</td>\n",
       "      <td>0.804795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.473500</td>\n",
       "      <td>0.497654</td>\n",
       "      <td>0.804137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.406400</td>\n",
       "      <td>0.519854</td>\n",
       "      <td>0.799521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.379400</td>\n",
       "      <td>0.524502</td>\n",
       "      <td>0.800228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.406300</td>\n",
       "      <td>0.505039</td>\n",
       "      <td>0.805660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.438600</td>\n",
       "      <td>0.517158</td>\n",
       "      <td>0.799675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>0.509653</td>\n",
       "      <td>0.803538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.477800</td>\n",
       "      <td>0.501073</td>\n",
       "      <td>0.797828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.501000</td>\n",
       "      <td>0.502019</td>\n",
       "      <td>0.802742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.467700</td>\n",
       "      <td>0.516815</td>\n",
       "      <td>0.795957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.396900</td>\n",
       "      <td>0.516789</td>\n",
       "      <td>0.804564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.333600</td>\n",
       "      <td>0.549821</td>\n",
       "      <td>0.801274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.359300</td>\n",
       "      <td>0.555041</td>\n",
       "      <td>0.794663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.553890</td>\n",
       "      <td>0.795210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>0.555385</td>\n",
       "      <td>0.796858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.422400</td>\n",
       "      <td>0.534337</td>\n",
       "      <td>0.803864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.314200</td>\n",
       "      <td>0.562273</td>\n",
       "      <td>0.793054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.318500</td>\n",
       "      <td>0.574055</td>\n",
       "      <td>0.793600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.357400</td>\n",
       "      <td>0.562551</td>\n",
       "      <td>0.795103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.326700</td>\n",
       "      <td>0.563016</td>\n",
       "      <td>0.802216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.386800</td>\n",
       "      <td>0.557646</td>\n",
       "      <td>0.798612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.360800</td>\n",
       "      <td>0.569087</td>\n",
       "      <td>0.793442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.403000</td>\n",
       "      <td>0.560289</td>\n",
       "      <td>0.800413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>0.554342</td>\n",
       "      <td>0.795105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.375200</td>\n",
       "      <td>0.552853</td>\n",
       "      <td>0.798257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.366200</td>\n",
       "      <td>0.552510</td>\n",
       "      <td>0.796469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.421900</td>\n",
       "      <td>0.545778</td>\n",
       "      <td>0.797400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.354700</td>\n",
       "      <td>0.541721</td>\n",
       "      <td>0.803518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.552523</td>\n",
       "      <td>0.799699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.323000</td>\n",
       "      <td>0.566888</td>\n",
       "      <td>0.792521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.386700</td>\n",
       "      <td>0.547947</td>\n",
       "      <td>0.799138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.399800</td>\n",
       "      <td>0.544260</td>\n",
       "      <td>0.796872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.556980</td>\n",
       "      <td>0.791455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.388800</td>\n",
       "      <td>0.547826</td>\n",
       "      <td>0.796731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.558181</td>\n",
       "      <td>0.793748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.403000</td>\n",
       "      <td>0.538871</td>\n",
       "      <td>0.804871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.542973</td>\n",
       "      <td>0.801931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.379100</td>\n",
       "      <td>0.542153</td>\n",
       "      <td>0.800295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.346400</td>\n",
       "      <td>0.544424</td>\n",
       "      <td>0.797555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.347700</td>\n",
       "      <td>0.545194</td>\n",
       "      <td>0.801813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.352300</td>\n",
       "      <td>0.551234</td>\n",
       "      <td>0.801422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.556132</td>\n",
       "      <td>0.801378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.417800</td>\n",
       "      <td>0.556881</td>\n",
       "      <td>0.799254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.364200</td>\n",
       "      <td>0.553935</td>\n",
       "      <td>0.798168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.557197</td>\n",
       "      <td>0.798977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.328400</td>\n",
       "      <td>0.556929</td>\n",
       "      <td>0.799964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.377400</td>\n",
       "      <td>0.555793</td>\n",
       "      <td>0.800012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.327400</td>\n",
       "      <td>0.558023</td>\n",
       "      <td>0.801129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.386900</td>\n",
       "      <td>0.555403</td>\n",
       "      <td>0.799736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.316900</td>\n",
       "      <td>0.555310</td>\n",
       "      <td>0.799259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.415300</td>\n",
       "      <td>0.556436</td>\n",
       "      <td>0.798835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.376200</td>\n",
       "      <td>0.555825</td>\n",
       "      <td>0.799990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500 (score: 0.8048706750799355).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='272' max='272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [272/272 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▄▆▇▇▇█▇███▇▇███████████████████████████</td></tr><tr><td>eval/loss</td><td>█▅▃▂▂▂▁▂▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▂▂▂▂▁▂▂▁▁▂▂▂▁</td></tr><tr><td>eval/runtime</td><td>▁▂▃▃▃▃▄▃▃▄▄▄█▅▃▄▃▄█▃▃▅▃▃▄▃▃▅▄▄▄▅▃▄▅▃▅▃█▃</td></tr><tr><td>eval/samples_per_second</td><td>█▇▆▆▆▆▅▆▆▅▅▅▁▄▆▅▆▄▁▆▆▄▆▆▅▆▆▄▅▅▅▄▆▅▄▆▄▅▁▆</td></tr><tr><td>eval/steps_per_second</td><td>█▇▆▆▆▆▅▆▆▅▅▅▁▄▆▅▆▄▁▆▆▄▆▆▅▆▆▄▅▅▅▄▆▅▄▆▄▅▁▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▄▄▃▃▃▂▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.80487</td></tr><tr><td>eval/loss</td><td>0.53887</td></tr><tr><td>eval/runtime</td><td>6.1811</td></tr><tr><td>eval/samples_per_second</td><td>571.743</td></tr><tr><td>eval/steps_per_second</td><td>44.005</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>6342</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3762</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.47094</td></tr><tr><td>train/train_runtime</td><td>1369.1396</td></tr><tr><td>train/train_samples_per_second</td><td>60.213</td></tr><tr><td>train/train_steps_per_second</td><td>4.632</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">different-sweep-12</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/y8u2frff\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/y8u2frff</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220427_225140-y8u2frff/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vh3th6hz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8.939890110565372e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.01779760925759143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.019620735960260555\n",
      "2022-04-27 23:15:11.792590: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220427_231510-vh3th6hz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/vh3th6hz\" target=\"_blank\">blooming-sweep-13</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fadc6625f94d429efa7071a615ecba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6870\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6870' max='6870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6870/6870 25:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.102200</td>\n",
       "      <td>1.080980</td>\n",
       "      <td>0.386098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.064200</td>\n",
       "      <td>1.056130</td>\n",
       "      <td>0.388815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.003200</td>\n",
       "      <td>0.943792</td>\n",
       "      <td>0.480749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.889700</td>\n",
       "      <td>0.798167</td>\n",
       "      <td>0.681305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.735600</td>\n",
       "      <td>0.714551</td>\n",
       "      <td>0.671733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.718300</td>\n",
       "      <td>0.649411</td>\n",
       "      <td>0.727319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.705600</td>\n",
       "      <td>0.625576</td>\n",
       "      <td>0.740492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.687800</td>\n",
       "      <td>0.623238</td>\n",
       "      <td>0.739187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.646600</td>\n",
       "      <td>0.599019</td>\n",
       "      <td>0.754900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.596542</td>\n",
       "      <td>0.758281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.636800</td>\n",
       "      <td>0.611603</td>\n",
       "      <td>0.744742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.628800</td>\n",
       "      <td>0.568151</td>\n",
       "      <td>0.773393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.582900</td>\n",
       "      <td>0.594728</td>\n",
       "      <td>0.749359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.576300</td>\n",
       "      <td>0.561897</td>\n",
       "      <td>0.773661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.615200</td>\n",
       "      <td>0.551972</td>\n",
       "      <td>0.779638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.586600</td>\n",
       "      <td>0.555704</td>\n",
       "      <td>0.780479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.570400</td>\n",
       "      <td>0.551050</td>\n",
       "      <td>0.779417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>0.540231</td>\n",
       "      <td>0.779812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.536400</td>\n",
       "      <td>0.542608</td>\n",
       "      <td>0.782139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.517100</td>\n",
       "      <td>0.556540</td>\n",
       "      <td>0.780316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.566600</td>\n",
       "      <td>0.534289</td>\n",
       "      <td>0.787061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.533500</td>\n",
       "      <td>0.560920</td>\n",
       "      <td>0.774992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.541700</td>\n",
       "      <td>0.560650</td>\n",
       "      <td>0.769252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.572600</td>\n",
       "      <td>0.549433</td>\n",
       "      <td>0.775587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.521200</td>\n",
       "      <td>0.573509</td>\n",
       "      <td>0.763008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>0.529230</td>\n",
       "      <td>0.784281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.568400</td>\n",
       "      <td>0.528805</td>\n",
       "      <td>0.789010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.501600</td>\n",
       "      <td>0.548458</td>\n",
       "      <td>0.782782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.541700</td>\n",
       "      <td>0.536164</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.610300</td>\n",
       "      <td>0.531466</td>\n",
       "      <td>0.782251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.550900</td>\n",
       "      <td>0.523016</td>\n",
       "      <td>0.786555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.527700</td>\n",
       "      <td>0.521663</td>\n",
       "      <td>0.786259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.560400</td>\n",
       "      <td>0.535162</td>\n",
       "      <td>0.782787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.540800</td>\n",
       "      <td>0.515588</td>\n",
       "      <td>0.784957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.585900</td>\n",
       "      <td>0.528619</td>\n",
       "      <td>0.783008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.535900</td>\n",
       "      <td>0.521659</td>\n",
       "      <td>0.789302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.591700</td>\n",
       "      <td>0.519761</td>\n",
       "      <td>0.786681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.535900</td>\n",
       "      <td>0.535939</td>\n",
       "      <td>0.782744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.500900</td>\n",
       "      <td>0.541272</td>\n",
       "      <td>0.779471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.532800</td>\n",
       "      <td>0.527782</td>\n",
       "      <td>0.789175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.547700</td>\n",
       "      <td>0.510282</td>\n",
       "      <td>0.785829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.539400</td>\n",
       "      <td>0.530453</td>\n",
       "      <td>0.779629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.450200</td>\n",
       "      <td>0.548923</td>\n",
       "      <td>0.780231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.472200</td>\n",
       "      <td>0.526923</td>\n",
       "      <td>0.793170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.555900</td>\n",
       "      <td>0.507541</td>\n",
       "      <td>0.790057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.504985</td>\n",
       "      <td>0.790480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.394100</td>\n",
       "      <td>0.531564</td>\n",
       "      <td>0.794148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.406000</td>\n",
       "      <td>0.552416</td>\n",
       "      <td>0.785829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.419100</td>\n",
       "      <td>0.536055</td>\n",
       "      <td>0.790550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.543263</td>\n",
       "      <td>0.794388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.502100</td>\n",
       "      <td>0.529247</td>\n",
       "      <td>0.794213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.444800</td>\n",
       "      <td>0.521526</td>\n",
       "      <td>0.794752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.469400</td>\n",
       "      <td>0.531789</td>\n",
       "      <td>0.794786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.458600</td>\n",
       "      <td>0.527753</td>\n",
       "      <td>0.792322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.514801</td>\n",
       "      <td>0.796721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.425500</td>\n",
       "      <td>0.548683</td>\n",
       "      <td>0.789785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.375200</td>\n",
       "      <td>0.539558</td>\n",
       "      <td>0.794443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.454500</td>\n",
       "      <td>0.534863</td>\n",
       "      <td>0.796273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.481500</td>\n",
       "      <td>0.511777</td>\n",
       "      <td>0.799787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.412000</td>\n",
       "      <td>0.545046</td>\n",
       "      <td>0.796426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>0.520332</td>\n",
       "      <td>0.795407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.472300</td>\n",
       "      <td>0.515828</td>\n",
       "      <td>0.796629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.432400</td>\n",
       "      <td>0.533055</td>\n",
       "      <td>0.793646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.417000</td>\n",
       "      <td>0.572968</td>\n",
       "      <td>0.787329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.419800</td>\n",
       "      <td>0.524675</td>\n",
       "      <td>0.792960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.477400</td>\n",
       "      <td>0.515429</td>\n",
       "      <td>0.791471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.513800</td>\n",
       "      <td>0.504199</td>\n",
       "      <td>0.796095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.390300</td>\n",
       "      <td>0.558020</td>\n",
       "      <td>0.784841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>0.531571</td>\n",
       "      <td>0.795630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.434600</td>\n",
       "      <td>0.532943</td>\n",
       "      <td>0.795363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.464100</td>\n",
       "      <td>0.516214</td>\n",
       "      <td>0.799559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.444200</td>\n",
       "      <td>0.517587</td>\n",
       "      <td>0.796519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>0.527283</td>\n",
       "      <td>0.792455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.549517</td>\n",
       "      <td>0.788694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.437200</td>\n",
       "      <td>0.533525</td>\n",
       "      <td>0.793215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.389300</td>\n",
       "      <td>0.536370</td>\n",
       "      <td>0.793123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.442500</td>\n",
       "      <td>0.530727</td>\n",
       "      <td>0.792721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.452100</td>\n",
       "      <td>0.526175</td>\n",
       "      <td>0.801241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.436900</td>\n",
       "      <td>0.531597</td>\n",
       "      <td>0.785227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.424200</td>\n",
       "      <td>0.550444</td>\n",
       "      <td>0.792465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.468900</td>\n",
       "      <td>0.511580</td>\n",
       "      <td>0.801694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.471400</td>\n",
       "      <td>0.511148</td>\n",
       "      <td>0.801596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.458300</td>\n",
       "      <td>0.509610</td>\n",
       "      <td>0.801103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.406200</td>\n",
       "      <td>0.520591</td>\n",
       "      <td>0.797386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>0.510610</td>\n",
       "      <td>0.800212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.391600</td>\n",
       "      <td>0.528960</td>\n",
       "      <td>0.798966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.417000</td>\n",
       "      <td>0.525489</td>\n",
       "      <td>0.794908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.458400</td>\n",
       "      <td>0.519027</td>\n",
       "      <td>0.796723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.475300</td>\n",
       "      <td>0.500887</td>\n",
       "      <td>0.795703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.490800</td>\n",
       "      <td>0.504797</td>\n",
       "      <td>0.797819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.474900</td>\n",
       "      <td>0.524674</td>\n",
       "      <td>0.790553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.420700</td>\n",
       "      <td>0.518312</td>\n",
       "      <td>0.793287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>0.547967</td>\n",
       "      <td>0.793061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.363000</td>\n",
       "      <td>0.555803</td>\n",
       "      <td>0.795157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.344100</td>\n",
       "      <td>0.558220</td>\n",
       "      <td>0.792175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.354800</td>\n",
       "      <td>0.562383</td>\n",
       "      <td>0.796181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.391000</td>\n",
       "      <td>0.535606</td>\n",
       "      <td>0.799584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.537225</td>\n",
       "      <td>0.802815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.312900</td>\n",
       "      <td>0.555239</td>\n",
       "      <td>0.798538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.313200</td>\n",
       "      <td>0.586130</td>\n",
       "      <td>0.787013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.329800</td>\n",
       "      <td>0.582455</td>\n",
       "      <td>0.789307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.362200</td>\n",
       "      <td>0.571129</td>\n",
       "      <td>0.794876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.401500</td>\n",
       "      <td>0.568613</td>\n",
       "      <td>0.793873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.363600</td>\n",
       "      <td>0.564432</td>\n",
       "      <td>0.795357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.400900</td>\n",
       "      <td>0.560142</td>\n",
       "      <td>0.796370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.334100</td>\n",
       "      <td>0.563705</td>\n",
       "      <td>0.792831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.370800</td>\n",
       "      <td>0.555219</td>\n",
       "      <td>0.791320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>0.547894</td>\n",
       "      <td>0.795845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.409300</td>\n",
       "      <td>0.550973</td>\n",
       "      <td>0.795616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.374700</td>\n",
       "      <td>0.560503</td>\n",
       "      <td>0.791913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.353100</td>\n",
       "      <td>0.559755</td>\n",
       "      <td>0.798338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.334300</td>\n",
       "      <td>0.578183</td>\n",
       "      <td>0.790949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.344200</td>\n",
       "      <td>0.570260</td>\n",
       "      <td>0.789273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.375500</td>\n",
       "      <td>0.559722</td>\n",
       "      <td>0.798274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.458300</td>\n",
       "      <td>0.551665</td>\n",
       "      <td>0.796083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.569861</td>\n",
       "      <td>0.786076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.403400</td>\n",
       "      <td>0.559405</td>\n",
       "      <td>0.787502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>0.567742</td>\n",
       "      <td>0.788566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.416800</td>\n",
       "      <td>0.551616</td>\n",
       "      <td>0.796266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.354600</td>\n",
       "      <td>0.548931</td>\n",
       "      <td>0.795489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.552469</td>\n",
       "      <td>0.793497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.331900</td>\n",
       "      <td>0.556802</td>\n",
       "      <td>0.793811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.389100</td>\n",
       "      <td>0.551421</td>\n",
       "      <td>0.794594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>0.552502</td>\n",
       "      <td>0.794768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.356500</td>\n",
       "      <td>0.555787</td>\n",
       "      <td>0.794546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.401800</td>\n",
       "      <td>0.560174</td>\n",
       "      <td>0.794179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>0.559402</td>\n",
       "      <td>0.793036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.343200</td>\n",
       "      <td>0.560756</td>\n",
       "      <td>0.794674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>0.400800</td>\n",
       "      <td>0.562311</td>\n",
       "      <td>0.794664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.340100</td>\n",
       "      <td>0.564094</td>\n",
       "      <td>0.795047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.561049</td>\n",
       "      <td>0.795103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.352200</td>\n",
       "      <td>0.561643</td>\n",
       "      <td>0.794823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>0.406600</td>\n",
       "      <td>0.560054</td>\n",
       "      <td>0.795926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.559339</td>\n",
       "      <td>0.795687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.404900</td>\n",
       "      <td>0.559779</td>\n",
       "      <td>0.796130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.429900</td>\n",
       "      <td>0.559769</td>\n",
       "      <td>0.794755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>0.333900</td>\n",
       "      <td>0.559413</td>\n",
       "      <td>0.795337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500 (score: 0.7978192465650367).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='295' max='295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [295/295 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▅▃▂▂▁▂▁▂▁▁▁▁▁▁▁▂▂▂▂▁▂▁▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>eval/runtime</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>████████████████████▁███████████████████</td></tr><tr><td>eval/steps_per_second</td><td>████████████████████▁███████████████████</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▇███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▃▄▃▄▃▃▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▃▁▂▁▁▂▂▂▁▁▁▁▁▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79782</td></tr><tr><td>eval/loss</td><td>0.5048</td></tr><tr><td>eval/runtime</td><td>6.4004</td></tr><tr><td>eval/samples_per_second</td><td>552.156</td></tr><tr><td>eval/steps_per_second</td><td>46.091</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>6870</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3339</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.47465</td></tr><tr><td>train/train_runtime</td><td>1535.6198</td></tr><tr><td>train/train_samples_per_second</td><td>53.685</td></tr><tr><td>train/train_steps_per_second</td><td>4.474</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">blooming-sweep-13</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/vh3th6hz\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/vh3th6hz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220427_231510-vh3th6hz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lxn8t4qi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.96240459515764e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.028794991188710845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01687520407386576\n",
      "2022-04-27 23:41:20.312812: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220427_234118-lxn8t4qi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/lxn8t4qi\" target=\"_blank\">proud-sweep-14</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0278b9ecb245d7bda7cabd65faadce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 11\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 11\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4998\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4998' max='4998' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4998/4998 18:15, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.112000</td>\n",
       "      <td>1.086702</td>\n",
       "      <td>0.355025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.061700</td>\n",
       "      <td>1.025974</td>\n",
       "      <td>0.461645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.014100</td>\n",
       "      <td>0.931987</td>\n",
       "      <td>0.584657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.877600</td>\n",
       "      <td>0.818456</td>\n",
       "      <td>0.656634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.756100</td>\n",
       "      <td>0.704049</td>\n",
       "      <td>0.718473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.721500</td>\n",
       "      <td>0.654817</td>\n",
       "      <td>0.737616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.684800</td>\n",
       "      <td>0.634653</td>\n",
       "      <td>0.743231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.729700</td>\n",
       "      <td>0.624705</td>\n",
       "      <td>0.748118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.676600</td>\n",
       "      <td>0.606950</td>\n",
       "      <td>0.752882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.642300</td>\n",
       "      <td>0.589314</td>\n",
       "      <td>0.762454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.582789</td>\n",
       "      <td>0.769478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.633200</td>\n",
       "      <td>0.605340</td>\n",
       "      <td>0.751973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.645100</td>\n",
       "      <td>0.563814</td>\n",
       "      <td>0.773528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.578200</td>\n",
       "      <td>0.597270</td>\n",
       "      <td>0.759668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.592100</td>\n",
       "      <td>0.546819</td>\n",
       "      <td>0.782776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.540168</td>\n",
       "      <td>0.785876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.598900</td>\n",
       "      <td>0.541346</td>\n",
       "      <td>0.782884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.559800</td>\n",
       "      <td>0.571918</td>\n",
       "      <td>0.766713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.607800</td>\n",
       "      <td>0.559825</td>\n",
       "      <td>0.772191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.555400</td>\n",
       "      <td>0.539614</td>\n",
       "      <td>0.780536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.556900</td>\n",
       "      <td>0.567187</td>\n",
       "      <td>0.775457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.519300</td>\n",
       "      <td>0.534050</td>\n",
       "      <td>0.785974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.563400</td>\n",
       "      <td>0.529227</td>\n",
       "      <td>0.785856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.509900</td>\n",
       "      <td>0.560612</td>\n",
       "      <td>0.770323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.535900</td>\n",
       "      <td>0.526909</td>\n",
       "      <td>0.780919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.570700</td>\n",
       "      <td>0.545467</td>\n",
       "      <td>0.772750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.503200</td>\n",
       "      <td>0.548196</td>\n",
       "      <td>0.780511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.648500</td>\n",
       "      <td>0.537728</td>\n",
       "      <td>0.779275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.551100</td>\n",
       "      <td>0.537986</td>\n",
       "      <td>0.781880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.515900</td>\n",
       "      <td>0.528931</td>\n",
       "      <td>0.796553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.522700</td>\n",
       "      <td>0.528686</td>\n",
       "      <td>0.788239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.539800</td>\n",
       "      <td>0.543543</td>\n",
       "      <td>0.786604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.603500</td>\n",
       "      <td>0.540465</td>\n",
       "      <td>0.779870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.537600</td>\n",
       "      <td>0.522839</td>\n",
       "      <td>0.792792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.546500</td>\n",
       "      <td>0.526806</td>\n",
       "      <td>0.789702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.592300</td>\n",
       "      <td>0.551956</td>\n",
       "      <td>0.770728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>0.522092</td>\n",
       "      <td>0.789863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.573500</td>\n",
       "      <td>0.535965</td>\n",
       "      <td>0.780260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>0.516762</td>\n",
       "      <td>0.794428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.600900</td>\n",
       "      <td>0.519425</td>\n",
       "      <td>0.787676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.535900</td>\n",
       "      <td>0.548640</td>\n",
       "      <td>0.783829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.489100</td>\n",
       "      <td>0.534224</td>\n",
       "      <td>0.783605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.488700</td>\n",
       "      <td>0.525702</td>\n",
       "      <td>0.787961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.544600</td>\n",
       "      <td>0.519443</td>\n",
       "      <td>0.792383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.563100</td>\n",
       "      <td>0.512476</td>\n",
       "      <td>0.788585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.501800</td>\n",
       "      <td>0.532709</td>\n",
       "      <td>0.781214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.478600</td>\n",
       "      <td>0.529742</td>\n",
       "      <td>0.786144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.436600</td>\n",
       "      <td>0.525581</td>\n",
       "      <td>0.795458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.529762</td>\n",
       "      <td>0.788398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.520700</td>\n",
       "      <td>0.527974</td>\n",
       "      <td>0.789200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.394100</td>\n",
       "      <td>0.525492</td>\n",
       "      <td>0.793571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.559050</td>\n",
       "      <td>0.788428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.435900</td>\n",
       "      <td>0.533817</td>\n",
       "      <td>0.798179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.418200</td>\n",
       "      <td>0.538193</td>\n",
       "      <td>0.792826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.530400</td>\n",
       "      <td>0.530308</td>\n",
       "      <td>0.791378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.455000</td>\n",
       "      <td>0.534117</td>\n",
       "      <td>0.792812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.468300</td>\n",
       "      <td>0.511893</td>\n",
       "      <td>0.794496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.447500</td>\n",
       "      <td>0.524693</td>\n",
       "      <td>0.800626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.436700</td>\n",
       "      <td>0.547193</td>\n",
       "      <td>0.789724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.457900</td>\n",
       "      <td>0.524422</td>\n",
       "      <td>0.796226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.432800</td>\n",
       "      <td>0.541261</td>\n",
       "      <td>0.789587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.537856</td>\n",
       "      <td>0.796946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.436400</td>\n",
       "      <td>0.535307</td>\n",
       "      <td>0.798294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.469300</td>\n",
       "      <td>0.533910</td>\n",
       "      <td>0.791709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.544056</td>\n",
       "      <td>0.797305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.428000</td>\n",
       "      <td>0.554873</td>\n",
       "      <td>0.791541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.461300</td>\n",
       "      <td>0.538586</td>\n",
       "      <td>0.794335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.435000</td>\n",
       "      <td>0.546804</td>\n",
       "      <td>0.788086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.433400</td>\n",
       "      <td>0.543925</td>\n",
       "      <td>0.797168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.435800</td>\n",
       "      <td>0.558229</td>\n",
       "      <td>0.790670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>0.538342</td>\n",
       "      <td>0.796024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.445100</td>\n",
       "      <td>0.527966</td>\n",
       "      <td>0.794356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.514500</td>\n",
       "      <td>0.511731</td>\n",
       "      <td>0.796023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.410300</td>\n",
       "      <td>0.531560</td>\n",
       "      <td>0.793001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.457200</td>\n",
       "      <td>0.528325</td>\n",
       "      <td>0.796113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.393000</td>\n",
       "      <td>0.542206</td>\n",
       "      <td>0.794193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.465300</td>\n",
       "      <td>0.547444</td>\n",
       "      <td>0.793358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.433700</td>\n",
       "      <td>0.529845</td>\n",
       "      <td>0.797898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.426700</td>\n",
       "      <td>0.535022</td>\n",
       "      <td>0.798216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.476900</td>\n",
       "      <td>0.523137</td>\n",
       "      <td>0.796883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>0.539150</td>\n",
       "      <td>0.792734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.434700</td>\n",
       "      <td>0.541278</td>\n",
       "      <td>0.794850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.396800</td>\n",
       "      <td>0.539961</td>\n",
       "      <td>0.794879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.428900</td>\n",
       "      <td>0.536475</td>\n",
       "      <td>0.792299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.421600</td>\n",
       "      <td>0.525495</td>\n",
       "      <td>0.796287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.426900</td>\n",
       "      <td>0.532145</td>\n",
       "      <td>0.796319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.413900</td>\n",
       "      <td>0.535210</td>\n",
       "      <td>0.795696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.450100</td>\n",
       "      <td>0.530619</td>\n",
       "      <td>0.797162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.454900</td>\n",
       "      <td>0.523723</td>\n",
       "      <td>0.798443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.461100</td>\n",
       "      <td>0.519735</td>\n",
       "      <td>0.797528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.394100</td>\n",
       "      <td>0.523241</td>\n",
       "      <td>0.796562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.364700</td>\n",
       "      <td>0.528774</td>\n",
       "      <td>0.796815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.422700</td>\n",
       "      <td>0.522660</td>\n",
       "      <td>0.795682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.394100</td>\n",
       "      <td>0.525798</td>\n",
       "      <td>0.796810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.424300</td>\n",
       "      <td>0.529128</td>\n",
       "      <td>0.796378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.439600</td>\n",
       "      <td>0.527962</td>\n",
       "      <td>0.795344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>0.526642</td>\n",
       "      <td>0.794784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.508300</td>\n",
       "      <td>0.524045</td>\n",
       "      <td>0.796334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.482800</td>\n",
       "      <td>0.524431</td>\n",
       "      <td>0.797431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500 (score: 0.7975283852939548).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='322' max='322' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [322/322 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▅▇▇███▇████████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▆▃▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▂▂▃▂▄▃▂▃▂▂▃▃▄▂▃▄▃▃▃▃▃▂▄▄▃▃▂█▂▃▃▄▃▃▂▂▃▃█</td></tr><tr><td>eval/samples_per_second</td><td>█▇▇▆▇▅▆▇▆▆▇▆▆▅▇▆▅▆▆▆▆▆▆▅▅▆▆▇▁▆▆▆▅▆▆▇▇▆▆▁</td></tr><tr><td>eval/steps_per_second</td><td>█▇▇▆▇▅▆▇▆▆▇▆▆▅▇▆▅▆▆▆▆▆▆▅▅▆▆▇▁▆▆▆▅▆▆▇▇▆▆▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▃███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▄▄▃▃▃▃▃▃▃▃▂▃▃▃▂▂▂▁▁▁▂▂▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79753</td></tr><tr><td>eval/loss</td><td>0.51974</td></tr><tr><td>eval/runtime</td><td>6.9336</td></tr><tr><td>eval/samples_per_second</td><td>509.692</td></tr><tr><td>eval/steps_per_second</td><td>46.441</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>4998</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4828</td></tr><tr><td>train/total_flos</td><td>1977050665396800.0</td></tr><tr><td>train/train_loss</td><td>0.52162</td></tr><tr><td>train/train_runtime</td><td>1095.4829</td></tr><tr><td>train/train_samples_per_second</td><td>50.17</td></tr><tr><td>train/train_steps_per_second</td><td>4.562</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">proud-sweep-14</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/lxn8t4qi\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/lxn8t4qi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220427_234118-lxn8t4qi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4dqz6064 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.739374396218434e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.044924916549757846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.00670969741446307\n",
      "2022-04-28 00:00:14.898559: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_000013-4dqz6064</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/4dqz6064\" target=\"_blank\">glorious-sweep-15</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa8c9bec2fe41afa974808e1a104007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 29\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 29\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2844\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2844' max='2844' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2844/2844 12:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.103900</td>\n",
       "      <td>1.069895</td>\n",
       "      <td>0.417689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.050600</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.535174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.924600</td>\n",
       "      <td>0.782095</td>\n",
       "      <td>0.702054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.733300</td>\n",
       "      <td>0.657005</td>\n",
       "      <td>0.726144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.678200</td>\n",
       "      <td>0.619531</td>\n",
       "      <td>0.752884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>0.586790</td>\n",
       "      <td>0.768395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.605800</td>\n",
       "      <td>0.577178</td>\n",
       "      <td>0.760969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.601500</td>\n",
       "      <td>0.560180</td>\n",
       "      <td>0.778457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.554326</td>\n",
       "      <td>0.780346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.567000</td>\n",
       "      <td>0.554908</td>\n",
       "      <td>0.772289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.605100</td>\n",
       "      <td>0.542326</td>\n",
       "      <td>0.777075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.553600</td>\n",
       "      <td>0.565750</td>\n",
       "      <td>0.770826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.579400</td>\n",
       "      <td>0.530122</td>\n",
       "      <td>0.780141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.563100</td>\n",
       "      <td>0.533381</td>\n",
       "      <td>0.775766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.563400</td>\n",
       "      <td>0.525113</td>\n",
       "      <td>0.787437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.537200</td>\n",
       "      <td>0.548526</td>\n",
       "      <td>0.775706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.568000</td>\n",
       "      <td>0.514737</td>\n",
       "      <td>0.788649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.501900</td>\n",
       "      <td>0.520526</td>\n",
       "      <td>0.790336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.529351</td>\n",
       "      <td>0.783980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.445400</td>\n",
       "      <td>0.535942</td>\n",
       "      <td>0.789769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.500700</td>\n",
       "      <td>0.531597</td>\n",
       "      <td>0.783397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.494600</td>\n",
       "      <td>0.511599</td>\n",
       "      <td>0.792738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.516601</td>\n",
       "      <td>0.794442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>0.519669</td>\n",
       "      <td>0.800356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.468500</td>\n",
       "      <td>0.529199</td>\n",
       "      <td>0.788499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.481300</td>\n",
       "      <td>0.527165</td>\n",
       "      <td>0.788581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.438000</td>\n",
       "      <td>0.513461</td>\n",
       "      <td>0.796108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.475700</td>\n",
       "      <td>0.546137</td>\n",
       "      <td>0.784912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.440900</td>\n",
       "      <td>0.522757</td>\n",
       "      <td>0.795758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.463500</td>\n",
       "      <td>0.508954</td>\n",
       "      <td>0.792149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.475800</td>\n",
       "      <td>0.521918</td>\n",
       "      <td>0.786821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.445700</td>\n",
       "      <td>0.514500</td>\n",
       "      <td>0.792042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.461500</td>\n",
       "      <td>0.519720</td>\n",
       "      <td>0.788436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.498200</td>\n",
       "      <td>0.502648</td>\n",
       "      <td>0.801271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.431800</td>\n",
       "      <td>0.512961</td>\n",
       "      <td>0.793609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.431800</td>\n",
       "      <td>0.524740</td>\n",
       "      <td>0.792158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.493400</td>\n",
       "      <td>0.502150</td>\n",
       "      <td>0.792476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.477800</td>\n",
       "      <td>0.509667</td>\n",
       "      <td>0.789977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.391000</td>\n",
       "      <td>0.517307</td>\n",
       "      <td>0.797196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.406500</td>\n",
       "      <td>0.519675</td>\n",
       "      <td>0.792549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.396500</td>\n",
       "      <td>0.525818</td>\n",
       "      <td>0.792675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.388100</td>\n",
       "      <td>0.528391</td>\n",
       "      <td>0.796417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.413600</td>\n",
       "      <td>0.525932</td>\n",
       "      <td>0.791000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.386900</td>\n",
       "      <td>0.523765</td>\n",
       "      <td>0.792836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.409600</td>\n",
       "      <td>0.521131</td>\n",
       "      <td>0.792805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.420400</td>\n",
       "      <td>0.515357</td>\n",
       "      <td>0.799437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.518228</td>\n",
       "      <td>0.791911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.411500</td>\n",
       "      <td>0.526521</td>\n",
       "      <td>0.786927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.415400</td>\n",
       "      <td>0.522865</td>\n",
       "      <td>0.789923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.417600</td>\n",
       "      <td>0.514729</td>\n",
       "      <td>0.796061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.387400</td>\n",
       "      <td>0.513965</td>\n",
       "      <td>0.797942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.379000</td>\n",
       "      <td>0.518283</td>\n",
       "      <td>0.796890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.518925</td>\n",
       "      <td>0.796563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.413100</td>\n",
       "      <td>0.520070</td>\n",
       "      <td>0.792825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.388200</td>\n",
       "      <td>0.519156</td>\n",
       "      <td>0.795255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.400600</td>\n",
       "      <td>0.520049</td>\n",
       "      <td>0.796359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500 (score: 0.7960611829776058).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 29\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='122' max='122' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [122/122 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▃▆▇▇███▇███████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▇▄▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▁▃▂▄▄▃▄▄▃▄▃▃▄▄▃▄▃▃▃▃▃▃▄▃▄▃▅▄█▄▄▄▄▃▅▃▄▄▆</td></tr><tr><td>eval/samples_per_second</td><td>██▆▇▅▅▆▅▅▆▅▆▆▅▅▆▅▆▆▆▆▆▆▅▆▅▆▄▅▁▅▅▅▅▆▄▆▅▄▃</td></tr><tr><td>eval/steps_per_second</td><td>██▆▇▅▅▆▅▅▆▅▆▆▅▅▆▅▆▆▆▆▆▆▅▆▅▆▄▅▁▅▅▅▅▆▄▆▅▄▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▄▆███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▄▄▃▃▃▃▃▃▂▃▂▂▂▂▂▂▁▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79606</td></tr><tr><td>eval/loss</td><td>0.51473</td></tr><tr><td>eval/runtime</td><td>6.0737</td></tr><tr><td>eval/samples_per_second</td><td>581.857</td></tr><tr><td>eval/steps_per_second</td><td>20.087</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>2844</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4006</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.50812</td></tr><tr><td>train/train_runtime</td><td>768.5847</td></tr><tr><td>train/train_samples_per_second</td><td>107.262</td></tr><tr><td>train/train_steps_per_second</td><td>3.7</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">glorious-sweep-15</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/4dqz6064\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/4dqz6064</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_000013-4dqz6064/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: svd1b1o3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8.600787737791735e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.034674461693173196\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.006164235104078892\n",
      "2022-04-28 00:13:32.306799: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_001330-svd1b1o3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/svd1b1o3\" target=\"_blank\">restful-sweep-16</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5e088cf0f240659fad5122d0649a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 17\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 17\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4851\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4851' max='4851' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4851/4851 18:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.105200</td>\n",
       "      <td>1.088298</td>\n",
       "      <td>0.351016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.078500</td>\n",
       "      <td>1.040350</td>\n",
       "      <td>0.471462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.015300</td>\n",
       "      <td>0.947666</td>\n",
       "      <td>0.603146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.883200</td>\n",
       "      <td>0.785028</td>\n",
       "      <td>0.671424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.784300</td>\n",
       "      <td>0.684391</td>\n",
       "      <td>0.731441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.706100</td>\n",
       "      <td>0.636845</td>\n",
       "      <td>0.741881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.651200</td>\n",
       "      <td>0.616124</td>\n",
       "      <td>0.752317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.657500</td>\n",
       "      <td>0.605086</td>\n",
       "      <td>0.753842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.637700</td>\n",
       "      <td>0.587937</td>\n",
       "      <td>0.764206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.593100</td>\n",
       "      <td>0.575207</td>\n",
       "      <td>0.762624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.617700</td>\n",
       "      <td>0.578185</td>\n",
       "      <td>0.768746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.589600</td>\n",
       "      <td>0.552006</td>\n",
       "      <td>0.777781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.578200</td>\n",
       "      <td>0.556719</td>\n",
       "      <td>0.773359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.551400</td>\n",
       "      <td>0.574102</td>\n",
       "      <td>0.767252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.581300</td>\n",
       "      <td>0.538580</td>\n",
       "      <td>0.785436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.542200</td>\n",
       "      <td>0.536661</td>\n",
       "      <td>0.783952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>0.547497</td>\n",
       "      <td>0.771475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.582100</td>\n",
       "      <td>0.542009</td>\n",
       "      <td>0.774978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.579500</td>\n",
       "      <td>0.532326</td>\n",
       "      <td>0.780150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.537600</td>\n",
       "      <td>0.535147</td>\n",
       "      <td>0.788209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.585400</td>\n",
       "      <td>0.533876</td>\n",
       "      <td>0.785698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.573300</td>\n",
       "      <td>0.520989</td>\n",
       "      <td>0.787172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.555500</td>\n",
       "      <td>0.522457</td>\n",
       "      <td>0.791756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>0.527089</td>\n",
       "      <td>0.785143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.568400</td>\n",
       "      <td>0.518184</td>\n",
       "      <td>0.784807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.568700</td>\n",
       "      <td>0.523181</td>\n",
       "      <td>0.785448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.524100</td>\n",
       "      <td>0.524564</td>\n",
       "      <td>0.787775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.522900</td>\n",
       "      <td>0.518820</td>\n",
       "      <td>0.786598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.571100</td>\n",
       "      <td>0.511175</td>\n",
       "      <td>0.788992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.491100</td>\n",
       "      <td>0.516337</td>\n",
       "      <td>0.792676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.488000</td>\n",
       "      <td>0.528682</td>\n",
       "      <td>0.785622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.527300</td>\n",
       "      <td>0.508957</td>\n",
       "      <td>0.789029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.467800</td>\n",
       "      <td>0.527053</td>\n",
       "      <td>0.783687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.446800</td>\n",
       "      <td>0.526576</td>\n",
       "      <td>0.791695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.533792</td>\n",
       "      <td>0.785101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.527800</td>\n",
       "      <td>0.521532</td>\n",
       "      <td>0.788185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>0.513212</td>\n",
       "      <td>0.788905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.484700</td>\n",
       "      <td>0.531893</td>\n",
       "      <td>0.788427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>0.510608</td>\n",
       "      <td>0.791297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.421100</td>\n",
       "      <td>0.528712</td>\n",
       "      <td>0.795922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>0.522839</td>\n",
       "      <td>0.795651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.464500</td>\n",
       "      <td>0.515274</td>\n",
       "      <td>0.793766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.511278</td>\n",
       "      <td>0.795035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.467100</td>\n",
       "      <td>0.531671</td>\n",
       "      <td>0.782179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.464700</td>\n",
       "      <td>0.531269</td>\n",
       "      <td>0.793060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.447100</td>\n",
       "      <td>0.509273</td>\n",
       "      <td>0.794824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.494600</td>\n",
       "      <td>0.501415</td>\n",
       "      <td>0.796068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.534796</td>\n",
       "      <td>0.787125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.452300</td>\n",
       "      <td>0.515506</td>\n",
       "      <td>0.794724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.440400</td>\n",
       "      <td>0.513237</td>\n",
       "      <td>0.796624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.429900</td>\n",
       "      <td>0.516097</td>\n",
       "      <td>0.790491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>0.503347</td>\n",
       "      <td>0.794579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.451100</td>\n",
       "      <td>0.516530</td>\n",
       "      <td>0.789704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.417400</td>\n",
       "      <td>0.530875</td>\n",
       "      <td>0.789738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.448900</td>\n",
       "      <td>0.515788</td>\n",
       "      <td>0.799173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.465900</td>\n",
       "      <td>0.506681</td>\n",
       "      <td>0.790837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>0.512599</td>\n",
       "      <td>0.795855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.498100</td>\n",
       "      <td>0.503265</td>\n",
       "      <td>0.794419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.414700</td>\n",
       "      <td>0.512793</td>\n",
       "      <td>0.796084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.424600</td>\n",
       "      <td>0.507990</td>\n",
       "      <td>0.794612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.417500</td>\n",
       "      <td>0.513749</td>\n",
       "      <td>0.796495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.512385</td>\n",
       "      <td>0.797053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.535200</td>\n",
       "      <td>0.492877</td>\n",
       "      <td>0.794350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>0.506906</td>\n",
       "      <td>0.791664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.440700</td>\n",
       "      <td>0.512345</td>\n",
       "      <td>0.791981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>0.526163</td>\n",
       "      <td>0.796705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.407800</td>\n",
       "      <td>0.526031</td>\n",
       "      <td>0.790422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.415100</td>\n",
       "      <td>0.524863</td>\n",
       "      <td>0.792978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.413200</td>\n",
       "      <td>0.515376</td>\n",
       "      <td>0.798708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.344900</td>\n",
       "      <td>0.536577</td>\n",
       "      <td>0.793013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.370100</td>\n",
       "      <td>0.533053</td>\n",
       "      <td>0.790891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.369100</td>\n",
       "      <td>0.528146</td>\n",
       "      <td>0.793372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.400600</td>\n",
       "      <td>0.533073</td>\n",
       "      <td>0.792061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.395500</td>\n",
       "      <td>0.526191</td>\n",
       "      <td>0.796685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.533808</td>\n",
       "      <td>0.793444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.383400</td>\n",
       "      <td>0.538640</td>\n",
       "      <td>0.793886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.428700</td>\n",
       "      <td>0.529597</td>\n",
       "      <td>0.792135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.389500</td>\n",
       "      <td>0.526279</td>\n",
       "      <td>0.797782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.377600</td>\n",
       "      <td>0.531126</td>\n",
       "      <td>0.794507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.395900</td>\n",
       "      <td>0.525578</td>\n",
       "      <td>0.794798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.518754</td>\n",
       "      <td>0.797927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.432400</td>\n",
       "      <td>0.530488</td>\n",
       "      <td>0.790157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.387300</td>\n",
       "      <td>0.528240</td>\n",
       "      <td>0.791288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.408400</td>\n",
       "      <td>0.516738</td>\n",
       "      <td>0.799364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.386500</td>\n",
       "      <td>0.517517</td>\n",
       "      <td>0.797151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.356400</td>\n",
       "      <td>0.515609</td>\n",
       "      <td>0.794433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.384600</td>\n",
       "      <td>0.515763</td>\n",
       "      <td>0.795820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.381100</td>\n",
       "      <td>0.523045</td>\n",
       "      <td>0.793669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.396200</td>\n",
       "      <td>0.525487</td>\n",
       "      <td>0.794151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.526242</td>\n",
       "      <td>0.793198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.410100</td>\n",
       "      <td>0.525832</td>\n",
       "      <td>0.794057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.370100</td>\n",
       "      <td>0.526255</td>\n",
       "      <td>0.794116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.357800</td>\n",
       "      <td>0.526601</td>\n",
       "      <td>0.793382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.393600</td>\n",
       "      <td>0.527718</td>\n",
       "      <td>0.794207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.342900</td>\n",
       "      <td>0.528376</td>\n",
       "      <td>0.795341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.431100</td>\n",
       "      <td>0.528273</td>\n",
       "      <td>0.794737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.354700</td>\n",
       "      <td>0.527977</td>\n",
       "      <td>0.794451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500 (score: 0.7966240187493925).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='208' max='208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [208/208 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▅▇▇▇███████████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▆▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▁▁▃▂▃▃▃▂▂▃▂▂█▃▃▃▂▂▂▂▃▃▃▅▂▃▂▃▃▃▂▂▂▄▃▃▃▃▂</td></tr><tr><td>eval/samples_per_second</td><td>▇██▆▇▆▆▆▇▇▆▇▇▁▆▆▆▆▇▆▇▆▆▆▃▇▆▆▆▆▆▇▇▇▅▆▆▆▆▇</td></tr><tr><td>eval/steps_per_second</td><td>▇██▆▇▆▆▆▇▇▆▇▇▁▆▆▆▆▇▆▇▆▆▆▃▇▆▆▆▆▆▇▇▇▅▆▆▆▆▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▃▇███▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▅▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▂▂▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79662</td></tr><tr><td>eval/loss</td><td>0.51324</td></tr><tr><td>eval/runtime</td><td>6.111</td></tr><tr><td>eval/samples_per_second</td><td>578.299</td></tr><tr><td>eval/steps_per_second</td><td>34.037</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>4851</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3547</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.49257</td></tr><tr><td>train/train_runtime</td><td>1122.919</td></tr><tr><td>train/train_samples_per_second</td><td>73.416</td></tr><tr><td>train/train_steps_per_second</td><td>4.32</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">restful-sweep-16</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/svd1b1o3\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/svd1b1o3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_001330-svd1b1o3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uqptmdpv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.430715197658142e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.0975721052306532\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.007009274908742593\n",
      "2022-04-28 00:32:47.507335: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_003246-uqptmdpv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/uqptmdpv\" target=\"_blank\">lunar-sweep-17</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1743c782dbcb42638128af70b0b56fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6870\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6870' max='6870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6870/6870 24:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.108100</td>\n",
       "      <td>1.104087</td>\n",
       "      <td>0.273054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.088614</td>\n",
       "      <td>0.353107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.085200</td>\n",
       "      <td>1.070208</td>\n",
       "      <td>0.416486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.074000</td>\n",
       "      <td>1.047406</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.043000</td>\n",
       "      <td>1.005326</td>\n",
       "      <td>0.500838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.002600</td>\n",
       "      <td>0.944572</td>\n",
       "      <td>0.585744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.920400</td>\n",
       "      <td>0.858601</td>\n",
       "      <td>0.654049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.865800</td>\n",
       "      <td>0.800716</td>\n",
       "      <td>0.640072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.790400</td>\n",
       "      <td>0.708735</td>\n",
       "      <td>0.723047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>0.649245</td>\n",
       "      <td>0.740461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.703100</td>\n",
       "      <td>0.664931</td>\n",
       "      <td>0.722729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.602540</td>\n",
       "      <td>0.760830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.627900</td>\n",
       "      <td>0.640857</td>\n",
       "      <td>0.724301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.626900</td>\n",
       "      <td>0.595541</td>\n",
       "      <td>0.762236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.648600</td>\n",
       "      <td>0.595358</td>\n",
       "      <td>0.760150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.613100</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.762495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.600900</td>\n",
       "      <td>0.574593</td>\n",
       "      <td>0.767100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.611700</td>\n",
       "      <td>0.562257</td>\n",
       "      <td>0.771733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.563600</td>\n",
       "      <td>0.554193</td>\n",
       "      <td>0.774407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.533800</td>\n",
       "      <td>0.569646</td>\n",
       "      <td>0.772954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.592300</td>\n",
       "      <td>0.559020</td>\n",
       "      <td>0.776159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.541800</td>\n",
       "      <td>0.587826</td>\n",
       "      <td>0.760782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.570292</td>\n",
       "      <td>0.762161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.580400</td>\n",
       "      <td>0.557437</td>\n",
       "      <td>0.772328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.527200</td>\n",
       "      <td>0.596183</td>\n",
       "      <td>0.763160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.623400</td>\n",
       "      <td>0.537607</td>\n",
       "      <td>0.784262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.576400</td>\n",
       "      <td>0.538751</td>\n",
       "      <td>0.788273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.542227</td>\n",
       "      <td>0.784110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.560700</td>\n",
       "      <td>0.544769</td>\n",
       "      <td>0.785378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.615700</td>\n",
       "      <td>0.560386</td>\n",
       "      <td>0.771317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.554600</td>\n",
       "      <td>0.528890</td>\n",
       "      <td>0.784323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.532900</td>\n",
       "      <td>0.521710</td>\n",
       "      <td>0.788895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>0.548984</td>\n",
       "      <td>0.772114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>0.518650</td>\n",
       "      <td>0.785792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.584400</td>\n",
       "      <td>0.538052</td>\n",
       "      <td>0.777415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.538900</td>\n",
       "      <td>0.525719</td>\n",
       "      <td>0.789552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.595400</td>\n",
       "      <td>0.522648</td>\n",
       "      <td>0.785249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.538800</td>\n",
       "      <td>0.536653</td>\n",
       "      <td>0.786309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>0.550115</td>\n",
       "      <td>0.774193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.534400</td>\n",
       "      <td>0.535684</td>\n",
       "      <td>0.789463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.556300</td>\n",
       "      <td>0.512056</td>\n",
       "      <td>0.787231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.547100</td>\n",
       "      <td>0.533172</td>\n",
       "      <td>0.778423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.455600</td>\n",
       "      <td>0.568261</td>\n",
       "      <td>0.769796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.481900</td>\n",
       "      <td>0.524777</td>\n",
       "      <td>0.793754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.553900</td>\n",
       "      <td>0.511630</td>\n",
       "      <td>0.790467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.511300</td>\n",
       "      <td>0.503803</td>\n",
       "      <td>0.793272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.403500</td>\n",
       "      <td>0.529972</td>\n",
       "      <td>0.798226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.412700</td>\n",
       "      <td>0.550265</td>\n",
       "      <td>0.783723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.424000</td>\n",
       "      <td>0.532397</td>\n",
       "      <td>0.790374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.494700</td>\n",
       "      <td>0.540216</td>\n",
       "      <td>0.795403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.508800</td>\n",
       "      <td>0.519267</td>\n",
       "      <td>0.791863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.446000</td>\n",
       "      <td>0.515836</td>\n",
       "      <td>0.793298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.459700</td>\n",
       "      <td>0.535902</td>\n",
       "      <td>0.796823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.527110</td>\n",
       "      <td>0.790825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.445500</td>\n",
       "      <td>0.510313</td>\n",
       "      <td>0.799012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.435500</td>\n",
       "      <td>0.541562</td>\n",
       "      <td>0.789227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.388100</td>\n",
       "      <td>0.536029</td>\n",
       "      <td>0.794222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.468400</td>\n",
       "      <td>0.534420</td>\n",
       "      <td>0.793993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.506162</td>\n",
       "      <td>0.801920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.414800</td>\n",
       "      <td>0.543991</td>\n",
       "      <td>0.799797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.480500</td>\n",
       "      <td>0.514049</td>\n",
       "      <td>0.799504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.479700</td>\n",
       "      <td>0.516250</td>\n",
       "      <td>0.798547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.437900</td>\n",
       "      <td>0.537000</td>\n",
       "      <td>0.793819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.422100</td>\n",
       "      <td>0.563939</td>\n",
       "      <td>0.785365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.421000</td>\n",
       "      <td>0.520782</td>\n",
       "      <td>0.793871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.477000</td>\n",
       "      <td>0.509892</td>\n",
       "      <td>0.794376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.523600</td>\n",
       "      <td>0.498346</td>\n",
       "      <td>0.795486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.387100</td>\n",
       "      <td>0.559674</td>\n",
       "      <td>0.784603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.468600</td>\n",
       "      <td>0.524338</td>\n",
       "      <td>0.798171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.441100</td>\n",
       "      <td>0.523652</td>\n",
       "      <td>0.798511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.473700</td>\n",
       "      <td>0.507467</td>\n",
       "      <td>0.803962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.445700</td>\n",
       "      <td>0.509032</td>\n",
       "      <td>0.798461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.445300</td>\n",
       "      <td>0.515272</td>\n",
       "      <td>0.794743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.446700</td>\n",
       "      <td>0.541328</td>\n",
       "      <td>0.792888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.522142</td>\n",
       "      <td>0.796307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.394500</td>\n",
       "      <td>0.529616</td>\n",
       "      <td>0.795139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.447800</td>\n",
       "      <td>0.527787</td>\n",
       "      <td>0.795090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.446900</td>\n",
       "      <td>0.524925</td>\n",
       "      <td>0.803865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>0.530399</td>\n",
       "      <td>0.787787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.541888</td>\n",
       "      <td>0.792979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.458600</td>\n",
       "      <td>0.513720</td>\n",
       "      <td>0.803756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.486200</td>\n",
       "      <td>0.501388</td>\n",
       "      <td>0.803278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.461300</td>\n",
       "      <td>0.505983</td>\n",
       "      <td>0.801097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.399200</td>\n",
       "      <td>0.515582</td>\n",
       "      <td>0.801378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.436200</td>\n",
       "      <td>0.505776</td>\n",
       "      <td>0.798622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.395800</td>\n",
       "      <td>0.522400</td>\n",
       "      <td>0.798355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.422200</td>\n",
       "      <td>0.521528</td>\n",
       "      <td>0.795571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.462200</td>\n",
       "      <td>0.517146</td>\n",
       "      <td>0.797953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.473700</td>\n",
       "      <td>0.493947</td>\n",
       "      <td>0.798299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.491600</td>\n",
       "      <td>0.499354</td>\n",
       "      <td>0.795393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.474800</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>0.792761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.418900</td>\n",
       "      <td>0.511620</td>\n",
       "      <td>0.796965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.316500</td>\n",
       "      <td>0.549304</td>\n",
       "      <td>0.795747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.351200</td>\n",
       "      <td>0.568124</td>\n",
       "      <td>0.796101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.341300</td>\n",
       "      <td>0.560218</td>\n",
       "      <td>0.795263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.360100</td>\n",
       "      <td>0.563936</td>\n",
       "      <td>0.795394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.397100</td>\n",
       "      <td>0.535682</td>\n",
       "      <td>0.801945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.417200</td>\n",
       "      <td>0.539987</td>\n",
       "      <td>0.801088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.313100</td>\n",
       "      <td>0.560876</td>\n",
       "      <td>0.795834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.304100</td>\n",
       "      <td>0.589213</td>\n",
       "      <td>0.789322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.326700</td>\n",
       "      <td>0.584474</td>\n",
       "      <td>0.793577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.357900</td>\n",
       "      <td>0.573214</td>\n",
       "      <td>0.794873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.570429</td>\n",
       "      <td>0.792240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.356300</td>\n",
       "      <td>0.562945</td>\n",
       "      <td>0.797297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.403400</td>\n",
       "      <td>0.561062</td>\n",
       "      <td>0.796073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.326700</td>\n",
       "      <td>0.566565</td>\n",
       "      <td>0.793267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.369800</td>\n",
       "      <td>0.557893</td>\n",
       "      <td>0.790597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.403500</td>\n",
       "      <td>0.553474</td>\n",
       "      <td>0.793760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.410700</td>\n",
       "      <td>0.554221</td>\n",
       "      <td>0.795414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.377300</td>\n",
       "      <td>0.561559</td>\n",
       "      <td>0.793590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.350300</td>\n",
       "      <td>0.555276</td>\n",
       "      <td>0.799195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.327900</td>\n",
       "      <td>0.576388</td>\n",
       "      <td>0.790237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.565468</td>\n",
       "      <td>0.792790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.556845</td>\n",
       "      <td>0.799604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.446800</td>\n",
       "      <td>0.551362</td>\n",
       "      <td>0.795817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.340500</td>\n",
       "      <td>0.571508</td>\n",
       "      <td>0.788850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.396900</td>\n",
       "      <td>0.556445</td>\n",
       "      <td>0.791276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.341700</td>\n",
       "      <td>0.563728</td>\n",
       "      <td>0.790359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.406100</td>\n",
       "      <td>0.549423</td>\n",
       "      <td>0.796261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.338100</td>\n",
       "      <td>0.548176</td>\n",
       "      <td>0.796490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.380600</td>\n",
       "      <td>0.552353</td>\n",
       "      <td>0.794281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.333500</td>\n",
       "      <td>0.556223</td>\n",
       "      <td>0.792543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.376500</td>\n",
       "      <td>0.553527</td>\n",
       "      <td>0.794806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.366400</td>\n",
       "      <td>0.552616</td>\n",
       "      <td>0.797911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.349800</td>\n",
       "      <td>0.556946</td>\n",
       "      <td>0.796618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.403000</td>\n",
       "      <td>0.559712</td>\n",
       "      <td>0.795659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.352400</td>\n",
       "      <td>0.558381</td>\n",
       "      <td>0.794128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.338200</td>\n",
       "      <td>0.558462</td>\n",
       "      <td>0.794977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>0.393800</td>\n",
       "      <td>0.561579</td>\n",
       "      <td>0.795246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.346600</td>\n",
       "      <td>0.562887</td>\n",
       "      <td>0.795577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.559484</td>\n",
       "      <td>0.797431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.347700</td>\n",
       "      <td>0.559998</td>\n",
       "      <td>0.796039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>0.394300</td>\n",
       "      <td>0.558321</td>\n",
       "      <td>0.796504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.307200</td>\n",
       "      <td>0.557376</td>\n",
       "      <td>0.796902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.406400</td>\n",
       "      <td>0.557803</td>\n",
       "      <td>0.797024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.418600</td>\n",
       "      <td>0.557996</td>\n",
       "      <td>0.795854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>0.328500</td>\n",
       "      <td>0.557624</td>\n",
       "      <td>0.796436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000 (score: 0.7997965130075313).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='295' max='295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [295/295 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▂▅▇▇█▇█████████████████████████████████</td></tr><tr><td>eval/loss</td><td>██▅▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>eval/runtime</td><td>▂▁▂▂█▂▂▂▂▂▃▂▅▂▂▂▂▂▂▃▄▂▂▂▂▂▂▃▄▂▂▂▂▂▃▂▅▂▂▂</td></tr><tr><td>eval/samples_per_second</td><td>▇█▇▇▁▇▇▇▇▇▆▇▄▆▇▇▇▇▇▆▅▇▆▇▇▇▇▅▅▇▇▇▇▇▆▇▃▇▆▇</td></tr><tr><td>eval/steps_per_second</td><td>▇█▇▇▁▇▇▇▇▇▆▇▄▆▇▇▇▇▇▆▅▇▆▇▇▇▇▅▅▇▇▇▇▇▆▇▃▇▆▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▆▄▄▄▃▄▃▃▃▃▃▃▃▂▂▂▃▃▂▂▂▂▂▂▃▁▂▁▁▂▂▂▁▁▂▁▁▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.7998</td></tr><tr><td>eval/loss</td><td>0.54399</td></tr><tr><td>eval/runtime</td><td>6.3028</td></tr><tr><td>eval/samples_per_second</td><td>560.705</td></tr><tr><td>eval/steps_per_second</td><td>46.805</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>6870</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3285</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.48998</td></tr><tr><td>train/train_runtime</td><td>1486.3553</td></tr><tr><td>train/train_samples_per_second</td><td>55.465</td></tr><tr><td>train/train_steps_per_second</td><td>4.622</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lunar-sweep-17</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/uqptmdpv\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/uqptmdpv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_003246-uqptmdpv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v6tzr5ir with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.609926705839952e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.09838013176050524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01973679513149403\n",
      "2022-04-28 00:58:07.156114: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_005805-v6tzr5ir</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/v6tzr5ir\" target=\"_blank\">zany-sweep-18</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee68584c14e48949378486755c690d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 11\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 11\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7497\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7497' max='7497' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7497/7497 27:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.117600</td>\n",
       "      <td>1.105259</td>\n",
       "      <td>0.270322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.093000</td>\n",
       "      <td>1.088687</td>\n",
       "      <td>0.347490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.096800</td>\n",
       "      <td>1.071033</td>\n",
       "      <td>0.412178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.072700</td>\n",
       "      <td>1.047589</td>\n",
       "      <td>0.445696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.036600</td>\n",
       "      <td>1.014880</td>\n",
       "      <td>0.496214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.006600</td>\n",
       "      <td>0.980156</td>\n",
       "      <td>0.561362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.941100</td>\n",
       "      <td>0.881802</td>\n",
       "      <td>0.587222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.922100</td>\n",
       "      <td>0.826881</td>\n",
       "      <td>0.659657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.828400</td>\n",
       "      <td>0.780714</td>\n",
       "      <td>0.668544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.782700</td>\n",
       "      <td>0.713168</td>\n",
       "      <td>0.712217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.673747</td>\n",
       "      <td>0.716997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.728300</td>\n",
       "      <td>0.680946</td>\n",
       "      <td>0.701536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.697400</td>\n",
       "      <td>0.611435</td>\n",
       "      <td>0.753276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.633900</td>\n",
       "      <td>0.645165</td>\n",
       "      <td>0.731633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.646900</td>\n",
       "      <td>0.609139</td>\n",
       "      <td>0.759963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.645200</td>\n",
       "      <td>0.580581</td>\n",
       "      <td>0.770691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.618800</td>\n",
       "      <td>0.582208</td>\n",
       "      <td>0.764699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.599200</td>\n",
       "      <td>0.591885</td>\n",
       "      <td>0.761658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.577349</td>\n",
       "      <td>0.764602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.581800</td>\n",
       "      <td>0.565664</td>\n",
       "      <td>0.771122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.578800</td>\n",
       "      <td>0.577234</td>\n",
       "      <td>0.767555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.535700</td>\n",
       "      <td>0.577387</td>\n",
       "      <td>0.766946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.554414</td>\n",
       "      <td>0.773791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.531500</td>\n",
       "      <td>0.595752</td>\n",
       "      <td>0.755995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.561500</td>\n",
       "      <td>0.547448</td>\n",
       "      <td>0.772545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.583000</td>\n",
       "      <td>0.576105</td>\n",
       "      <td>0.765131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.515200</td>\n",
       "      <td>0.573707</td>\n",
       "      <td>0.772408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.670400</td>\n",
       "      <td>0.548674</td>\n",
       "      <td>0.774550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.572000</td>\n",
       "      <td>0.543193</td>\n",
       "      <td>0.782901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.537300</td>\n",
       "      <td>0.546216</td>\n",
       "      <td>0.788406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.523900</td>\n",
       "      <td>0.547895</td>\n",
       "      <td>0.779757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>0.563010</td>\n",
       "      <td>0.779449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.624400</td>\n",
       "      <td>0.551801</td>\n",
       "      <td>0.776441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.558700</td>\n",
       "      <td>0.529376</td>\n",
       "      <td>0.786757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.555500</td>\n",
       "      <td>0.533611</td>\n",
       "      <td>0.783843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.562523</td>\n",
       "      <td>0.765682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.533500</td>\n",
       "      <td>0.532798</td>\n",
       "      <td>0.784844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.577600</td>\n",
       "      <td>0.564970</td>\n",
       "      <td>0.774105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.514200</td>\n",
       "      <td>0.529368</td>\n",
       "      <td>0.795255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>0.528968</td>\n",
       "      <td>0.786738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.544200</td>\n",
       "      <td>0.562268</td>\n",
       "      <td>0.779488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.493200</td>\n",
       "      <td>0.549965</td>\n",
       "      <td>0.779472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>0.536113</td>\n",
       "      <td>0.785315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.556600</td>\n",
       "      <td>0.532488</td>\n",
       "      <td>0.791339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.569800</td>\n",
       "      <td>0.521396</td>\n",
       "      <td>0.787281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.510700</td>\n",
       "      <td>0.550104</td>\n",
       "      <td>0.780534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.484200</td>\n",
       "      <td>0.544086</td>\n",
       "      <td>0.785526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.443500</td>\n",
       "      <td>0.529087</td>\n",
       "      <td>0.794657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.522700</td>\n",
       "      <td>0.536930</td>\n",
       "      <td>0.787669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.523200</td>\n",
       "      <td>0.528298</td>\n",
       "      <td>0.788186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.402500</td>\n",
       "      <td>0.535597</td>\n",
       "      <td>0.795501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.412700</td>\n",
       "      <td>0.563739</td>\n",
       "      <td>0.792735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>0.530663</td>\n",
       "      <td>0.798345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.436700</td>\n",
       "      <td>0.538330</td>\n",
       "      <td>0.794003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.528900</td>\n",
       "      <td>0.532373</td>\n",
       "      <td>0.790652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.463600</td>\n",
       "      <td>0.532962</td>\n",
       "      <td>0.799024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>0.516283</td>\n",
       "      <td>0.792947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.456400</td>\n",
       "      <td>0.541943</td>\n",
       "      <td>0.792604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.438400</td>\n",
       "      <td>0.556453</td>\n",
       "      <td>0.789071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.454800</td>\n",
       "      <td>0.534306</td>\n",
       "      <td>0.791717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.560968</td>\n",
       "      <td>0.783452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.398900</td>\n",
       "      <td>0.550993</td>\n",
       "      <td>0.797199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>0.528710</td>\n",
       "      <td>0.798612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.479000</td>\n",
       "      <td>0.525894</td>\n",
       "      <td>0.791201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.418700</td>\n",
       "      <td>0.550618</td>\n",
       "      <td>0.795249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>0.557460</td>\n",
       "      <td>0.790542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.549274</td>\n",
       "      <td>0.788853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.444600</td>\n",
       "      <td>0.565846</td>\n",
       "      <td>0.787441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.451700</td>\n",
       "      <td>0.544338</td>\n",
       "      <td>0.797845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.432900</td>\n",
       "      <td>0.560587</td>\n",
       "      <td>0.790360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.418700</td>\n",
       "      <td>0.529483</td>\n",
       "      <td>0.793224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.457500</td>\n",
       "      <td>0.535270</td>\n",
       "      <td>0.792039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.522300</td>\n",
       "      <td>0.506975</td>\n",
       "      <td>0.797587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.417100</td>\n",
       "      <td>0.544187</td>\n",
       "      <td>0.789641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.470600</td>\n",
       "      <td>0.537423</td>\n",
       "      <td>0.797524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.408500</td>\n",
       "      <td>0.561013</td>\n",
       "      <td>0.792413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.479900</td>\n",
       "      <td>0.552885</td>\n",
       "      <td>0.792733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.457300</td>\n",
       "      <td>0.524993</td>\n",
       "      <td>0.796201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.434800</td>\n",
       "      <td>0.535371</td>\n",
       "      <td>0.793016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.486100</td>\n",
       "      <td>0.509923</td>\n",
       "      <td>0.799586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.431400</td>\n",
       "      <td>0.548700</td>\n",
       "      <td>0.786954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.457600</td>\n",
       "      <td>0.537470</td>\n",
       "      <td>0.791183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.410400</td>\n",
       "      <td>0.547232</td>\n",
       "      <td>0.792386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.442300</td>\n",
       "      <td>0.547505</td>\n",
       "      <td>0.790003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.429900</td>\n",
       "      <td>0.535568</td>\n",
       "      <td>0.797637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.448200</td>\n",
       "      <td>0.542020</td>\n",
       "      <td>0.791513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.432300</td>\n",
       "      <td>0.537142</td>\n",
       "      <td>0.793673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.452900</td>\n",
       "      <td>0.528628</td>\n",
       "      <td>0.798796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.465800</td>\n",
       "      <td>0.523737</td>\n",
       "      <td>0.797501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.468400</td>\n",
       "      <td>0.516299</td>\n",
       "      <td>0.796587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.403700</td>\n",
       "      <td>0.529436</td>\n",
       "      <td>0.797011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.371600</td>\n",
       "      <td>0.535914</td>\n",
       "      <td>0.795724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.447400</td>\n",
       "      <td>0.511193</td>\n",
       "      <td>0.795101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.521049</td>\n",
       "      <td>0.799122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.442300</td>\n",
       "      <td>0.532080</td>\n",
       "      <td>0.795997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.463900</td>\n",
       "      <td>0.521691</td>\n",
       "      <td>0.793623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.512729</td>\n",
       "      <td>0.790500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.518800</td>\n",
       "      <td>0.503767</td>\n",
       "      <td>0.799170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.490300</td>\n",
       "      <td>0.515685</td>\n",
       "      <td>0.793564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.443100</td>\n",
       "      <td>0.521337</td>\n",
       "      <td>0.792668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.352400</td>\n",
       "      <td>0.550105</td>\n",
       "      <td>0.795460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.295900</td>\n",
       "      <td>0.564390</td>\n",
       "      <td>0.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.401400</td>\n",
       "      <td>0.554120</td>\n",
       "      <td>0.796698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.353500</td>\n",
       "      <td>0.551766</td>\n",
       "      <td>0.794133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.362200</td>\n",
       "      <td>0.570210</td>\n",
       "      <td>0.794108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.379600</td>\n",
       "      <td>0.545804</td>\n",
       "      <td>0.799402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.398800</td>\n",
       "      <td>0.557687</td>\n",
       "      <td>0.797289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.324000</td>\n",
       "      <td>0.577714</td>\n",
       "      <td>0.796411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.301600</td>\n",
       "      <td>0.621805</td>\n",
       "      <td>0.784202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.369700</td>\n",
       "      <td>0.596374</td>\n",
       "      <td>0.790784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.345300</td>\n",
       "      <td>0.591989</td>\n",
       "      <td>0.798151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.381400</td>\n",
       "      <td>0.577647</td>\n",
       "      <td>0.795058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.565134</td>\n",
       "      <td>0.796237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.339900</td>\n",
       "      <td>0.579741</td>\n",
       "      <td>0.800402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.361100</td>\n",
       "      <td>0.585356</td>\n",
       "      <td>0.793707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.366100</td>\n",
       "      <td>0.577391</td>\n",
       "      <td>0.795369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.357700</td>\n",
       "      <td>0.593350</td>\n",
       "      <td>0.794901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.385500</td>\n",
       "      <td>0.567844</td>\n",
       "      <td>0.795871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.404100</td>\n",
       "      <td>0.558496</td>\n",
       "      <td>0.794298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.369000</td>\n",
       "      <td>0.563569</td>\n",
       "      <td>0.796679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.358500</td>\n",
       "      <td>0.562166</td>\n",
       "      <td>0.795681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>0.568421</td>\n",
       "      <td>0.794306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.331800</td>\n",
       "      <td>0.585974</td>\n",
       "      <td>0.788995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.388100</td>\n",
       "      <td>0.570741</td>\n",
       "      <td>0.796635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.350700</td>\n",
       "      <td>0.567985</td>\n",
       "      <td>0.793793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.567659</td>\n",
       "      <td>0.794168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.387900</td>\n",
       "      <td>0.571186</td>\n",
       "      <td>0.791001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.344600</td>\n",
       "      <td>0.575030</td>\n",
       "      <td>0.789227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>0.382400</td>\n",
       "      <td>0.563527</td>\n",
       "      <td>0.793329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.387300</td>\n",
       "      <td>0.553441</td>\n",
       "      <td>0.797258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>0.340100</td>\n",
       "      <td>0.553793</td>\n",
       "      <td>0.795861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.381100</td>\n",
       "      <td>0.554384</td>\n",
       "      <td>0.797077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>0.325400</td>\n",
       "      <td>0.554787</td>\n",
       "      <td>0.793792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.558235</td>\n",
       "      <td>0.795783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.315400</td>\n",
       "      <td>0.567188</td>\n",
       "      <td>0.795272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.373400</td>\n",
       "      <td>0.570493</td>\n",
       "      <td>0.795468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>0.349200</td>\n",
       "      <td>0.569093</td>\n",
       "      <td>0.795036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.570372</td>\n",
       "      <td>0.793181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>0.336400</td>\n",
       "      <td>0.568674</td>\n",
       "      <td>0.793770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.568689</td>\n",
       "      <td>0.796446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>0.355200</td>\n",
       "      <td>0.572800</td>\n",
       "      <td>0.796001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.376900</td>\n",
       "      <td>0.571726</td>\n",
       "      <td>0.796921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>0.353500</td>\n",
       "      <td>0.569381</td>\n",
       "      <td>0.795580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.347500</td>\n",
       "      <td>0.571539</td>\n",
       "      <td>0.795848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.402400</td>\n",
       "      <td>0.571774</td>\n",
       "      <td>0.795795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.310300</td>\n",
       "      <td>0.571250</td>\n",
       "      <td>0.795086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>0.337800</td>\n",
       "      <td>0.571435</td>\n",
       "      <td>0.795251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.393300</td>\n",
       "      <td>0.570871</td>\n",
       "      <td>0.796079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>0.375200</td>\n",
       "      <td>0.570203</td>\n",
       "      <td>0.796640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-7000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-7000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-7000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000 (score: 0.7995855733305075).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='322' max='322' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [322/322 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▃▆▆██▇██▇██████████████████████████████</td></tr><tr><td>eval/loss</td><td>██▅▃▂▂▂▁▂▂▂▁▁▁▁▁▁▂▂▁▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>eval/runtime</td><td>▁▃▃▃▄▄▅▄▄▄▃█▃▅▄▄▂▃▇▄▃▃▄▅▄▃▄▃▃▄▄▄▃▆▃▃▃▄▄▄</td></tr><tr><td>eval/samples_per_second</td><td>█▅▆▆▅▅▄▅▅▅▆▁▆▄▅▅▆▆▂▅▆▆▅▄▅▆▅▅▆▅▅▅▆▃▆▆▆▅▅▅</td></tr><tr><td>eval/steps_per_second</td><td>█▅▆▆▅▅▄▅▅▅▆▁▆▄▅▅▆▆▂▅▆▆▅▄▅▆▅▅▆▅▅▅▆▃▆▆▆▅▅▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▆▅▄▄▃▄▃▃▃▃▃▃▂▂▂▃▂▂▂▃▂▂▃▂▃▁▂▂▁▂▂▂▁▂▂▂▁▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79959</td></tr><tr><td>eval/loss</td><td>0.50992</td></tr><tr><td>eval/runtime</td><td>6.6947</td></tr><tr><td>eval/samples_per_second</td><td>527.879</td></tr><tr><td>eval/steps_per_second</td><td>48.098</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>7497</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3752</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.48886</td></tr><tr><td>train/train_runtime</td><td>1645.8796</td></tr><tr><td>train/train_samples_per_second</td><td>50.089</td></tr><tr><td>train/train_steps_per_second</td><td>4.555</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">zany-sweep-18</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/v6tzr5ir\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/v6tzr5ir</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_005805-v6tzr5ir/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oj5nbe6x with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.684561667950975e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.09315087732337514\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.015645776061200413\n",
      "2022-04-28 01:26:17.729555: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_012613-oj5nbe6x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/oj5nbe6x\" target=\"_blank\">logical-sweep-19</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9009ef0fdaf8435bb5555dda07a456c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 9\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 9\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6108\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6108' max='6108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6108/6108 20:46, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.116500</td>\n",
       "      <td>1.104763</td>\n",
       "      <td>0.272145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.095900</td>\n",
       "      <td>1.088369</td>\n",
       "      <td>0.346710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.091100</td>\n",
       "      <td>1.064494</td>\n",
       "      <td>0.417881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.060900</td>\n",
       "      <td>1.046752</td>\n",
       "      <td>0.461434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.041400</td>\n",
       "      <td>1.005529</td>\n",
       "      <td>0.491096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.986700</td>\n",
       "      <td>0.923851</td>\n",
       "      <td>0.614180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.899100</td>\n",
       "      <td>0.848889</td>\n",
       "      <td>0.646758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.841200</td>\n",
       "      <td>0.765451</td>\n",
       "      <td>0.686594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.762300</td>\n",
       "      <td>0.742525</td>\n",
       "      <td>0.651990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.749900</td>\n",
       "      <td>0.688570</td>\n",
       "      <td>0.695087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.735400</td>\n",
       "      <td>0.638432</td>\n",
       "      <td>0.735049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>0.620677</td>\n",
       "      <td>0.745775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.622600</td>\n",
       "      <td>0.629589</td>\n",
       "      <td>0.744970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.633700</td>\n",
       "      <td>0.611436</td>\n",
       "      <td>0.763733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.636500</td>\n",
       "      <td>0.606274</td>\n",
       "      <td>0.755936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.637200</td>\n",
       "      <td>0.570560</td>\n",
       "      <td>0.771905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.585700</td>\n",
       "      <td>0.586507</td>\n",
       "      <td>0.764745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.595900</td>\n",
       "      <td>0.558153</td>\n",
       "      <td>0.779394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.572600</td>\n",
       "      <td>0.591356</td>\n",
       "      <td>0.757355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.662700</td>\n",
       "      <td>0.600523</td>\n",
       "      <td>0.760062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.583300</td>\n",
       "      <td>0.585355</td>\n",
       "      <td>0.771239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.583200</td>\n",
       "      <td>0.619164</td>\n",
       "      <td>0.754814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.563696</td>\n",
       "      <td>0.779160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>0.550922</td>\n",
       "      <td>0.779615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.536700</td>\n",
       "      <td>0.544205</td>\n",
       "      <td>0.785751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.537900</td>\n",
       "      <td>0.569942</td>\n",
       "      <td>0.779853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.546300</td>\n",
       "      <td>0.543382</td>\n",
       "      <td>0.784534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.589500</td>\n",
       "      <td>0.545138</td>\n",
       "      <td>0.785539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.551500</td>\n",
       "      <td>0.547304</td>\n",
       "      <td>0.783211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.524300</td>\n",
       "      <td>0.561731</td>\n",
       "      <td>0.782929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.628700</td>\n",
       "      <td>0.564756</td>\n",
       "      <td>0.762137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.561200</td>\n",
       "      <td>0.557414</td>\n",
       "      <td>0.774669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.489900</td>\n",
       "      <td>0.564790</td>\n",
       "      <td>0.772115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.678600</td>\n",
       "      <td>0.528614</td>\n",
       "      <td>0.783494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.536700</td>\n",
       "      <td>0.541361</td>\n",
       "      <td>0.781147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.547100</td>\n",
       "      <td>0.533574</td>\n",
       "      <td>0.789765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.489400</td>\n",
       "      <td>0.577262</td>\n",
       "      <td>0.781794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.551300</td>\n",
       "      <td>0.532868</td>\n",
       "      <td>0.792750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.529600</td>\n",
       "      <td>0.540388</td>\n",
       "      <td>0.788979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.644200</td>\n",
       "      <td>0.532720</td>\n",
       "      <td>0.787718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.537400</td>\n",
       "      <td>0.518452</td>\n",
       "      <td>0.794657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.524400</td>\n",
       "      <td>0.543061</td>\n",
       "      <td>0.787923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.577800</td>\n",
       "      <td>0.523528</td>\n",
       "      <td>0.793595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.557700</td>\n",
       "      <td>0.560793</td>\n",
       "      <td>0.767038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.572400</td>\n",
       "      <td>0.508623</td>\n",
       "      <td>0.795951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.541400</td>\n",
       "      <td>0.570800</td>\n",
       "      <td>0.778192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.541700</td>\n",
       "      <td>0.514428</td>\n",
       "      <td>0.790949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.548700</td>\n",
       "      <td>0.526874</td>\n",
       "      <td>0.793935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.586600</td>\n",
       "      <td>0.530798</td>\n",
       "      <td>0.788599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.559600</td>\n",
       "      <td>0.548187</td>\n",
       "      <td>0.784596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.489600</td>\n",
       "      <td>0.532062</td>\n",
       "      <td>0.793579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.528500</td>\n",
       "      <td>0.548544</td>\n",
       "      <td>0.778518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.503600</td>\n",
       "      <td>0.541602</td>\n",
       "      <td>0.787476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.544100</td>\n",
       "      <td>0.528602</td>\n",
       "      <td>0.792244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.616900</td>\n",
       "      <td>0.529421</td>\n",
       "      <td>0.789696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.505400</td>\n",
       "      <td>0.548052</td>\n",
       "      <td>0.779741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.787633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.521900</td>\n",
       "      <td>0.519961</td>\n",
       "      <td>0.796463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.481900</td>\n",
       "      <td>0.543713</td>\n",
       "      <td>0.794669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.547100</td>\n",
       "      <td>0.533388</td>\n",
       "      <td>0.794524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.523100</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.788938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.414800</td>\n",
       "      <td>0.523079</td>\n",
       "      <td>0.794293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.388600</td>\n",
       "      <td>0.580345</td>\n",
       "      <td>0.788923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.410700</td>\n",
       "      <td>0.579399</td>\n",
       "      <td>0.788264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.413200</td>\n",
       "      <td>0.559948</td>\n",
       "      <td>0.796958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.570960</td>\n",
       "      <td>0.790896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.593800</td>\n",
       "      <td>0.526863</td>\n",
       "      <td>0.792651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.440300</td>\n",
       "      <td>0.547530</td>\n",
       "      <td>0.796598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.445300</td>\n",
       "      <td>0.550783</td>\n",
       "      <td>0.789660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.456700</td>\n",
       "      <td>0.536725</td>\n",
       "      <td>0.794377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.534429</td>\n",
       "      <td>0.798017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.454100</td>\n",
       "      <td>0.540279</td>\n",
       "      <td>0.791743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.474700</td>\n",
       "      <td>0.539738</td>\n",
       "      <td>0.790989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.544223</td>\n",
       "      <td>0.797341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.454200</td>\n",
       "      <td>0.556528</td>\n",
       "      <td>0.794280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>0.564047</td>\n",
       "      <td>0.800145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.464400</td>\n",
       "      <td>0.542188</td>\n",
       "      <td>0.796146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.512500</td>\n",
       "      <td>0.545076</td>\n",
       "      <td>0.791450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.412300</td>\n",
       "      <td>0.550433</td>\n",
       "      <td>0.795855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.403100</td>\n",
       "      <td>0.566851</td>\n",
       "      <td>0.796906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.516600</td>\n",
       "      <td>0.538455</td>\n",
       "      <td>0.799179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.460200</td>\n",
       "      <td>0.540634</td>\n",
       "      <td>0.791085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>0.542021</td>\n",
       "      <td>0.793683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.437700</td>\n",
       "      <td>0.546651</td>\n",
       "      <td>0.796021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.401100</td>\n",
       "      <td>0.560914</td>\n",
       "      <td>0.792192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.411700</td>\n",
       "      <td>0.557272</td>\n",
       "      <td>0.794788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.458900</td>\n",
       "      <td>0.545164</td>\n",
       "      <td>0.795424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.448500</td>\n",
       "      <td>0.536137</td>\n",
       "      <td>0.795059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>0.518974</td>\n",
       "      <td>0.799649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.423400</td>\n",
       "      <td>0.544412</td>\n",
       "      <td>0.790614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.392300</td>\n",
       "      <td>0.551272</td>\n",
       "      <td>0.795042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.430600</td>\n",
       "      <td>0.542301</td>\n",
       "      <td>0.796145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.425600</td>\n",
       "      <td>0.553900</td>\n",
       "      <td>0.802741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.458600</td>\n",
       "      <td>0.554349</td>\n",
       "      <td>0.796229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.442500</td>\n",
       "      <td>0.534625</td>\n",
       "      <td>0.802116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.455300</td>\n",
       "      <td>0.539614</td>\n",
       "      <td>0.797300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.433300</td>\n",
       "      <td>0.555391</td>\n",
       "      <td>0.793153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.437800</td>\n",
       "      <td>0.532575</td>\n",
       "      <td>0.798839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.408900</td>\n",
       "      <td>0.548082</td>\n",
       "      <td>0.797343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.404900</td>\n",
       "      <td>0.550630</td>\n",
       "      <td>0.794289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.548091</td>\n",
       "      <td>0.798011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.491600</td>\n",
       "      <td>0.547185</td>\n",
       "      <td>0.795558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.427400</td>\n",
       "      <td>0.547850</td>\n",
       "      <td>0.795871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.535555</td>\n",
       "      <td>0.800557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.538817</td>\n",
       "      <td>0.794992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.444200</td>\n",
       "      <td>0.535870</td>\n",
       "      <td>0.794323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.433400</td>\n",
       "      <td>0.538137</td>\n",
       "      <td>0.795341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.468200</td>\n",
       "      <td>0.526856</td>\n",
       "      <td>0.800810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.445200</td>\n",
       "      <td>0.526394</td>\n",
       "      <td>0.804045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.462600</td>\n",
       "      <td>0.525556</td>\n",
       "      <td>0.801563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.392400</td>\n",
       "      <td>0.529947</td>\n",
       "      <td>0.801409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.389200</td>\n",
       "      <td>0.532710</td>\n",
       "      <td>0.799044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.427000</td>\n",
       "      <td>0.532594</td>\n",
       "      <td>0.799298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>0.530175</td>\n",
       "      <td>0.801408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.380900</td>\n",
       "      <td>0.532817</td>\n",
       "      <td>0.799138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.433500</td>\n",
       "      <td>0.534303</td>\n",
       "      <td>0.799908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.436300</td>\n",
       "      <td>0.534823</td>\n",
       "      <td>0.801286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.486700</td>\n",
       "      <td>0.533667</td>\n",
       "      <td>0.800405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.574500</td>\n",
       "      <td>0.530840</td>\n",
       "      <td>0.799845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.422500</td>\n",
       "      <td>0.530138</td>\n",
       "      <td>0.800244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.449500</td>\n",
       "      <td>0.530766</td>\n",
       "      <td>0.799939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.422000</td>\n",
       "      <td>0.531177</td>\n",
       "      <td>0.799917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500 (score: 0.801563268181779).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='393' max='393' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [393/393 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▄▆▇▇█▇▇████████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▇▅▃▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▂▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▁▂▄▂▄▂▅▂▃▃▃▃▄▂▄▅▃▅█▅▅▅▄▄▄▃▅▇▆▅▃▃▄▃▅▅▃▅▃</td></tr><tr><td>eval/samples_per_second</td><td>██▇▅▇▅▇▄▆▆▆▅▆▄▇▅▄▆▄▁▄▄▄▅▅▅▆▄▂▃▄▅▆▅▆▄▄▆▄▆</td></tr><tr><td>eval/steps_per_second</td><td>██▇▅▇▅▇▄▆▆▆▅▆▄▇▅▄▆▄▁▄▄▄▅▅▅▆▄▂▃▄▅▆▅▆▄▄▆▄▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▅▃▃▃▃▂▃▃▂▃▂▃▃▃▂▂▂▁▁▂▂▂▂▂▂▂▁▂▁▁▁▂▂▁▁▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.80156</td></tr><tr><td>eval/loss</td><td>0.52556</td></tr><tr><td>eval/runtime</td><td>6.5645</td></tr><tr><td>eval/samples_per_second</td><td>538.351</td></tr><tr><td>eval/steps_per_second</td><td>59.868</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>6108</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.422</td></tr><tr><td>train/total_flos</td><td>1977050665396800.0</td></tr><tr><td>train/train_loss</td><td>0.53579</td></tr><tr><td>train/train_runtime</td><td>1246.9918</td></tr><tr><td>train/train_samples_per_second</td><td>44.074</td></tr><tr><td>train/train_steps_per_second</td><td>4.898</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">logical-sweep-19</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/oj5nbe6x\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/oj5nbe6x</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_012613-oj5nbe6x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ttjdz89f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.912291707766184e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.0968691677085702\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.017896753930457126\n",
      "2022-04-28 01:47:35.860688: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_014734-ttjdz89f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/ttjdz89f\" target=\"_blank\">celestial-sweep-20</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518b61169add4f45af9dc0638c5297e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 26\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 26\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2114\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2114' max='2114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2114/2114 09:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.105500</td>\n",
       "      <td>1.084861</td>\n",
       "      <td>0.368607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.073300</td>\n",
       "      <td>1.035620</td>\n",
       "      <td>0.473168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.990400</td>\n",
       "      <td>0.912076</td>\n",
       "      <td>0.571555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.872300</td>\n",
       "      <td>0.769947</td>\n",
       "      <td>0.705692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.736200</td>\n",
       "      <td>0.646636</td>\n",
       "      <td>0.735004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.661200</td>\n",
       "      <td>0.618861</td>\n",
       "      <td>0.741519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.633600</td>\n",
       "      <td>0.579558</td>\n",
       "      <td>0.768195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.614700</td>\n",
       "      <td>0.579795</td>\n",
       "      <td>0.757732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.569800</td>\n",
       "      <td>0.566493</td>\n",
       "      <td>0.769717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.576500</td>\n",
       "      <td>0.546398</td>\n",
       "      <td>0.783366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.573100</td>\n",
       "      <td>0.550161</td>\n",
       "      <td>0.772054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.602000</td>\n",
       "      <td>0.543391</td>\n",
       "      <td>0.778050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.538100</td>\n",
       "      <td>0.547464</td>\n",
       "      <td>0.781736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.585700</td>\n",
       "      <td>0.543865</td>\n",
       "      <td>0.772557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>0.534449</td>\n",
       "      <td>0.779607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.547600</td>\n",
       "      <td>0.540159</td>\n",
       "      <td>0.782531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.568800</td>\n",
       "      <td>0.535267</td>\n",
       "      <td>0.782834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.530100</td>\n",
       "      <td>0.526612</td>\n",
       "      <td>0.782932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.555400</td>\n",
       "      <td>0.519663</td>\n",
       "      <td>0.787084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.488600</td>\n",
       "      <td>0.533876</td>\n",
       "      <td>0.783708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.507200</td>\n",
       "      <td>0.510361</td>\n",
       "      <td>0.793081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.452100</td>\n",
       "      <td>0.534293</td>\n",
       "      <td>0.787196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.461200</td>\n",
       "      <td>0.524066</td>\n",
       "      <td>0.789875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.526680</td>\n",
       "      <td>0.783231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.477100</td>\n",
       "      <td>0.532463</td>\n",
       "      <td>0.785359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.466900</td>\n",
       "      <td>0.527774</td>\n",
       "      <td>0.794583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.468800</td>\n",
       "      <td>0.519006</td>\n",
       "      <td>0.789562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.467500</td>\n",
       "      <td>0.523106</td>\n",
       "      <td>0.790204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.469600</td>\n",
       "      <td>0.525225</td>\n",
       "      <td>0.785882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>0.523319</td>\n",
       "      <td>0.796153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>0.504830</td>\n",
       "      <td>0.793464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.442800</td>\n",
       "      <td>0.513722</td>\n",
       "      <td>0.798819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.471000</td>\n",
       "      <td>0.505669</td>\n",
       "      <td>0.798527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.459600</td>\n",
       "      <td>0.505437</td>\n",
       "      <td>0.797486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.459700</td>\n",
       "      <td>0.511911</td>\n",
       "      <td>0.797835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.474200</td>\n",
       "      <td>0.510310</td>\n",
       "      <td>0.794876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.456700</td>\n",
       "      <td>0.513267</td>\n",
       "      <td>0.792618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.475500</td>\n",
       "      <td>0.506941</td>\n",
       "      <td>0.796612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.438300</td>\n",
       "      <td>0.508412</td>\n",
       "      <td>0.795736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.442000</td>\n",
       "      <td>0.508672</td>\n",
       "      <td>0.795937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>0.509080</td>\n",
       "      <td>0.794720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.504300</td>\n",
       "      <td>0.508189</td>\n",
       "      <td>0.796207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500 (score: 0.7961530351226774).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▃▄▆▇▇█▇████████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▇▆▄▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▁▃▃▅▅▃▄▄▄▄▃▂▄▃▄▄█▃▃▂▄▄▄▄▄▃▅▃▄▃▃▄▅▅▄▃▅▂▆</td></tr><tr><td>eval/samples_per_second</td><td>▇█▆▆▄▄▆▅▅▅▅▆▇▅▆▅▅▁▆▆▆▅▅▅▅▅▆▄▆▅▆▆▄▄▄▅▆▄▆▃</td></tr><tr><td>eval/steps_per_second</td><td>▇█▆▆▄▄▆▅▅▅▅▆▇▅▆▅▅▁▆▆▆▅▅▅▅▅▆▄▆▅▆▆▄▄▄▅▆▄▆▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▃▄▆████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▆▄▃▃▃▂▂▂▃▂▃▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79615</td></tr><tr><td>eval/loss</td><td>0.52332</td></tr><tr><td>eval/runtime</td><td>6.0061</td></tr><tr><td>eval/samples_per_second</td><td>588.4</td></tr><tr><td>eval/steps_per_second</td><td>22.644</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>2114</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5043</td></tr><tr><td>train/total_flos</td><td>1977050665396800.0</td></tr><tr><td>train/train_loss</td><td>0.56379</td></tr><tr><td>train/train_runtime</td><td>550.6595</td></tr><tr><td>train/train_samples_per_second</td><td>99.808</td></tr><tr><td>train/train_steps_per_second</td><td>3.839</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">celestial-sweep-20</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/ttjdz89f\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/ttjdz89f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_014734-ttjdz89f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fytl0lhe with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.259855487778042e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.09660425700796545\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.005966513075506588\n",
      "2022-04-28 01:57:25.248729: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_015723-fytl0lhe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/fytl0lhe\" target=\"_blank\">cerulean-sweep-21</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0f6c0e1a6b4ed3856c9568eaedfca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 30\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 30\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2748\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2748' max='2748' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2748/2748 12:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.109600</td>\n",
       "      <td>1.089393</td>\n",
       "      <td>0.349011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.075000</td>\n",
       "      <td>1.044207</td>\n",
       "      <td>0.439823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.029000</td>\n",
       "      <td>0.958784</td>\n",
       "      <td>0.536021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.917200</td>\n",
       "      <td>0.830484</td>\n",
       "      <td>0.675720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.711157</td>\n",
       "      <td>0.717425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.720100</td>\n",
       "      <td>0.640621</td>\n",
       "      <td>0.745427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.643800</td>\n",
       "      <td>0.613863</td>\n",
       "      <td>0.745146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.611800</td>\n",
       "      <td>0.609165</td>\n",
       "      <td>0.744810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.593300</td>\n",
       "      <td>0.576213</td>\n",
       "      <td>0.763662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.618700</td>\n",
       "      <td>0.625738</td>\n",
       "      <td>0.733213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.611600</td>\n",
       "      <td>0.557369</td>\n",
       "      <td>0.767387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.588500</td>\n",
       "      <td>0.551196</td>\n",
       "      <td>0.774468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.542772</td>\n",
       "      <td>0.769336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.569400</td>\n",
       "      <td>0.556663</td>\n",
       "      <td>0.765118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.577100</td>\n",
       "      <td>0.548505</td>\n",
       "      <td>0.767043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.532700</td>\n",
       "      <td>0.535549</td>\n",
       "      <td>0.776694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.532300</td>\n",
       "      <td>0.534943</td>\n",
       "      <td>0.781158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.528500</td>\n",
       "      <td>0.525333</td>\n",
       "      <td>0.782982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.490100</td>\n",
       "      <td>0.538526</td>\n",
       "      <td>0.780422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.485500</td>\n",
       "      <td>0.535565</td>\n",
       "      <td>0.784234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.506500</td>\n",
       "      <td>0.523037</td>\n",
       "      <td>0.784599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.490500</td>\n",
       "      <td>0.518702</td>\n",
       "      <td>0.786745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.467200</td>\n",
       "      <td>0.517596</td>\n",
       "      <td>0.794526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.488400</td>\n",
       "      <td>0.520792</td>\n",
       "      <td>0.788997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.504200</td>\n",
       "      <td>0.527823</td>\n",
       "      <td>0.783375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.461200</td>\n",
       "      <td>0.519890</td>\n",
       "      <td>0.792798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.487800</td>\n",
       "      <td>0.522859</td>\n",
       "      <td>0.783241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.466900</td>\n",
       "      <td>0.514197</td>\n",
       "      <td>0.788939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.473600</td>\n",
       "      <td>0.513254</td>\n",
       "      <td>0.790563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.491400</td>\n",
       "      <td>0.513167</td>\n",
       "      <td>0.788952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.456100</td>\n",
       "      <td>0.517754</td>\n",
       "      <td>0.792285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.464300</td>\n",
       "      <td>0.520241</td>\n",
       "      <td>0.795073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>0.508039</td>\n",
       "      <td>0.795434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.443900</td>\n",
       "      <td>0.507534</td>\n",
       "      <td>0.793987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.450500</td>\n",
       "      <td>0.506354</td>\n",
       "      <td>0.795073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.522400</td>\n",
       "      <td>0.497027</td>\n",
       "      <td>0.797421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.468900</td>\n",
       "      <td>0.508677</td>\n",
       "      <td>0.790629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.401300</td>\n",
       "      <td>0.518694</td>\n",
       "      <td>0.789034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.447400</td>\n",
       "      <td>0.508140</td>\n",
       "      <td>0.796750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.374900</td>\n",
       "      <td>0.529490</td>\n",
       "      <td>0.787767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.518772</td>\n",
       "      <td>0.791672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>0.516677</td>\n",
       "      <td>0.797359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.395300</td>\n",
       "      <td>0.520964</td>\n",
       "      <td>0.793335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.445200</td>\n",
       "      <td>0.509372</td>\n",
       "      <td>0.795139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>0.523405</td>\n",
       "      <td>0.789258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.449100</td>\n",
       "      <td>0.516144</td>\n",
       "      <td>0.793705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.411700</td>\n",
       "      <td>0.520066</td>\n",
       "      <td>0.790379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.415900</td>\n",
       "      <td>0.515831</td>\n",
       "      <td>0.795702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.417900</td>\n",
       "      <td>0.511854</td>\n",
       "      <td>0.799173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.401100</td>\n",
       "      <td>0.513483</td>\n",
       "      <td>0.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.419800</td>\n",
       "      <td>0.514914</td>\n",
       "      <td>0.795230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.431200</td>\n",
       "      <td>0.513083</td>\n",
       "      <td>0.797186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.391200</td>\n",
       "      <td>0.512366</td>\n",
       "      <td>0.797836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.418800</td>\n",
       "      <td>0.513015</td>\n",
       "      <td>0.798353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500 (score: 0.7968753310304827).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='118' max='118' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [118/118 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▂▄▇▇▇▇▇██▇█████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▇▆▃▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▁▂▂▁▁▁▁▂▂▁▂▂▂█▂▂▂▂▂▂▂▁▃▂▂▂▂▂▂▄▂▂▂▂▂▂▂▂▂</td></tr><tr><td>eval/samples_per_second</td><td>██▇▇████▇▇█▇▇▇▁▇▇▇▇▇▇▇█▆▇▇▇▇▇▇▅▇▇▇▇▇▇▇▇▇</td></tr><tr><td>eval/steps_per_second</td><td>██▇▇████▇▇█▇▇▇▁▇▇▇▇▇▇▇█▆▇▇▇▇▇▇▅▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▂▄▅████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▇▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79688</td></tr><tr><td>eval/loss</td><td>0.51348</td></tr><tr><td>eval/runtime</td><td>6.0086</td></tr><tr><td>eval/samples_per_second</td><td>588.156</td></tr><tr><td>eval/steps_per_second</td><td>19.638</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>2748</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4188</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.53107</td></tr><tr><td>train/train_runtime</td><td>755.6668</td></tr><tr><td>train/train_samples_per_second</td><td>109.096</td></tr><tr><td>train/train_steps_per_second</td><td>3.637</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">cerulean-sweep-21</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/fytl0lhe\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/fytl0lhe</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_015723-fytl0lhe/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 06iktv2u with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.889686759703528e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.0863000623867988\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.009800085364689067\n",
      "2022-04-28 02:10:31.211499: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_021029-06iktv2u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/06iktv2u\" target=\"_blank\">peachy-sweep-22</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a435a57bcb240eb88c1f594171150a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 18\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 18\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3054\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3054' max='3054' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3054/3054 11:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.110300</td>\n",
       "      <td>1.091957</td>\n",
       "      <td>0.332028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.089200</td>\n",
       "      <td>1.060129</td>\n",
       "      <td>0.434588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.050800</td>\n",
       "      <td>1.002470</td>\n",
       "      <td>0.508225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.975500</td>\n",
       "      <td>0.886947</td>\n",
       "      <td>0.615653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.861500</td>\n",
       "      <td>0.793642</td>\n",
       "      <td>0.648420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.766600</td>\n",
       "      <td>0.679314</td>\n",
       "      <td>0.724770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.686600</td>\n",
       "      <td>0.653137</td>\n",
       "      <td>0.732692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.672100</td>\n",
       "      <td>0.614597</td>\n",
       "      <td>0.758524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.628400</td>\n",
       "      <td>0.592830</td>\n",
       "      <td>0.765838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.638700</td>\n",
       "      <td>0.577924</td>\n",
       "      <td>0.769435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.595700</td>\n",
       "      <td>0.597913</td>\n",
       "      <td>0.745996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.568279</td>\n",
       "      <td>0.773619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.557200</td>\n",
       "      <td>0.566210</td>\n",
       "      <td>0.766417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.583500</td>\n",
       "      <td>0.551790</td>\n",
       "      <td>0.776724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.528600</td>\n",
       "      <td>0.559063</td>\n",
       "      <td>0.781253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.611500</td>\n",
       "      <td>0.546739</td>\n",
       "      <td>0.777810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.591200</td>\n",
       "      <td>0.546491</td>\n",
       "      <td>0.774059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.575100</td>\n",
       "      <td>0.541635</td>\n",
       "      <td>0.775889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.539700</td>\n",
       "      <td>0.539007</td>\n",
       "      <td>0.778262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.588400</td>\n",
       "      <td>0.529772</td>\n",
       "      <td>0.783837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.543700</td>\n",
       "      <td>0.535661</td>\n",
       "      <td>0.778802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.576200</td>\n",
       "      <td>0.546652</td>\n",
       "      <td>0.773193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.551900</td>\n",
       "      <td>0.558260</td>\n",
       "      <td>0.780070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.547500</td>\n",
       "      <td>0.527779</td>\n",
       "      <td>0.784236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.561300</td>\n",
       "      <td>0.547794</td>\n",
       "      <td>0.777571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.507800</td>\n",
       "      <td>0.540433</td>\n",
       "      <td>0.779080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.535200</td>\n",
       "      <td>0.530214</td>\n",
       "      <td>0.785694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.558200</td>\n",
       "      <td>0.546900</td>\n",
       "      <td>0.775966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.478800</td>\n",
       "      <td>0.521084</td>\n",
       "      <td>0.785962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.507800</td>\n",
       "      <td>0.519266</td>\n",
       "      <td>0.788603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.470200</td>\n",
       "      <td>0.521306</td>\n",
       "      <td>0.790752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.431100</td>\n",
       "      <td>0.547850</td>\n",
       "      <td>0.790189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.443000</td>\n",
       "      <td>0.541877</td>\n",
       "      <td>0.783392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>0.525968</td>\n",
       "      <td>0.784956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>0.520191</td>\n",
       "      <td>0.786229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.462900</td>\n",
       "      <td>0.543029</td>\n",
       "      <td>0.783181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.520221</td>\n",
       "      <td>0.793029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>0.532379</td>\n",
       "      <td>0.792849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.488200</td>\n",
       "      <td>0.525825</td>\n",
       "      <td>0.792468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.439900</td>\n",
       "      <td>0.534520</td>\n",
       "      <td>0.795707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.491800</td>\n",
       "      <td>0.522545</td>\n",
       "      <td>0.792033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>0.519868</td>\n",
       "      <td>0.792342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.418800</td>\n",
       "      <td>0.532464</td>\n",
       "      <td>0.793691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.444700</td>\n",
       "      <td>0.514669</td>\n",
       "      <td>0.798421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.482900</td>\n",
       "      <td>0.532914</td>\n",
       "      <td>0.785861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.521931</td>\n",
       "      <td>0.796549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.442200</td>\n",
       "      <td>0.528831</td>\n",
       "      <td>0.793960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.440700</td>\n",
       "      <td>0.521205</td>\n",
       "      <td>0.794602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.450200</td>\n",
       "      <td>0.515171</td>\n",
       "      <td>0.797862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>0.527124</td>\n",
       "      <td>0.790946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.420600</td>\n",
       "      <td>0.531441</td>\n",
       "      <td>0.790074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.444300</td>\n",
       "      <td>0.523811</td>\n",
       "      <td>0.795460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>0.527937</td>\n",
       "      <td>0.794270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.459100</td>\n",
       "      <td>0.525022</td>\n",
       "      <td>0.797263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.473000</td>\n",
       "      <td>0.516452</td>\n",
       "      <td>0.797879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.419700</td>\n",
       "      <td>0.521507</td>\n",
       "      <td>0.796105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.421300</td>\n",
       "      <td>0.518613</td>\n",
       "      <td>0.798657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>0.522563</td>\n",
       "      <td>0.796755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.457400</td>\n",
       "      <td>0.521897</td>\n",
       "      <td>0.794871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.518000</td>\n",
       "      <td>0.519010</td>\n",
       "      <td>0.794951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.460700</td>\n",
       "      <td>0.519568</td>\n",
       "      <td>0.795205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000 (score: 0.7957067224232945).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 18\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='197' max='197' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [197/197 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▃▅▆▇▇█▇████████████████████████████████</td></tr><tr><td>eval/loss</td><td>██▆▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▂▁▄▃▃▂▂▅▃▃▄▂▁▂▅▅▂▃▂▂▃█▂▂▂▂▂▂▃▂▄▂▃▂▆▄▄▁▂</td></tr><tr><td>eval/samples_per_second</td><td>█▇█▅▆▆▇▇▄▆▆▅▇█▇▄▄▇▆▇▇▆▁▇▇▇▇▇▇▆▇▅▇▆▇▃▅▅█▇</td></tr><tr><td>eval/steps_per_second</td><td>█▇█▅▆▆▇▇▄▆▆▅▇█▇▄▄▇▆▇▇▆▁▇▇▇▇▇▇▆▇▅▇▆▇▃▅▅█▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▂▄▆████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▅▄▄▃▃▂▃▃▃▂▂▃▂▂▂▂▂▂▁▂▁▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79571</td></tr><tr><td>eval/loss</td><td>0.53452</td></tr><tr><td>eval/runtime</td><td>6.0024</td></tr><tr><td>eval/samples_per_second</td><td>588.764</td></tr><tr><td>eval/steps_per_second</td><td>32.82</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>3054</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4607</td></tr><tr><td>train/total_flos</td><td>1977050665396800.0</td></tr><tr><td>train/train_loss</td><td>0.55256</td></tr><tr><td>train/train_runtime</td><td>709.7903</td></tr><tr><td>train/train_samples_per_second</td><td>77.431</td></tr><tr><td>train/train_steps_per_second</td><td>4.303</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">peachy-sweep-22</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/06iktv2u\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/06iktv2u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_021029-06iktv2u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e253ngj9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.8304541282323e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.015202679880903854\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.00957195284247123\n",
      "2022-04-28 02:22:59.604096: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_022258-e253ngj9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/e253ngj9\" target=\"_blank\">sandy-sweep-23</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14deae296531480999bfab5207d7e055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6870\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6870' max='6870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6870/6870 24:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.100200</td>\n",
       "      <td>1.073676</td>\n",
       "      <td>0.408029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.053400</td>\n",
       "      <td>1.040652</td>\n",
       "      <td>0.408900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.959900</td>\n",
       "      <td>0.894369</td>\n",
       "      <td>0.528252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.820900</td>\n",
       "      <td>0.731616</td>\n",
       "      <td>0.682861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>0.702694</td>\n",
       "      <td>0.675289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>0.637380</td>\n",
       "      <td>0.732893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.697700</td>\n",
       "      <td>0.607822</td>\n",
       "      <td>0.750386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.669200</td>\n",
       "      <td>0.610504</td>\n",
       "      <td>0.746586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.634600</td>\n",
       "      <td>0.594572</td>\n",
       "      <td>0.757299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.584700</td>\n",
       "      <td>0.591870</td>\n",
       "      <td>0.765091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.630300</td>\n",
       "      <td>0.607408</td>\n",
       "      <td>0.753401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.616800</td>\n",
       "      <td>0.565638</td>\n",
       "      <td>0.772373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.577600</td>\n",
       "      <td>0.598480</td>\n",
       "      <td>0.754150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.575500</td>\n",
       "      <td>0.557821</td>\n",
       "      <td>0.778508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.616600</td>\n",
       "      <td>0.553793</td>\n",
       "      <td>0.782250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.588700</td>\n",
       "      <td>0.557439</td>\n",
       "      <td>0.777330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.574600</td>\n",
       "      <td>0.549743</td>\n",
       "      <td>0.783409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.597600</td>\n",
       "      <td>0.537411</td>\n",
       "      <td>0.782930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.533100</td>\n",
       "      <td>0.537981</td>\n",
       "      <td>0.784382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.514700</td>\n",
       "      <td>0.549703</td>\n",
       "      <td>0.783319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.567300</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.786742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.534700</td>\n",
       "      <td>0.564424</td>\n",
       "      <td>0.770787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.549100</td>\n",
       "      <td>0.568130</td>\n",
       "      <td>0.763616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.574900</td>\n",
       "      <td>0.547389</td>\n",
       "      <td>0.776226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.518500</td>\n",
       "      <td>0.572424</td>\n",
       "      <td>0.765805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.593600</td>\n",
       "      <td>0.528431</td>\n",
       "      <td>0.784319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>0.530929</td>\n",
       "      <td>0.788940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.499200</td>\n",
       "      <td>0.550583</td>\n",
       "      <td>0.783262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.548700</td>\n",
       "      <td>0.534712</td>\n",
       "      <td>0.788891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.615800</td>\n",
       "      <td>0.534393</td>\n",
       "      <td>0.780205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.551200</td>\n",
       "      <td>0.521473</td>\n",
       "      <td>0.789451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>0.519578</td>\n",
       "      <td>0.787977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.556700</td>\n",
       "      <td>0.538357</td>\n",
       "      <td>0.779504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.538900</td>\n",
       "      <td>0.511612</td>\n",
       "      <td>0.786684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.583400</td>\n",
       "      <td>0.526673</td>\n",
       "      <td>0.786299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.533300</td>\n",
       "      <td>0.519195</td>\n",
       "      <td>0.789365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.592900</td>\n",
       "      <td>0.517648</td>\n",
       "      <td>0.787845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.530700</td>\n",
       "      <td>0.532928</td>\n",
       "      <td>0.784995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.497700</td>\n",
       "      <td>0.539651</td>\n",
       "      <td>0.780554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.530100</td>\n",
       "      <td>0.527713</td>\n",
       "      <td>0.787758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.553000</td>\n",
       "      <td>0.505478</td>\n",
       "      <td>0.792560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>0.527780</td>\n",
       "      <td>0.778623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.449100</td>\n",
       "      <td>0.545933</td>\n",
       "      <td>0.779954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.474200</td>\n",
       "      <td>0.527502</td>\n",
       "      <td>0.795595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.552800</td>\n",
       "      <td>0.508329</td>\n",
       "      <td>0.791057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.512800</td>\n",
       "      <td>0.501035</td>\n",
       "      <td>0.791686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.391500</td>\n",
       "      <td>0.531896</td>\n",
       "      <td>0.797707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.401500</td>\n",
       "      <td>0.554068</td>\n",
       "      <td>0.787997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.415600</td>\n",
       "      <td>0.536367</td>\n",
       "      <td>0.791236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.481300</td>\n",
       "      <td>0.539482</td>\n",
       "      <td>0.793086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.491600</td>\n",
       "      <td>0.524385</td>\n",
       "      <td>0.797105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.438000</td>\n",
       "      <td>0.518539</td>\n",
       "      <td>0.793847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.461600</td>\n",
       "      <td>0.532172</td>\n",
       "      <td>0.796775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.455500</td>\n",
       "      <td>0.526015</td>\n",
       "      <td>0.791060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.425900</td>\n",
       "      <td>0.516169</td>\n",
       "      <td>0.797906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.421900</td>\n",
       "      <td>0.541897</td>\n",
       "      <td>0.793056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.373600</td>\n",
       "      <td>0.540439</td>\n",
       "      <td>0.794197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.443200</td>\n",
       "      <td>0.537239</td>\n",
       "      <td>0.795454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.477600</td>\n",
       "      <td>0.513118</td>\n",
       "      <td>0.800318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.410300</td>\n",
       "      <td>0.542146</td>\n",
       "      <td>0.798960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.469300</td>\n",
       "      <td>0.516885</td>\n",
       "      <td>0.796727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.470200</td>\n",
       "      <td>0.513719</td>\n",
       "      <td>0.797369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.426100</td>\n",
       "      <td>0.532355</td>\n",
       "      <td>0.796454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.406400</td>\n",
       "      <td>0.571025</td>\n",
       "      <td>0.787056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.419700</td>\n",
       "      <td>0.523585</td>\n",
       "      <td>0.793021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.468200</td>\n",
       "      <td>0.518344</td>\n",
       "      <td>0.790295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.514400</td>\n",
       "      <td>0.501200</td>\n",
       "      <td>0.797376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>0.555196</td>\n",
       "      <td>0.783935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.466600</td>\n",
       "      <td>0.531389</td>\n",
       "      <td>0.798062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.430900</td>\n",
       "      <td>0.529774</td>\n",
       "      <td>0.796949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.462800</td>\n",
       "      <td>0.516248</td>\n",
       "      <td>0.800750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.440400</td>\n",
       "      <td>0.517314</td>\n",
       "      <td>0.798731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.447100</td>\n",
       "      <td>0.526257</td>\n",
       "      <td>0.794378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.430700</td>\n",
       "      <td>0.545712</td>\n",
       "      <td>0.790072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.429500</td>\n",
       "      <td>0.528154</td>\n",
       "      <td>0.792521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.389600</td>\n",
       "      <td>0.534551</td>\n",
       "      <td>0.793942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.441900</td>\n",
       "      <td>0.527437</td>\n",
       "      <td>0.791951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.522447</td>\n",
       "      <td>0.800679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.426600</td>\n",
       "      <td>0.533751</td>\n",
       "      <td>0.785740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.414700</td>\n",
       "      <td>0.549967</td>\n",
       "      <td>0.792159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.464200</td>\n",
       "      <td>0.512900</td>\n",
       "      <td>0.801704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.471500</td>\n",
       "      <td>0.505306</td>\n",
       "      <td>0.802393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.447100</td>\n",
       "      <td>0.506798</td>\n",
       "      <td>0.804296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.520230</td>\n",
       "      <td>0.797990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.429000</td>\n",
       "      <td>0.510553</td>\n",
       "      <td>0.800448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>0.525897</td>\n",
       "      <td>0.800912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.411200</td>\n",
       "      <td>0.523461</td>\n",
       "      <td>0.797500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.457100</td>\n",
       "      <td>0.517380</td>\n",
       "      <td>0.799846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.471900</td>\n",
       "      <td>0.500933</td>\n",
       "      <td>0.796137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.486500</td>\n",
       "      <td>0.503309</td>\n",
       "      <td>0.798974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.465500</td>\n",
       "      <td>0.520341</td>\n",
       "      <td>0.791269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.414700</td>\n",
       "      <td>0.518819</td>\n",
       "      <td>0.796214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.548764</td>\n",
       "      <td>0.795243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.348200</td>\n",
       "      <td>0.559368</td>\n",
       "      <td>0.795441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.333700</td>\n",
       "      <td>0.561273</td>\n",
       "      <td>0.797249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.343600</td>\n",
       "      <td>0.563562</td>\n",
       "      <td>0.796116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>0.538091</td>\n",
       "      <td>0.803792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.414200</td>\n",
       "      <td>0.541270</td>\n",
       "      <td>0.800727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.306100</td>\n",
       "      <td>0.556325</td>\n",
       "      <td>0.795929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.300800</td>\n",
       "      <td>0.589999</td>\n",
       "      <td>0.787932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.321700</td>\n",
       "      <td>0.583281</td>\n",
       "      <td>0.791027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.356600</td>\n",
       "      <td>0.573294</td>\n",
       "      <td>0.794269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.396500</td>\n",
       "      <td>0.570045</td>\n",
       "      <td>0.793062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.349100</td>\n",
       "      <td>0.568345</td>\n",
       "      <td>0.795258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.564606</td>\n",
       "      <td>0.794535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.331200</td>\n",
       "      <td>0.565617</td>\n",
       "      <td>0.790934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.557313</td>\n",
       "      <td>0.793196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.395700</td>\n",
       "      <td>0.550819</td>\n",
       "      <td>0.794930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.401500</td>\n",
       "      <td>0.550851</td>\n",
       "      <td>0.794713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.362200</td>\n",
       "      <td>0.562363</td>\n",
       "      <td>0.789227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.346300</td>\n",
       "      <td>0.559321</td>\n",
       "      <td>0.797138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>0.582056</td>\n",
       "      <td>0.791155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.329500</td>\n",
       "      <td>0.574140</td>\n",
       "      <td>0.789973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.366700</td>\n",
       "      <td>0.562290</td>\n",
       "      <td>0.796667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>0.555808</td>\n",
       "      <td>0.792638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.336600</td>\n",
       "      <td>0.576357</td>\n",
       "      <td>0.786178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.394900</td>\n",
       "      <td>0.562195</td>\n",
       "      <td>0.787946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.336600</td>\n",
       "      <td>0.572517</td>\n",
       "      <td>0.783800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.411200</td>\n",
       "      <td>0.555258</td>\n",
       "      <td>0.793385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.347600</td>\n",
       "      <td>0.551333</td>\n",
       "      <td>0.795185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.384800</td>\n",
       "      <td>0.555595</td>\n",
       "      <td>0.794005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.316900</td>\n",
       "      <td>0.560839</td>\n",
       "      <td>0.793398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.379800</td>\n",
       "      <td>0.557104</td>\n",
       "      <td>0.794573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.346400</td>\n",
       "      <td>0.557764</td>\n",
       "      <td>0.797057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.349200</td>\n",
       "      <td>0.560604</td>\n",
       "      <td>0.793633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.392900</td>\n",
       "      <td>0.565982</td>\n",
       "      <td>0.795037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.344400</td>\n",
       "      <td>0.563133</td>\n",
       "      <td>0.791146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.332700</td>\n",
       "      <td>0.563923</td>\n",
       "      <td>0.793386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.566221</td>\n",
       "      <td>0.793117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>0.567734</td>\n",
       "      <td>0.793502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>0.367400</td>\n",
       "      <td>0.565373</td>\n",
       "      <td>0.794151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.565795</td>\n",
       "      <td>0.794758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>0.393400</td>\n",
       "      <td>0.564345</td>\n",
       "      <td>0.794911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.302800</td>\n",
       "      <td>0.563968</td>\n",
       "      <td>0.794437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.393800</td>\n",
       "      <td>0.564698</td>\n",
       "      <td>0.795706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.422800</td>\n",
       "      <td>0.564632</td>\n",
       "      <td>0.795463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>0.321100</td>\n",
       "      <td>0.564190</td>\n",
       "      <td>0.795728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500 (score: 0.7989735989105534).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='295' max='295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [295/295 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▆▇▇██▇█████████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▄▂▂▂▁▂▁▂▁▁▁▁▁▁▁▂▂▂▂▁▂▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>eval/runtime</td><td>▂▁▃▂▄▂▄▂▄▂▇▆▄▂▇▃▄▃▅█▄▃▃▅▃▄▇▃▄▅▄▄▃▆▅▃▂▆▆▆</td></tr><tr><td>eval/samples_per_second</td><td>▇█▆▇▅▇▅▇▅▇▂▃▄▇▂▆▅▆▄▁▅▆▆▄▆▅▂▆▅▄▅▅▆▃▄▆▇▃▃▃</td></tr><tr><td>eval/steps_per_second</td><td>▇█▆▇▅▇▅▇▅▇▂▃▄▇▂▆▅▆▄▁▅▆▆▄▆▅▂▆▅▄▅▅▆▃▄▆▇▃▃▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▄▃▄▃▃▃▃▃▃▃▂▂▂▃▃▂▂▂▂▂▂▃▁▂▁▁▂▂▂▁▁▁▁▁▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79897</td></tr><tr><td>eval/loss</td><td>0.50331</td></tr><tr><td>eval/runtime</td><td>6.3746</td></tr><tr><td>eval/samples_per_second</td><td>554.384</td></tr><tr><td>eval/steps_per_second</td><td>46.277</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>6870</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3211</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.4677</td></tr><tr><td>train/train_runtime</td><td>1482.4911</td></tr><tr><td>train/train_samples_per_second</td><td>55.609</td></tr><tr><td>train/train_steps_per_second</td><td>4.634</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">sandy-sweep-23</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/e253ngj9\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/e253ngj9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_022258-e253ngj9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qdp3ulbl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.479732198649685e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.06218110403959236\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01906891111675305\n",
      "2022-04-28 02:48:12.443203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_024811-qdp3ulbl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/qdp3ulbl\" target=\"_blank\">laced-sweep-24</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e9fa13fb124f1eb57ad19ed5548770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5154\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5154' max='5154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5154/5154 18:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.104500</td>\n",
       "      <td>1.096881</td>\n",
       "      <td>0.311108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.091700</td>\n",
       "      <td>1.068292</td>\n",
       "      <td>0.397859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.067400</td>\n",
       "      <td>1.028081</td>\n",
       "      <td>0.484774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.009000</td>\n",
       "      <td>0.953084</td>\n",
       "      <td>0.603285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.902600</td>\n",
       "      <td>0.816077</td>\n",
       "      <td>0.678262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.830600</td>\n",
       "      <td>0.731272</td>\n",
       "      <td>0.685053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.712100</td>\n",
       "      <td>0.651488</td>\n",
       "      <td>0.730149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.667500</td>\n",
       "      <td>0.624519</td>\n",
       "      <td>0.740352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.661700</td>\n",
       "      <td>0.606755</td>\n",
       "      <td>0.757772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.603400</td>\n",
       "      <td>0.596796</td>\n",
       "      <td>0.760079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.630700</td>\n",
       "      <td>0.576225</td>\n",
       "      <td>0.769786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.596100</td>\n",
       "      <td>0.575885</td>\n",
       "      <td>0.763136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.610900</td>\n",
       "      <td>0.569220</td>\n",
       "      <td>0.761248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.576200</td>\n",
       "      <td>0.589040</td>\n",
       "      <td>0.754232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.555800</td>\n",
       "      <td>0.552146</td>\n",
       "      <td>0.781222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.579600</td>\n",
       "      <td>0.543112</td>\n",
       "      <td>0.783743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.545700</td>\n",
       "      <td>0.538979</td>\n",
       "      <td>0.788319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.574200</td>\n",
       "      <td>0.556899</td>\n",
       "      <td>0.777181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.580300</td>\n",
       "      <td>0.564976</td>\n",
       "      <td>0.766080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.600700</td>\n",
       "      <td>0.542009</td>\n",
       "      <td>0.773419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.516900</td>\n",
       "      <td>0.536591</td>\n",
       "      <td>0.789265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.559200</td>\n",
       "      <td>0.536960</td>\n",
       "      <td>0.786145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.586400</td>\n",
       "      <td>0.523360</td>\n",
       "      <td>0.789079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.569500</td>\n",
       "      <td>0.523390</td>\n",
       "      <td>0.786044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.559500</td>\n",
       "      <td>0.522355</td>\n",
       "      <td>0.789342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.539500</td>\n",
       "      <td>0.541080</td>\n",
       "      <td>0.786866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>0.525024</td>\n",
       "      <td>0.789700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.570900</td>\n",
       "      <td>0.536772</td>\n",
       "      <td>0.779234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.504800</td>\n",
       "      <td>0.547447</td>\n",
       "      <td>0.773394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.533400</td>\n",
       "      <td>0.525434</td>\n",
       "      <td>0.789337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.574700</td>\n",
       "      <td>0.510903</td>\n",
       "      <td>0.788825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.467900</td>\n",
       "      <td>0.524332</td>\n",
       "      <td>0.788728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.481100</td>\n",
       "      <td>0.522481</td>\n",
       "      <td>0.791632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.518900</td>\n",
       "      <td>0.513759</td>\n",
       "      <td>0.791392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.452100</td>\n",
       "      <td>0.522613</td>\n",
       "      <td>0.789015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.420200</td>\n",
       "      <td>0.542369</td>\n",
       "      <td>0.787599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>0.525075</td>\n",
       "      <td>0.793026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.522300</td>\n",
       "      <td>0.525032</td>\n",
       "      <td>0.788422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.457500</td>\n",
       "      <td>0.525004</td>\n",
       "      <td>0.792567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.453300</td>\n",
       "      <td>0.521073</td>\n",
       "      <td>0.797686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.468000</td>\n",
       "      <td>0.546656</td>\n",
       "      <td>0.783608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.434400</td>\n",
       "      <td>0.540912</td>\n",
       "      <td>0.792501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.435800</td>\n",
       "      <td>0.521006</td>\n",
       "      <td>0.800427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.474300</td>\n",
       "      <td>0.514299</td>\n",
       "      <td>0.798254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.411800</td>\n",
       "      <td>0.525774</td>\n",
       "      <td>0.804130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.474400</td>\n",
       "      <td>0.520546</td>\n",
       "      <td>0.791785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.462300</td>\n",
       "      <td>0.520493</td>\n",
       "      <td>0.797049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.438500</td>\n",
       "      <td>0.547173</td>\n",
       "      <td>0.788218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.419800</td>\n",
       "      <td>0.515095</td>\n",
       "      <td>0.792397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.480900</td>\n",
       "      <td>0.501371</td>\n",
       "      <td>0.800356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.406500</td>\n",
       "      <td>0.542004</td>\n",
       "      <td>0.785673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.437000</td>\n",
       "      <td>0.522679</td>\n",
       "      <td>0.790669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.433400</td>\n",
       "      <td>0.525573</td>\n",
       "      <td>0.797323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.464700</td>\n",
       "      <td>0.514663</td>\n",
       "      <td>0.790610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.450600</td>\n",
       "      <td>0.498302</td>\n",
       "      <td>0.796750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.424000</td>\n",
       "      <td>0.531747</td>\n",
       "      <td>0.790616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.524598</td>\n",
       "      <td>0.795897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.445500</td>\n",
       "      <td>0.519502</td>\n",
       "      <td>0.794255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.432000</td>\n",
       "      <td>0.528298</td>\n",
       "      <td>0.795322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.451500</td>\n",
       "      <td>0.527377</td>\n",
       "      <td>0.794495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.454200</td>\n",
       "      <td>0.509064</td>\n",
       "      <td>0.797446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.462100</td>\n",
       "      <td>0.502295</td>\n",
       "      <td>0.796949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.421400</td>\n",
       "      <td>0.510294</td>\n",
       "      <td>0.793644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.421600</td>\n",
       "      <td>0.504479</td>\n",
       "      <td>0.797515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.411300</td>\n",
       "      <td>0.524779</td>\n",
       "      <td>0.790990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.446600</td>\n",
       "      <td>0.512386</td>\n",
       "      <td>0.796132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.500400</td>\n",
       "      <td>0.497523</td>\n",
       "      <td>0.795852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.477400</td>\n",
       "      <td>0.514101</td>\n",
       "      <td>0.790781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.430900</td>\n",
       "      <td>0.509493</td>\n",
       "      <td>0.795615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.350300</td>\n",
       "      <td>0.532055</td>\n",
       "      <td>0.801180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.373400</td>\n",
       "      <td>0.524224</td>\n",
       "      <td>0.796708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.380700</td>\n",
       "      <td>0.538853</td>\n",
       "      <td>0.792526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.519215</td>\n",
       "      <td>0.798413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.385900</td>\n",
       "      <td>0.537208</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.333500</td>\n",
       "      <td>0.565690</td>\n",
       "      <td>0.784550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.546602</td>\n",
       "      <td>0.792762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.357400</td>\n",
       "      <td>0.547930</td>\n",
       "      <td>0.796301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.382400</td>\n",
       "      <td>0.550234</td>\n",
       "      <td>0.791242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.364400</td>\n",
       "      <td>0.540328</td>\n",
       "      <td>0.794890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.364400</td>\n",
       "      <td>0.539740</td>\n",
       "      <td>0.794638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.541799</td>\n",
       "      <td>0.798488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.430300</td>\n",
       "      <td>0.529706</td>\n",
       "      <td>0.795600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.383000</td>\n",
       "      <td>0.529872</td>\n",
       "      <td>0.799927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.342300</td>\n",
       "      <td>0.544988</td>\n",
       "      <td>0.789585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.375800</td>\n",
       "      <td>0.540484</td>\n",
       "      <td>0.793530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.385500</td>\n",
       "      <td>0.537006</td>\n",
       "      <td>0.798788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.409700</td>\n",
       "      <td>0.548980</td>\n",
       "      <td>0.787942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.363900</td>\n",
       "      <td>0.546212</td>\n",
       "      <td>0.790289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.387600</td>\n",
       "      <td>0.540853</td>\n",
       "      <td>0.796116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.365400</td>\n",
       "      <td>0.536804</td>\n",
       "      <td>0.798386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.383600</td>\n",
       "      <td>0.533032</td>\n",
       "      <td>0.796158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.363700</td>\n",
       "      <td>0.530681</td>\n",
       "      <td>0.799256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>0.530999</td>\n",
       "      <td>0.799870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.373200</td>\n",
       "      <td>0.535196</td>\n",
       "      <td>0.798296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.424200</td>\n",
       "      <td>0.541804</td>\n",
       "      <td>0.790621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.333900</td>\n",
       "      <td>0.540269</td>\n",
       "      <td>0.795918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>0.541035</td>\n",
       "      <td>0.795635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.365900</td>\n",
       "      <td>0.539229</td>\n",
       "      <td>0.798891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.353100</td>\n",
       "      <td>0.539574</td>\n",
       "      <td>0.798931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.387100</td>\n",
       "      <td>0.539484</td>\n",
       "      <td>0.798118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.326900</td>\n",
       "      <td>0.540607</td>\n",
       "      <td>0.797832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.427400</td>\n",
       "      <td>0.540932</td>\n",
       "      <td>0.798040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.332100</td>\n",
       "      <td>0.540721</td>\n",
       "      <td>0.798339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500 (score: 0.8011801619794204).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='221' max='221' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [221/221 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▃▆▇█▇█▇████████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▇▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▂▁▁▂▂▁▁▁▁▂▁</td></tr><tr><td>eval/runtime</td><td>▁▂▂▄▂▅▃▄▅▃▄▆▃▅▇▄▅▃▆▃█▃▄▅▇▅▄▃▄▄▄▄▅▄▅▃▄▃▅▅</td></tr><tr><td>eval/samples_per_second</td><td>█▇▇▅▇▃▆▅▄▆▅▃▆▄▂▅▄▆▃▆▁▆▅▄▂▄▅▆▅▅▅▅▄▅▄▆▅▅▄▄</td></tr><tr><td>eval/steps_per_second</td><td>█▇▇▅▇▃▆▅▄▆▅▃▆▄▂▅▄▆▃▆▁▆▅▄▂▄▅▆▅▅▅▅▄▅▄▆▅▅▄▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▂▄████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▆▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▂▂▁▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.80118</td></tr><tr><td>eval/loss</td><td>0.53205</td></tr><tr><td>eval/runtime</td><td>5.8898</td></tr><tr><td>eval/samples_per_second</td><td>600.017</td></tr><tr><td>eval/steps_per_second</td><td>37.522</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>5154</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3321</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.48909</td></tr><tr><td>train/train_runtime</td><td>1134.5039</td></tr><tr><td>train/train_samples_per_second</td><td>72.666</td></tr><tr><td>train/train_steps_per_second</td><td>4.543</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">laced-sweep-24</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/qdp3ulbl\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/qdp3ulbl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_024811-qdp3ulbl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7ir1f0my with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.32520795895725e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 27\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.03184874678544275\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01936428996225857\n",
      "2022-04-28 03:07:45.023040: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_030743-7ir1f0my</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/7ir1f0my\" target=\"_blank\">fast-sweep-25</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe8f2264c704b0ea19bd3afd85e085d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 27\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 27\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3054\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3054' max='3054' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3054/3054 13:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.105300</td>\n",
       "      <td>1.066591</td>\n",
       "      <td>0.418808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.036100</td>\n",
       "      <td>0.946664</td>\n",
       "      <td>0.564470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.869300</td>\n",
       "      <td>0.772916</td>\n",
       "      <td>0.679092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.765800</td>\n",
       "      <td>0.668306</td>\n",
       "      <td>0.736900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.669300</td>\n",
       "      <td>0.625257</td>\n",
       "      <td>0.740017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.645500</td>\n",
       "      <td>0.593119</td>\n",
       "      <td>0.760730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.627700</td>\n",
       "      <td>0.574737</td>\n",
       "      <td>0.772232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>0.570248</td>\n",
       "      <td>0.772047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.581200</td>\n",
       "      <td>0.557937</td>\n",
       "      <td>0.777610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.543600</td>\n",
       "      <td>0.545924</td>\n",
       "      <td>0.783050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.582400</td>\n",
       "      <td>0.594273</td>\n",
       "      <td>0.757752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.609700</td>\n",
       "      <td>0.542893</td>\n",
       "      <td>0.773000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.542200</td>\n",
       "      <td>0.533856</td>\n",
       "      <td>0.781599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.591800</td>\n",
       "      <td>0.527268</td>\n",
       "      <td>0.784847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.578600</td>\n",
       "      <td>0.528117</td>\n",
       "      <td>0.775760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.558200</td>\n",
       "      <td>0.517569</td>\n",
       "      <td>0.790402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.542200</td>\n",
       "      <td>0.528021</td>\n",
       "      <td>0.786110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.536500</td>\n",
       "      <td>0.518617</td>\n",
       "      <td>0.788317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.527100</td>\n",
       "      <td>0.520503</td>\n",
       "      <td>0.786417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.511400</td>\n",
       "      <td>0.518320</td>\n",
       "      <td>0.788100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.469200</td>\n",
       "      <td>0.526457</td>\n",
       "      <td>0.785457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.448400</td>\n",
       "      <td>0.519145</td>\n",
       "      <td>0.790018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.505900</td>\n",
       "      <td>0.515052</td>\n",
       "      <td>0.790843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.485600</td>\n",
       "      <td>0.525823</td>\n",
       "      <td>0.788724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.454900</td>\n",
       "      <td>0.525837</td>\n",
       "      <td>0.793646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.468100</td>\n",
       "      <td>0.510469</td>\n",
       "      <td>0.793655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.457300</td>\n",
       "      <td>0.515235</td>\n",
       "      <td>0.792439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.479000</td>\n",
       "      <td>0.515396</td>\n",
       "      <td>0.793340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.449100</td>\n",
       "      <td>0.503936</td>\n",
       "      <td>0.793599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.481600</td>\n",
       "      <td>0.524662</td>\n",
       "      <td>0.791291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.444600</td>\n",
       "      <td>0.517477</td>\n",
       "      <td>0.793508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.463500</td>\n",
       "      <td>0.502831</td>\n",
       "      <td>0.795789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.467300</td>\n",
       "      <td>0.505057</td>\n",
       "      <td>0.793739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.460500</td>\n",
       "      <td>0.515336</td>\n",
       "      <td>0.792413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.510232</td>\n",
       "      <td>0.791002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.468000</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.795880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>0.501128</td>\n",
       "      <td>0.795141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.433300</td>\n",
       "      <td>0.494060</td>\n",
       "      <td>0.796620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.442600</td>\n",
       "      <td>0.505092</td>\n",
       "      <td>0.795703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.494212</td>\n",
       "      <td>0.799538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.459100</td>\n",
       "      <td>0.502566</td>\n",
       "      <td>0.797157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.399100</td>\n",
       "      <td>0.513027</td>\n",
       "      <td>0.798214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.436400</td>\n",
       "      <td>0.510385</td>\n",
       "      <td>0.797650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.395600</td>\n",
       "      <td>0.521098</td>\n",
       "      <td>0.794900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.388300</td>\n",
       "      <td>0.524186</td>\n",
       "      <td>0.796030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.422300</td>\n",
       "      <td>0.517226</td>\n",
       "      <td>0.795814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.404700</td>\n",
       "      <td>0.520471</td>\n",
       "      <td>0.796408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.402500</td>\n",
       "      <td>0.514371</td>\n",
       "      <td>0.794679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.431100</td>\n",
       "      <td>0.507026</td>\n",
       "      <td>0.797214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.383900</td>\n",
       "      <td>0.522855</td>\n",
       "      <td>0.795209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.427200</td>\n",
       "      <td>0.511936</td>\n",
       "      <td>0.796821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.436100</td>\n",
       "      <td>0.512634</td>\n",
       "      <td>0.792867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.402800</td>\n",
       "      <td>0.509196</td>\n",
       "      <td>0.795447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.414200</td>\n",
       "      <td>0.508410</td>\n",
       "      <td>0.799343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.371400</td>\n",
       "      <td>0.512836</td>\n",
       "      <td>0.796706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.415700</td>\n",
       "      <td>0.513290</td>\n",
       "      <td>0.796697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.395300</td>\n",
       "      <td>0.513675</td>\n",
       "      <td>0.795389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.415900</td>\n",
       "      <td>0.513309</td>\n",
       "      <td>0.797531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.514605</td>\n",
       "      <td>0.797575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.399100</td>\n",
       "      <td>0.514434</td>\n",
       "      <td>0.797243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.401300</td>\n",
       "      <td>0.514529</td>\n",
       "      <td>0.796619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000 (score: 0.7995375287203396).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 27\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='131' max='131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [131/131 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▄▇▇▇▇█▇████████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▇▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁█▃▂▃▃▂▃▂▅▃▆▃▄▄▃▃▄▃▄▃▄▄▄▃▃▂▃▂▃▃▂▄▂▃▄▄▃▃▃</td></tr><tr><td>eval/samples_per_second</td><td>█▁▆▇▆▆▇▆▇▄▆▃▅▅▅▆▆▅▆▅▆▅▅▄▆▆▇▆▇▆▆▇▅▇▆▅▅▆▆▆</td></tr><tr><td>eval/steps_per_second</td><td>█▁▆▇▆▆▇▆▇▄▆▃▅▅▅▆▆▅▆▅▆▅▅▄▆▆▇▆▇▆▆▇▅▇▆▅▅▆▆▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▅███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▅▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79954</td></tr><tr><td>eval/loss</td><td>0.49421</td></tr><tr><td>eval/runtime</td><td>5.8731</td></tr><tr><td>eval/samples_per_second</td><td>601.728</td></tr><tr><td>eval/steps_per_second</td><td>22.305</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>3054</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4013</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.5068</td></tr><tr><td>train/train_runtime</td><td>799.4282</td></tr><tr><td>train/train_samples_per_second</td><td>103.124</td></tr><tr><td>train/train_steps_per_second</td><td>3.82</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fast-sweep-25</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/7ir1f0my\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/7ir1f0my</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_030743-7ir1f0my/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: saa8thy5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.086642532152517e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 23\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.0101318078934975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.014458637946056951\n",
      "2022-04-28 03:21:36.675850: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_032135-saa8thy5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/saa8thy5\" target=\"_blank\">curious-sweep-26</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb45825652de47d2ae74087218385d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 23\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 23\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3585\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3585' max='3585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3585/3585 15:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.084400</td>\n",
       "      <td>1.031780</td>\n",
       "      <td>0.473245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.990600</td>\n",
       "      <td>0.886263</td>\n",
       "      <td>0.617636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.825700</td>\n",
       "      <td>0.748403</td>\n",
       "      <td>0.671433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.738600</td>\n",
       "      <td>0.663237</td>\n",
       "      <td>0.727171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.671400</td>\n",
       "      <td>0.646276</td>\n",
       "      <td>0.729799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.663200</td>\n",
       "      <td>0.611872</td>\n",
       "      <td>0.751150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.628600</td>\n",
       "      <td>0.583312</td>\n",
       "      <td>0.762735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.624700</td>\n",
       "      <td>0.570630</td>\n",
       "      <td>0.775860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.581800</td>\n",
       "      <td>0.566665</td>\n",
       "      <td>0.766574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.599400</td>\n",
       "      <td>0.562384</td>\n",
       "      <td>0.769820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.570800</td>\n",
       "      <td>0.548724</td>\n",
       "      <td>0.777924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.542900</td>\n",
       "      <td>0.550825</td>\n",
       "      <td>0.775770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.573500</td>\n",
       "      <td>0.595234</td>\n",
       "      <td>0.758699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.610800</td>\n",
       "      <td>0.547553</td>\n",
       "      <td>0.770991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.534400</td>\n",
       "      <td>0.548826</td>\n",
       "      <td>0.777136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.591900</td>\n",
       "      <td>0.536138</td>\n",
       "      <td>0.778212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.571200</td>\n",
       "      <td>0.531785</td>\n",
       "      <td>0.778086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.555100</td>\n",
       "      <td>0.542408</td>\n",
       "      <td>0.778402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.528459</td>\n",
       "      <td>0.783870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.547200</td>\n",
       "      <td>0.522629</td>\n",
       "      <td>0.782736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.550800</td>\n",
       "      <td>0.514255</td>\n",
       "      <td>0.788913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.526200</td>\n",
       "      <td>0.540649</td>\n",
       "      <td>0.773526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.496600</td>\n",
       "      <td>0.518081</td>\n",
       "      <td>0.790098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.537600</td>\n",
       "      <td>0.510484</td>\n",
       "      <td>0.791647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.433600</td>\n",
       "      <td>0.544095</td>\n",
       "      <td>0.787528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.466300</td>\n",
       "      <td>0.520404</td>\n",
       "      <td>0.792189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.511700</td>\n",
       "      <td>0.516685</td>\n",
       "      <td>0.789629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.470800</td>\n",
       "      <td>0.515397</td>\n",
       "      <td>0.794916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>0.515263</td>\n",
       "      <td>0.796400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.439900</td>\n",
       "      <td>0.519080</td>\n",
       "      <td>0.797399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.479400</td>\n",
       "      <td>0.511583</td>\n",
       "      <td>0.800025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.470800</td>\n",
       "      <td>0.508101</td>\n",
       "      <td>0.796803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.484900</td>\n",
       "      <td>0.512535</td>\n",
       "      <td>0.797751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.447700</td>\n",
       "      <td>0.510378</td>\n",
       "      <td>0.794248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.506100</td>\n",
       "      <td>0.501195</td>\n",
       "      <td>0.795539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.440400</td>\n",
       "      <td>0.503560</td>\n",
       "      <td>0.800941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.451900</td>\n",
       "      <td>0.508616</td>\n",
       "      <td>0.796408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.471500</td>\n",
       "      <td>0.522212</td>\n",
       "      <td>0.792092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.524888</td>\n",
       "      <td>0.788476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.448500</td>\n",
       "      <td>0.517233</td>\n",
       "      <td>0.794634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.441400</td>\n",
       "      <td>0.505404</td>\n",
       "      <td>0.796520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.511069</td>\n",
       "      <td>0.794738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.479900</td>\n",
       "      <td>0.496466</td>\n",
       "      <td>0.801709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.429800</td>\n",
       "      <td>0.508608</td>\n",
       "      <td>0.798154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.441000</td>\n",
       "      <td>0.508451</td>\n",
       "      <td>0.797291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.451300</td>\n",
       "      <td>0.514565</td>\n",
       "      <td>0.797250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.521600</td>\n",
       "      <td>0.497753</td>\n",
       "      <td>0.798768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.451700</td>\n",
       "      <td>0.502764</td>\n",
       "      <td>0.796234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.385900</td>\n",
       "      <td>0.514349</td>\n",
       "      <td>0.799253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.396600</td>\n",
       "      <td>0.516633</td>\n",
       "      <td>0.796648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.453800</td>\n",
       "      <td>0.508809</td>\n",
       "      <td>0.802621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.354200</td>\n",
       "      <td>0.522291</td>\n",
       "      <td>0.794529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.407900</td>\n",
       "      <td>0.518359</td>\n",
       "      <td>0.799728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.416400</td>\n",
       "      <td>0.521263</td>\n",
       "      <td>0.793921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.399300</td>\n",
       "      <td>0.518875</td>\n",
       "      <td>0.796591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.516750</td>\n",
       "      <td>0.793514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.417300</td>\n",
       "      <td>0.514225</td>\n",
       "      <td>0.794738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.405600</td>\n",
       "      <td>0.513833</td>\n",
       "      <td>0.800784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.394200</td>\n",
       "      <td>0.524499</td>\n",
       "      <td>0.791910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.427600</td>\n",
       "      <td>0.517466</td>\n",
       "      <td>0.796852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.517718</td>\n",
       "      <td>0.791149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.393900</td>\n",
       "      <td>0.514064</td>\n",
       "      <td>0.796286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.418800</td>\n",
       "      <td>0.512188</td>\n",
       "      <td>0.798080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.377300</td>\n",
       "      <td>0.513091</td>\n",
       "      <td>0.797879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.382500</td>\n",
       "      <td>0.515133</td>\n",
       "      <td>0.796604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.422400</td>\n",
       "      <td>0.517731</td>\n",
       "      <td>0.793490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.383400</td>\n",
       "      <td>0.515641</td>\n",
       "      <td>0.799498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.399800</td>\n",
       "      <td>0.517042</td>\n",
       "      <td>0.797362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.369300</td>\n",
       "      <td>0.516688</td>\n",
       "      <td>0.797966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.406400</td>\n",
       "      <td>0.516536</td>\n",
       "      <td>0.798516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.436700</td>\n",
       "      <td>0.517541</td>\n",
       "      <td>0.798412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500 (score: 0.7985161804675033).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 23\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='154' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [154/154 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▄▆▇▇▇▇▇▇▇██▇███████████████████████████</td></tr><tr><td>eval/loss</td><td>█▆▃▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▃▃▃█▄▄▃▃▃▅▄▅▅▄▅▃▇▄▅▄▅▄▃▅▄▄▃▃▃▃▃▅▅▃▃▃▃▃▃</td></tr><tr><td>eval/samples_per_second</td><td>█▆▆▆▁▅▅▆▆▆▄▅▄▄▅▄▆▂▅▄▅▄▅▆▄▅▅▆▆▆▆▆▄▄▆▆▆▆▅▆</td></tr><tr><td>eval/steps_per_second</td><td>█▆▆▆▁▅▅▆▆▆▄▅▄▄▅▄▆▂▅▄▅▄▅▆▄▅▅▆▆▆▆▆▄▄▆▆▆▆▅▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▅▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79852</td></tr><tr><td>eval/loss</td><td>0.51654</td></tr><tr><td>eval/runtime</td><td>6.6418</td></tr><tr><td>eval/samples_per_second</td><td>532.088</td></tr><tr><td>eval/steps_per_second</td><td>23.187</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>3585</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4367</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.49953</td></tr><tr><td>train/train_runtime</td><td>957.0349</td></tr><tr><td>train/train_samples_per_second</td><td>86.141</td></tr><tr><td>train/train_steps_per_second</td><td>3.746</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">curious-sweep-26</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/saa8thy5\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/saa8thy5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_032135-saa8thy5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rrw6lk70 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.994260213703804e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.06756573499615812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.00503673885464899\n",
      "2022-04-28 03:38:04.404217: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_033803-rrw6lk70</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/rrw6lk70\" target=\"_blank\">fiery-sweep-27</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e211e02d00ef4e239f64dc43c7912cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 11\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 11\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7497\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7497' max='7497' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7497/7497 27:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.116900</td>\n",
       "      <td>1.102769</td>\n",
       "      <td>0.282082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.088800</td>\n",
       "      <td>1.079172</td>\n",
       "      <td>0.384215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.088200</td>\n",
       "      <td>1.054098</td>\n",
       "      <td>0.438882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.055500</td>\n",
       "      <td>1.020087</td>\n",
       "      <td>0.477747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.993300</td>\n",
       "      <td>0.949455</td>\n",
       "      <td>0.595846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.938700</td>\n",
       "      <td>0.878495</td>\n",
       "      <td>0.649801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.840200</td>\n",
       "      <td>0.780037</td>\n",
       "      <td>0.672194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.838400</td>\n",
       "      <td>0.704586</td>\n",
       "      <td>0.724159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.737600</td>\n",
       "      <td>0.661241</td>\n",
       "      <td>0.729431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.699500</td>\n",
       "      <td>0.622099</td>\n",
       "      <td>0.753065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.637100</td>\n",
       "      <td>0.615904</td>\n",
       "      <td>0.743761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.665600</td>\n",
       "      <td>0.645955</td>\n",
       "      <td>0.738877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.666500</td>\n",
       "      <td>0.589309</td>\n",
       "      <td>0.760815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.608500</td>\n",
       "      <td>0.629727</td>\n",
       "      <td>0.747001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.604500</td>\n",
       "      <td>0.568035</td>\n",
       "      <td>0.772659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.612800</td>\n",
       "      <td>0.558832</td>\n",
       "      <td>0.775797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.611700</td>\n",
       "      <td>0.560687</td>\n",
       "      <td>0.777317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.569100</td>\n",
       "      <td>0.582586</td>\n",
       "      <td>0.769773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.623500</td>\n",
       "      <td>0.566293</td>\n",
       "      <td>0.771778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.571300</td>\n",
       "      <td>0.552656</td>\n",
       "      <td>0.775570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.563300</td>\n",
       "      <td>0.561800</td>\n",
       "      <td>0.777908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>0.547137</td>\n",
       "      <td>0.787641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>0.543957</td>\n",
       "      <td>0.779255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.523300</td>\n",
       "      <td>0.580797</td>\n",
       "      <td>0.763379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.538700</td>\n",
       "      <td>0.539945</td>\n",
       "      <td>0.774987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.577800</td>\n",
       "      <td>0.571243</td>\n",
       "      <td>0.769604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.507400</td>\n",
       "      <td>0.557201</td>\n",
       "      <td>0.782810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.649100</td>\n",
       "      <td>0.544295</td>\n",
       "      <td>0.780709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.561000</td>\n",
       "      <td>0.539464</td>\n",
       "      <td>0.785156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.522200</td>\n",
       "      <td>0.538935</td>\n",
       "      <td>0.793698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.517800</td>\n",
       "      <td>0.539529</td>\n",
       "      <td>0.787311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.539600</td>\n",
       "      <td>0.557244</td>\n",
       "      <td>0.781355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.614300</td>\n",
       "      <td>0.546438</td>\n",
       "      <td>0.780343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.552600</td>\n",
       "      <td>0.528043</td>\n",
       "      <td>0.788983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.549800</td>\n",
       "      <td>0.529932</td>\n",
       "      <td>0.787273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.593700</td>\n",
       "      <td>0.553025</td>\n",
       "      <td>0.770690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.530200</td>\n",
       "      <td>0.527592</td>\n",
       "      <td>0.787278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.576600</td>\n",
       "      <td>0.546244</td>\n",
       "      <td>0.779360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.512400</td>\n",
       "      <td>0.524299</td>\n",
       "      <td>0.793039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.608700</td>\n",
       "      <td>0.525760</td>\n",
       "      <td>0.784299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.556500</td>\n",
       "      <td>0.562026</td>\n",
       "      <td>0.783034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.489100</td>\n",
       "      <td>0.548497</td>\n",
       "      <td>0.778868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.492900</td>\n",
       "      <td>0.533307</td>\n",
       "      <td>0.787161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.549900</td>\n",
       "      <td>0.523937</td>\n",
       "      <td>0.793048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.571800</td>\n",
       "      <td>0.517386</td>\n",
       "      <td>0.790155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.512600</td>\n",
       "      <td>0.545620</td>\n",
       "      <td>0.780990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.485300</td>\n",
       "      <td>0.537291</td>\n",
       "      <td>0.786894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.439900</td>\n",
       "      <td>0.523833</td>\n",
       "      <td>0.794576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>0.533817</td>\n",
       "      <td>0.790354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.521600</td>\n",
       "      <td>0.529851</td>\n",
       "      <td>0.787303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.393300</td>\n",
       "      <td>0.532749</td>\n",
       "      <td>0.795774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.407400</td>\n",
       "      <td>0.566953</td>\n",
       "      <td>0.794299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.438900</td>\n",
       "      <td>0.528639</td>\n",
       "      <td>0.797458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.422400</td>\n",
       "      <td>0.541155</td>\n",
       "      <td>0.795752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.527600</td>\n",
       "      <td>0.526964</td>\n",
       "      <td>0.791904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.458500</td>\n",
       "      <td>0.531109</td>\n",
       "      <td>0.798259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.460100</td>\n",
       "      <td>0.511602</td>\n",
       "      <td>0.794529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.443400</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>0.794621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.435700</td>\n",
       "      <td>0.558118</td>\n",
       "      <td>0.789268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.451000</td>\n",
       "      <td>0.528527</td>\n",
       "      <td>0.796314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.440100</td>\n",
       "      <td>0.556586</td>\n",
       "      <td>0.784928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.390500</td>\n",
       "      <td>0.550652</td>\n",
       "      <td>0.797045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.456600</td>\n",
       "      <td>0.535754</td>\n",
       "      <td>0.801797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.482400</td>\n",
       "      <td>0.526767</td>\n",
       "      <td>0.794686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.415500</td>\n",
       "      <td>0.545796</td>\n",
       "      <td>0.799562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.425200</td>\n",
       "      <td>0.555921</td>\n",
       "      <td>0.794635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.473200</td>\n",
       "      <td>0.536436</td>\n",
       "      <td>0.792237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.432900</td>\n",
       "      <td>0.564774</td>\n",
       "      <td>0.787925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.443100</td>\n",
       "      <td>0.550041</td>\n",
       "      <td>0.798668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.424600</td>\n",
       "      <td>0.567949</td>\n",
       "      <td>0.790392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.418400</td>\n",
       "      <td>0.537197</td>\n",
       "      <td>0.794057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.454200</td>\n",
       "      <td>0.532640</td>\n",
       "      <td>0.792336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.519700</td>\n",
       "      <td>0.509477</td>\n",
       "      <td>0.796782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.411100</td>\n",
       "      <td>0.544812</td>\n",
       "      <td>0.795037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.540523</td>\n",
       "      <td>0.799961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.402200</td>\n",
       "      <td>0.565200</td>\n",
       "      <td>0.796250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.474600</td>\n",
       "      <td>0.557218</td>\n",
       "      <td>0.794673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.452700</td>\n",
       "      <td>0.537266</td>\n",
       "      <td>0.795761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.428000</td>\n",
       "      <td>0.540636</td>\n",
       "      <td>0.795964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.476500</td>\n",
       "      <td>0.515050</td>\n",
       "      <td>0.799457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.427200</td>\n",
       "      <td>0.556354</td>\n",
       "      <td>0.787810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.457400</td>\n",
       "      <td>0.541075</td>\n",
       "      <td>0.792796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.398100</td>\n",
       "      <td>0.547091</td>\n",
       "      <td>0.794769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.548788</td>\n",
       "      <td>0.792831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.419500</td>\n",
       "      <td>0.538029</td>\n",
       "      <td>0.797011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.441500</td>\n",
       "      <td>0.550545</td>\n",
       "      <td>0.791375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.425800</td>\n",
       "      <td>0.544207</td>\n",
       "      <td>0.794868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.453100</td>\n",
       "      <td>0.531272</td>\n",
       "      <td>0.798596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.465600</td>\n",
       "      <td>0.526428</td>\n",
       "      <td>0.799437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.463000</td>\n",
       "      <td>0.517136</td>\n",
       "      <td>0.797978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.526801</td>\n",
       "      <td>0.796987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>0.535877</td>\n",
       "      <td>0.800146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.436600</td>\n",
       "      <td>0.515842</td>\n",
       "      <td>0.799404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>0.524472</td>\n",
       "      <td>0.799661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.433300</td>\n",
       "      <td>0.533239</td>\n",
       "      <td>0.798951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.456600</td>\n",
       "      <td>0.525458</td>\n",
       "      <td>0.793674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>0.518395</td>\n",
       "      <td>0.790063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.511700</td>\n",
       "      <td>0.507763</td>\n",
       "      <td>0.800189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>0.516988</td>\n",
       "      <td>0.797467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.436800</td>\n",
       "      <td>0.522196</td>\n",
       "      <td>0.789423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.342400</td>\n",
       "      <td>0.548714</td>\n",
       "      <td>0.795814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.290600</td>\n",
       "      <td>0.565996</td>\n",
       "      <td>0.798846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.394500</td>\n",
       "      <td>0.556023</td>\n",
       "      <td>0.798118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.344800</td>\n",
       "      <td>0.553174</td>\n",
       "      <td>0.793459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.354100</td>\n",
       "      <td>0.574308</td>\n",
       "      <td>0.792244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.371600</td>\n",
       "      <td>0.551621</td>\n",
       "      <td>0.799632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>0.566131</td>\n",
       "      <td>0.797559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.314800</td>\n",
       "      <td>0.581536</td>\n",
       "      <td>0.796864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.297800</td>\n",
       "      <td>0.617809</td>\n",
       "      <td>0.784434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.358900</td>\n",
       "      <td>0.590752</td>\n",
       "      <td>0.793371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.340500</td>\n",
       "      <td>0.586791</td>\n",
       "      <td>0.802453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.363000</td>\n",
       "      <td>0.581428</td>\n",
       "      <td>0.797566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.390500</td>\n",
       "      <td>0.573579</td>\n",
       "      <td>0.796296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.333100</td>\n",
       "      <td>0.584214</td>\n",
       "      <td>0.800897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.354200</td>\n",
       "      <td>0.590720</td>\n",
       "      <td>0.792839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.354300</td>\n",
       "      <td>0.587848</td>\n",
       "      <td>0.794437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.607205</td>\n",
       "      <td>0.791175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.374800</td>\n",
       "      <td>0.586237</td>\n",
       "      <td>0.796545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.397500</td>\n",
       "      <td>0.575498</td>\n",
       "      <td>0.793078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.357300</td>\n",
       "      <td>0.574508</td>\n",
       "      <td>0.797275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.344200</td>\n",
       "      <td>0.572221</td>\n",
       "      <td>0.794886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.334900</td>\n",
       "      <td>0.573113</td>\n",
       "      <td>0.794407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.339400</td>\n",
       "      <td>0.590283</td>\n",
       "      <td>0.789463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.381900</td>\n",
       "      <td>0.573520</td>\n",
       "      <td>0.795637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.348600</td>\n",
       "      <td>0.568075</td>\n",
       "      <td>0.796062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.369900</td>\n",
       "      <td>0.572692</td>\n",
       "      <td>0.793280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.381700</td>\n",
       "      <td>0.578773</td>\n",
       "      <td>0.791242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>0.580613</td>\n",
       "      <td>0.787219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>0.371600</td>\n",
       "      <td>0.571065</td>\n",
       "      <td>0.793282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.374500</td>\n",
       "      <td>0.561367</td>\n",
       "      <td>0.795046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>0.329500</td>\n",
       "      <td>0.561453</td>\n",
       "      <td>0.796498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>0.560436</td>\n",
       "      <td>0.797353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>0.320800</td>\n",
       "      <td>0.560072</td>\n",
       "      <td>0.796018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.562241</td>\n",
       "      <td>0.797759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.316000</td>\n",
       "      <td>0.569947</td>\n",
       "      <td>0.799604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.371900</td>\n",
       "      <td>0.573021</td>\n",
       "      <td>0.798283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>0.341900</td>\n",
       "      <td>0.571940</td>\n",
       "      <td>0.793870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.421200</td>\n",
       "      <td>0.573228</td>\n",
       "      <td>0.794286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>0.320400</td>\n",
       "      <td>0.572875</td>\n",
       "      <td>0.794032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.358400</td>\n",
       "      <td>0.572046</td>\n",
       "      <td>0.794951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.578143</td>\n",
       "      <td>0.794515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.361500</td>\n",
       "      <td>0.577975</td>\n",
       "      <td>0.795430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>0.343700</td>\n",
       "      <td>0.575836</td>\n",
       "      <td>0.795810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.335600</td>\n",
       "      <td>0.578116</td>\n",
       "      <td>0.794708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.396900</td>\n",
       "      <td>0.577587</td>\n",
       "      <td>0.796150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.577289</td>\n",
       "      <td>0.796747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>0.333400</td>\n",
       "      <td>0.578189</td>\n",
       "      <td>0.796111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.384500</td>\n",
       "      <td>0.577696</td>\n",
       "      <td>0.796678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>0.374100</td>\n",
       "      <td>0.577108</td>\n",
       "      <td>0.796965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-7000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-7000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-7000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000 (score: 0.7994573804949225).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='322' max='322' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [322/322 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▃▇▇██▇██▇██████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▇▃▃▂▁▂▁▂▁▁▁▁▁▁▁▁▂▂▁▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>eval/runtime</td><td>▁▇▃▂▂▃▃▄█▃▃▄▃▄▂▃▄▃▄▄▅▃▄█▄▆▃▃▄▃▃▅▃▄▄▄▃▅▃▂</td></tr><tr><td>eval/samples_per_second</td><td>█▂▆▇▇▆▆▅▁▆▆▅▆▅▇▆▅▆▅▅▄▆▅▁▅▃▆▆▅▆▆▄▆▅▅▅▆▄▆▇</td></tr><tr><td>eval/steps_per_second</td><td>█▂▆▇▇▆▆▅▁▆▆▅▆▅▇▆▅▆▅▅▄▆▅▁▅▃▆▆▅▆▆▄▆▅▅▅▆▄▆▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▂▄▇███▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▆▄▄▃▃▄▃▃▄▃▃▃▂▂▂▃▂▂▂▃▂▂▃▂▃▁▂▂▁▂▂▂▁▂▂▂▁▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79946</td></tr><tr><td>eval/loss</td><td>0.51505</td></tr><tr><td>eval/runtime</td><td>6.6262</td></tr><tr><td>eval/samples_per_second</td><td>533.34</td></tr><tr><td>eval/steps_per_second</td><td>48.595</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>7497</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3741</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.47762</td></tr><tr><td>train/train_runtime</td><td>1644.8741</td></tr><tr><td>train/train_samples_per_second</td><td>50.119</td></tr><tr><td>train/train_steps_per_second</td><td>4.558</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fiery-sweep-27</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/rrw6lk70\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/rrw6lk70</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_033803-rrw6lk70/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hbjad4a8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.485042293629492e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.09753530955286356\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.008711860137409115\n",
      "2022-04-28 04:06:08.562841: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_040607-hbjad4a8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/hbjad4a8\" target=\"_blank\">zany-sweep-28</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532f9163883f4861bfa6ab305c8e9980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6870\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6870' max='6870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6870/6870 24:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.111400</td>\n",
       "      <td>1.105937</td>\n",
       "      <td>0.265571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.097500</td>\n",
       "      <td>1.092935</td>\n",
       "      <td>0.330832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.098300</td>\n",
       "      <td>1.078800</td>\n",
       "      <td>0.367378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.070000</td>\n",
       "      <td>1.051086</td>\n",
       "      <td>0.411678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.077400</td>\n",
       "      <td>1.030608</td>\n",
       "      <td>0.489210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.030500</td>\n",
       "      <td>0.987312</td>\n",
       "      <td>0.541452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.965200</td>\n",
       "      <td>0.922078</td>\n",
       "      <td>0.603284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.917200</td>\n",
       "      <td>0.871897</td>\n",
       "      <td>0.652947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.851600</td>\n",
       "      <td>0.808981</td>\n",
       "      <td>0.644547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.786100</td>\n",
       "      <td>0.770670</td>\n",
       "      <td>0.656340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.840400</td>\n",
       "      <td>0.707708</td>\n",
       "      <td>0.722266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.729300</td>\n",
       "      <td>0.691730</td>\n",
       "      <td>0.703440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.710300</td>\n",
       "      <td>0.660224</td>\n",
       "      <td>0.735031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.680600</td>\n",
       "      <td>0.631271</td>\n",
       "      <td>0.748118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.609800</td>\n",
       "      <td>0.649521</td>\n",
       "      <td>0.741142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.712100</td>\n",
       "      <td>0.636771</td>\n",
       "      <td>0.742193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.606900</td>\n",
       "      <td>0.636743</td>\n",
       "      <td>0.753736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.666100</td>\n",
       "      <td>0.605377</td>\n",
       "      <td>0.758984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.626200</td>\n",
       "      <td>0.593618</td>\n",
       "      <td>0.756921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.571400</td>\n",
       "      <td>0.581557</td>\n",
       "      <td>0.774459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.634800</td>\n",
       "      <td>0.601126</td>\n",
       "      <td>0.757801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.644200</td>\n",
       "      <td>0.555592</td>\n",
       "      <td>0.780066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.648000</td>\n",
       "      <td>0.567077</td>\n",
       "      <td>0.772358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>0.583104</td>\n",
       "      <td>0.768834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.555800</td>\n",
       "      <td>0.574377</td>\n",
       "      <td>0.763358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.625900</td>\n",
       "      <td>0.573448</td>\n",
       "      <td>0.766275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.586500</td>\n",
       "      <td>0.561997</td>\n",
       "      <td>0.778337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.562300</td>\n",
       "      <td>0.548788</td>\n",
       "      <td>0.779930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.583300</td>\n",
       "      <td>0.546106</td>\n",
       "      <td>0.786203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.531000</td>\n",
       "      <td>0.583153</td>\n",
       "      <td>0.771043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.613500</td>\n",
       "      <td>0.553459</td>\n",
       "      <td>0.783289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.592800</td>\n",
       "      <td>0.561396</td>\n",
       "      <td>0.779300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.536100</td>\n",
       "      <td>0.622195</td>\n",
       "      <td>0.758624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.590400</td>\n",
       "      <td>0.546046</td>\n",
       "      <td>0.781124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.547336</td>\n",
       "      <td>0.772171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.534200</td>\n",
       "      <td>0.567737</td>\n",
       "      <td>0.765249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.540500</td>\n",
       "      <td>0.562148</td>\n",
       "      <td>0.772504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.619400</td>\n",
       "      <td>0.553217</td>\n",
       "      <td>0.770646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.596100</td>\n",
       "      <td>0.531893</td>\n",
       "      <td>0.784097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.566600</td>\n",
       "      <td>0.566756</td>\n",
       "      <td>0.773253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.521000</td>\n",
       "      <td>0.555072</td>\n",
       "      <td>0.786467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.489600</td>\n",
       "      <td>0.557119</td>\n",
       "      <td>0.782679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.565100</td>\n",
       "      <td>0.537617</td>\n",
       "      <td>0.788933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.542900</td>\n",
       "      <td>0.561421</td>\n",
       "      <td>0.785761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.659700</td>\n",
       "      <td>0.545695</td>\n",
       "      <td>0.778157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.524808</td>\n",
       "      <td>0.788393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.541100</td>\n",
       "      <td>0.553422</td>\n",
       "      <td>0.781103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.589000</td>\n",
       "      <td>0.532148</td>\n",
       "      <td>0.788628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.580600</td>\n",
       "      <td>0.521941</td>\n",
       "      <td>0.787491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.586600</td>\n",
       "      <td>0.536660</td>\n",
       "      <td>0.785152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.561400</td>\n",
       "      <td>0.526120</td>\n",
       "      <td>0.785779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.555900</td>\n",
       "      <td>0.547489</td>\n",
       "      <td>0.780734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.559900</td>\n",
       "      <td>0.526063</td>\n",
       "      <td>0.785089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.544200</td>\n",
       "      <td>0.531079</td>\n",
       "      <td>0.791066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.598400</td>\n",
       "      <td>0.536000</td>\n",
       "      <td>0.782959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.537400</td>\n",
       "      <td>0.546643</td>\n",
       "      <td>0.782831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.530700</td>\n",
       "      <td>0.567065</td>\n",
       "      <td>0.783136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.481600</td>\n",
       "      <td>0.563849</td>\n",
       "      <td>0.776820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.553500</td>\n",
       "      <td>0.524645</td>\n",
       "      <td>0.787406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.550700</td>\n",
       "      <td>0.530143</td>\n",
       "      <td>0.790913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.540200</td>\n",
       "      <td>0.527505</td>\n",
       "      <td>0.791600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.601800</td>\n",
       "      <td>0.521239</td>\n",
       "      <td>0.791041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>0.553694</td>\n",
       "      <td>0.782030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.433600</td>\n",
       "      <td>0.555426</td>\n",
       "      <td>0.788802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.507800</td>\n",
       "      <td>0.522197</td>\n",
       "      <td>0.789022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.542159</td>\n",
       "      <td>0.790873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.539700</td>\n",
       "      <td>0.535633</td>\n",
       "      <td>0.791647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>0.532717</td>\n",
       "      <td>0.788781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.502400</td>\n",
       "      <td>0.526803</td>\n",
       "      <td>0.794698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.382400</td>\n",
       "      <td>0.562203</td>\n",
       "      <td>0.788939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.389600</td>\n",
       "      <td>0.590373</td>\n",
       "      <td>0.790408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.440900</td>\n",
       "      <td>0.583939</td>\n",
       "      <td>0.791665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.556861</td>\n",
       "      <td>0.796013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.439700</td>\n",
       "      <td>0.559808</td>\n",
       "      <td>0.794288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.564300</td>\n",
       "      <td>0.537343</td>\n",
       "      <td>0.794970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.493200</td>\n",
       "      <td>0.543155</td>\n",
       "      <td>0.791140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.458200</td>\n",
       "      <td>0.550811</td>\n",
       "      <td>0.792562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.455300</td>\n",
       "      <td>0.531418</td>\n",
       "      <td>0.795864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.448600</td>\n",
       "      <td>0.538746</td>\n",
       "      <td>0.794054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.440800</td>\n",
       "      <td>0.546646</td>\n",
       "      <td>0.793109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.473500</td>\n",
       "      <td>0.556406</td>\n",
       "      <td>0.788784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>0.566742</td>\n",
       "      <td>0.788115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.423800</td>\n",
       "      <td>0.552292</td>\n",
       "      <td>0.794445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.432700</td>\n",
       "      <td>0.577652</td>\n",
       "      <td>0.791803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.387000</td>\n",
       "      <td>0.585181</td>\n",
       "      <td>0.795789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.475400</td>\n",
       "      <td>0.587643</td>\n",
       "      <td>0.797289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.475300</td>\n",
       "      <td>0.571669</td>\n",
       "      <td>0.795614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.512300</td>\n",
       "      <td>0.549352</td>\n",
       "      <td>0.793710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.434500</td>\n",
       "      <td>0.541577</td>\n",
       "      <td>0.794662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.574552</td>\n",
       "      <td>0.796269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.528600</td>\n",
       "      <td>0.557924</td>\n",
       "      <td>0.792206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.426600</td>\n",
       "      <td>0.561277</td>\n",
       "      <td>0.791707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.474200</td>\n",
       "      <td>0.542061</td>\n",
       "      <td>0.796033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.460300</td>\n",
       "      <td>0.558520</td>\n",
       "      <td>0.797921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.468100</td>\n",
       "      <td>0.557993</td>\n",
       "      <td>0.799659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.414100</td>\n",
       "      <td>0.589870</td>\n",
       "      <td>0.786771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.391400</td>\n",
       "      <td>0.566393</td>\n",
       "      <td>0.794263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.505100</td>\n",
       "      <td>0.546481</td>\n",
       "      <td>0.795635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.415200</td>\n",
       "      <td>0.543077</td>\n",
       "      <td>0.795464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.506800</td>\n",
       "      <td>0.529037</td>\n",
       "      <td>0.796304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.455800</td>\n",
       "      <td>0.551488</td>\n",
       "      <td>0.791487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.352300</td>\n",
       "      <td>0.581813</td>\n",
       "      <td>0.790813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.551200</td>\n",
       "      <td>0.555181</td>\n",
       "      <td>0.796422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.376400</td>\n",
       "      <td>0.562719</td>\n",
       "      <td>0.795031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.491200</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.796734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.468000</td>\n",
       "      <td>0.563591</td>\n",
       "      <td>0.795383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.484300</td>\n",
       "      <td>0.541281</td>\n",
       "      <td>0.799145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.444900</td>\n",
       "      <td>0.559461</td>\n",
       "      <td>0.793148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.464000</td>\n",
       "      <td>0.561250</td>\n",
       "      <td>0.792047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.461400</td>\n",
       "      <td>0.539582</td>\n",
       "      <td>0.797816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.425500</td>\n",
       "      <td>0.568038</td>\n",
       "      <td>0.792354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.453900</td>\n",
       "      <td>0.559916</td>\n",
       "      <td>0.792180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.424500</td>\n",
       "      <td>0.566243</td>\n",
       "      <td>0.793979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.394700</td>\n",
       "      <td>0.560415</td>\n",
       "      <td>0.794697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.456000</td>\n",
       "      <td>0.553639</td>\n",
       "      <td>0.793758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.407300</td>\n",
       "      <td>0.551674</td>\n",
       "      <td>0.793643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.477300</td>\n",
       "      <td>0.544460</td>\n",
       "      <td>0.796494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.402000</td>\n",
       "      <td>0.541948</td>\n",
       "      <td>0.796053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.548256</td>\n",
       "      <td>0.796956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.420100</td>\n",
       "      <td>0.558256</td>\n",
       "      <td>0.797941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.437800</td>\n",
       "      <td>0.543205</td>\n",
       "      <td>0.795924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.492600</td>\n",
       "      <td>0.533475</td>\n",
       "      <td>0.799196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.458900</td>\n",
       "      <td>0.536826</td>\n",
       "      <td>0.799464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.444800</td>\n",
       "      <td>0.537519</td>\n",
       "      <td>0.799007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.438900</td>\n",
       "      <td>0.545090</td>\n",
       "      <td>0.797680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.370500</td>\n",
       "      <td>0.545918</td>\n",
       "      <td>0.797491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.408800</td>\n",
       "      <td>0.543938</td>\n",
       "      <td>0.798270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.419800</td>\n",
       "      <td>0.542498</td>\n",
       "      <td>0.800001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>0.406400</td>\n",
       "      <td>0.547062</td>\n",
       "      <td>0.799465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.418900</td>\n",
       "      <td>0.547732</td>\n",
       "      <td>0.799065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>0.395100</td>\n",
       "      <td>0.547596</td>\n",
       "      <td>0.800264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.492300</td>\n",
       "      <td>0.546854</td>\n",
       "      <td>0.799505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>0.461900</td>\n",
       "      <td>0.545376</td>\n",
       "      <td>0.798365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.583200</td>\n",
       "      <td>0.541371</td>\n",
       "      <td>0.798389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.446700</td>\n",
       "      <td>0.541146</td>\n",
       "      <td>0.798447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.468600</td>\n",
       "      <td>0.541915</td>\n",
       "      <td>0.798928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>0.440100</td>\n",
       "      <td>0.542474</td>\n",
       "      <td>0.798907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-6500 (score: 0.7990645844191246).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='442' max='442' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [442/442 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▂▆▇▇▇████▇█████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▇▅▃▂▂▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▄▄▂▄▄▃▂▄▄▅▄▂▅▃▃▃▅▃▅▄▆█▃▃▄▄▃▄▃▃▃▅▄▅▆▅▃▄▃</td></tr><tr><td>eval/samples_per_second</td><td>█▅▅▇▅▅▆▇▅▅▄▅▇▄▆▆▆▄▆▄▄▃▁▆▆▅▅▅▅▆▆▆▄▅▄▃▄▆▅▆</td></tr><tr><td>eval/steps_per_second</td><td>█▅▅▇▅▅▆▇▅▅▄▅▇▄▆▆▆▄▆▄▄▃▁▆▆▅▅▅▅▆▆▆▄▅▄▃▄▆▅▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▆▅▄▄▄▄▃▃▃▃▂▃▃▃▃▃▃▂▁▂▂▂▂▂▁▂▂▂▁▂▂▁▁▂▂▂▂▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79906</td></tr><tr><td>eval/loss</td><td>0.54773</td></tr><tr><td>eval/runtime</td><td>7.0391</td></tr><tr><td>eval/samples_per_second</td><td>502.051</td></tr><tr><td>eval/steps_per_second</td><td>62.792</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>6870</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4401</td></tr><tr><td>train/total_flos</td><td>1977050665396800.0</td></tr><tr><td>train/train_loss</td><td>0.54524</td></tr><tr><td>train/train_runtime</td><td>1454.3054</td></tr><tr><td>train/train_samples_per_second</td><td>37.791</td></tr><tr><td>train/train_steps_per_second</td><td>4.724</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">zany-sweep-28</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/hbjad4a8\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/hbjad4a8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_040607-hbjad4a8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yiooorms with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.924254907729883e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.06683568599979443\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01968109060260556\n",
      "2022-04-28 04:30:55.804752: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_043054-yiooorms</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/yiooorms\" target=\"_blank\">dark-sweep-29</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd38d51f49b47a29008add2a670296f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 14\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 14\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3926\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3926' max='3926' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3926/3926 13:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.110800</td>\n",
       "      <td>1.092831</td>\n",
       "      <td>0.340434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.085100</td>\n",
       "      <td>1.057159</td>\n",
       "      <td>0.434202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.054100</td>\n",
       "      <td>1.013103</td>\n",
       "      <td>0.536075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.970600</td>\n",
       "      <td>0.880632</td>\n",
       "      <td>0.658487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.863900</td>\n",
       "      <td>0.751814</td>\n",
       "      <td>0.691469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.752200</td>\n",
       "      <td>0.655270</td>\n",
       "      <td>0.739702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.635328</td>\n",
       "      <td>0.739465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.661400</td>\n",
       "      <td>0.601155</td>\n",
       "      <td>0.757512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.626600</td>\n",
       "      <td>0.616118</td>\n",
       "      <td>0.750414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.641900</td>\n",
       "      <td>0.591134</td>\n",
       "      <td>0.765319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.607600</td>\n",
       "      <td>0.580904</td>\n",
       "      <td>0.762666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.582400</td>\n",
       "      <td>0.568131</td>\n",
       "      <td>0.775503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.623200</td>\n",
       "      <td>0.544164</td>\n",
       "      <td>0.784034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.575700</td>\n",
       "      <td>0.557579</td>\n",
       "      <td>0.778798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.599800</td>\n",
       "      <td>0.553362</td>\n",
       "      <td>0.776258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.560100</td>\n",
       "      <td>0.559825</td>\n",
       "      <td>0.777516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.545600</td>\n",
       "      <td>0.553042</td>\n",
       "      <td>0.780157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.532849</td>\n",
       "      <td>0.783584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.513500</td>\n",
       "      <td>0.539319</td>\n",
       "      <td>0.781778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.581300</td>\n",
       "      <td>0.545948</td>\n",
       "      <td>0.775933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.519700</td>\n",
       "      <td>0.565089</td>\n",
       "      <td>0.773819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.632700</td>\n",
       "      <td>0.530418</td>\n",
       "      <td>0.784080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.547800</td>\n",
       "      <td>0.536635</td>\n",
       "      <td>0.788700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.515300</td>\n",
       "      <td>0.539889</td>\n",
       "      <td>0.790287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.557200</td>\n",
       "      <td>0.533534</td>\n",
       "      <td>0.786014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.581600</td>\n",
       "      <td>0.525193</td>\n",
       "      <td>0.787109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.539800</td>\n",
       "      <td>0.532284</td>\n",
       "      <td>0.787551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.595100</td>\n",
       "      <td>0.517521</td>\n",
       "      <td>0.787868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.561600</td>\n",
       "      <td>0.522717</td>\n",
       "      <td>0.787381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.542600</td>\n",
       "      <td>0.539879</td>\n",
       "      <td>0.779495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.539900</td>\n",
       "      <td>0.520023</td>\n",
       "      <td>0.789675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.538559</td>\n",
       "      <td>0.787124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.534700</td>\n",
       "      <td>0.524580</td>\n",
       "      <td>0.786111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.507400</td>\n",
       "      <td>0.516634</td>\n",
       "      <td>0.790539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.581400</td>\n",
       "      <td>0.510430</td>\n",
       "      <td>0.791580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.523000</td>\n",
       "      <td>0.527749</td>\n",
       "      <td>0.785506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.479700</td>\n",
       "      <td>0.519809</td>\n",
       "      <td>0.793271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.474400</td>\n",
       "      <td>0.512575</td>\n",
       "      <td>0.792458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.499400</td>\n",
       "      <td>0.515511</td>\n",
       "      <td>0.791022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>0.522007</td>\n",
       "      <td>0.792884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.406200</td>\n",
       "      <td>0.539097</td>\n",
       "      <td>0.795066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.433200</td>\n",
       "      <td>0.521168</td>\n",
       "      <td>0.795467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.508200</td>\n",
       "      <td>0.521181</td>\n",
       "      <td>0.797823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.479700</td>\n",
       "      <td>0.518156</td>\n",
       "      <td>0.795761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.449100</td>\n",
       "      <td>0.511404</td>\n",
       "      <td>0.794756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>0.527829</td>\n",
       "      <td>0.794809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.466800</td>\n",
       "      <td>0.512942</td>\n",
       "      <td>0.792515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.423200</td>\n",
       "      <td>0.528741</td>\n",
       "      <td>0.792712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.397700</td>\n",
       "      <td>0.522509</td>\n",
       "      <td>0.796937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.470100</td>\n",
       "      <td>0.530346</td>\n",
       "      <td>0.795207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.429400</td>\n",
       "      <td>0.521185</td>\n",
       "      <td>0.799342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.468100</td>\n",
       "      <td>0.519023</td>\n",
       "      <td>0.793784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.459100</td>\n",
       "      <td>0.509017</td>\n",
       "      <td>0.796929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.448700</td>\n",
       "      <td>0.524491</td>\n",
       "      <td>0.794548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.396300</td>\n",
       "      <td>0.544211</td>\n",
       "      <td>0.792743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.435300</td>\n",
       "      <td>0.520809</td>\n",
       "      <td>0.792841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.477400</td>\n",
       "      <td>0.503096</td>\n",
       "      <td>0.795247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.423900</td>\n",
       "      <td>0.542217</td>\n",
       "      <td>0.788604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.449800</td>\n",
       "      <td>0.521611</td>\n",
       "      <td>0.796245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.411000</td>\n",
       "      <td>0.534661</td>\n",
       "      <td>0.796348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.466900</td>\n",
       "      <td>0.515172</td>\n",
       "      <td>0.799527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.422500</td>\n",
       "      <td>0.520094</td>\n",
       "      <td>0.797425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.457200</td>\n",
       "      <td>0.507554</td>\n",
       "      <td>0.798204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.440100</td>\n",
       "      <td>0.521374</td>\n",
       "      <td>0.795847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.394600</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.794450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.424800</td>\n",
       "      <td>0.527913</td>\n",
       "      <td>0.791230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.431600</td>\n",
       "      <td>0.514369</td>\n",
       "      <td>0.798462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>0.519186</td>\n",
       "      <td>0.796559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.438500</td>\n",
       "      <td>0.521759</td>\n",
       "      <td>0.795340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.471400</td>\n",
       "      <td>0.510778</td>\n",
       "      <td>0.798722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.445800</td>\n",
       "      <td>0.509181</td>\n",
       "      <td>0.799705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.396800</td>\n",
       "      <td>0.514095</td>\n",
       "      <td>0.797351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.425800</td>\n",
       "      <td>0.509798</td>\n",
       "      <td>0.798123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.391800</td>\n",
       "      <td>0.513410</td>\n",
       "      <td>0.798623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.416900</td>\n",
       "      <td>0.515283</td>\n",
       "      <td>0.795933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.478400</td>\n",
       "      <td>0.514655</td>\n",
       "      <td>0.795182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.514900</td>\n",
       "      <td>0.511809</td>\n",
       "      <td>0.794759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.473100</td>\n",
       "      <td>0.512559</td>\n",
       "      <td>0.794740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500 (score: 0.7987216379873137).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='253' max='253' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [253/253 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▄▆▇▇▇██████████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▇▄▃▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▃▃▄▃▂▂▃▂▂▁▃█▂▂▁▂▂▃▂▂▃▂▂▂▃▂▂▂▃▁▂▁▃▄▂▂▂▃▂</td></tr><tr><td>eval/samples_per_second</td><td>▇▆▆▅▆▇▇▆▇▇█▆▁▇▇█▇▇▆▇▇▆▇▇▇▆▇▇▇▆█▇█▆▅▇▇▇▆▇</td></tr><tr><td>eval/steps_per_second</td><td>▇▆▆▅▆▇▇▆▇▇█▆▁▇▇█▇▇▆▇▇▆▇▇▇▆▇▇▇▆█▇█▆▅▇▇▇▆▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▂▄▆███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▇▅▄▃▃▃▃▃▃▃▂▃▃▂▃▂▂▂▂▁▂▂▁▂▂▂▁▁▁▁▁▁▂▂▁▁▂▂</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79872</td></tr><tr><td>eval/loss</td><td>0.51078</td></tr><tr><td>eval/runtime</td><td>5.8125</td></tr><tr><td>eval/samples_per_second</td><td>608.004</td></tr><tr><td>eval/steps_per_second</td><td>43.527</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>3926</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4731</td></tr><tr><td>train/total_flos</td><td>1977050665396800.0</td></tr><tr><td>train/train_loss</td><td>0.53527</td></tr><tr><td>train/train_runtime</td><td>830.3943</td></tr><tr><td>train/train_samples_per_second</td><td>66.185</td></tr><tr><td>train/train_steps_per_second</td><td>4.728</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dark-sweep-29</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/yiooorms\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/yiooorms</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_043054-yiooorms/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: amnuaim1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.946671407381753e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 17\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.03675389145958507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.014297433006695273\n",
      "2022-04-28 04:45:17.482843: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_044516-amnuaim1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/amnuaim1\" target=\"_blank\">exalted-sweep-30</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec35c8641ff491faad8a9ae3e720013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 17\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 17\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4851\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4851' max='4851' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4851/4851 18:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.104600</td>\n",
       "      <td>1.086370</td>\n",
       "      <td>0.357139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.076300</td>\n",
       "      <td>1.035122</td>\n",
       "      <td>0.477285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.006900</td>\n",
       "      <td>0.942968</td>\n",
       "      <td>0.609309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.864900</td>\n",
       "      <td>0.771823</td>\n",
       "      <td>0.665207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.762700</td>\n",
       "      <td>0.659330</td>\n",
       "      <td>0.743029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.691200</td>\n",
       "      <td>0.623253</td>\n",
       "      <td>0.749359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.638900</td>\n",
       "      <td>0.615743</td>\n",
       "      <td>0.755237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.649800</td>\n",
       "      <td>0.594063</td>\n",
       "      <td>0.761149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.626400</td>\n",
       "      <td>0.576042</td>\n",
       "      <td>0.770393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.581600</td>\n",
       "      <td>0.569026</td>\n",
       "      <td>0.764287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.615900</td>\n",
       "      <td>0.573750</td>\n",
       "      <td>0.771943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.581000</td>\n",
       "      <td>0.545916</td>\n",
       "      <td>0.777720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.572600</td>\n",
       "      <td>0.555469</td>\n",
       "      <td>0.774355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.546900</td>\n",
       "      <td>0.566975</td>\n",
       "      <td>0.768589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.576400</td>\n",
       "      <td>0.534163</td>\n",
       "      <td>0.787629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.538300</td>\n",
       "      <td>0.531181</td>\n",
       "      <td>0.783903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.573600</td>\n",
       "      <td>0.545711</td>\n",
       "      <td>0.775007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.576600</td>\n",
       "      <td>0.539111</td>\n",
       "      <td>0.774920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.574600</td>\n",
       "      <td>0.529514</td>\n",
       "      <td>0.780702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.534700</td>\n",
       "      <td>0.532600</td>\n",
       "      <td>0.789437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.575400</td>\n",
       "      <td>0.529468</td>\n",
       "      <td>0.785904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.567300</td>\n",
       "      <td>0.517657</td>\n",
       "      <td>0.788006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.554500</td>\n",
       "      <td>0.517621</td>\n",
       "      <td>0.791212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.554900</td>\n",
       "      <td>0.522198</td>\n",
       "      <td>0.786000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.562700</td>\n",
       "      <td>0.511369</td>\n",
       "      <td>0.787819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.566000</td>\n",
       "      <td>0.520732</td>\n",
       "      <td>0.784668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.517400</td>\n",
       "      <td>0.518885</td>\n",
       "      <td>0.789043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.522400</td>\n",
       "      <td>0.512291</td>\n",
       "      <td>0.790158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.565900</td>\n",
       "      <td>0.505604</td>\n",
       "      <td>0.790607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.484900</td>\n",
       "      <td>0.513584</td>\n",
       "      <td>0.792219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.482800</td>\n",
       "      <td>0.525769</td>\n",
       "      <td>0.787073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.523600</td>\n",
       "      <td>0.504671</td>\n",
       "      <td>0.789969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>0.525755</td>\n",
       "      <td>0.784804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.439700</td>\n",
       "      <td>0.523118</td>\n",
       "      <td>0.791695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.415600</td>\n",
       "      <td>0.534453</td>\n",
       "      <td>0.788050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>0.517640</td>\n",
       "      <td>0.789729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>0.510166</td>\n",
       "      <td>0.792049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.473300</td>\n",
       "      <td>0.529264</td>\n",
       "      <td>0.787144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.448700</td>\n",
       "      <td>0.509179</td>\n",
       "      <td>0.792707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.408200</td>\n",
       "      <td>0.530081</td>\n",
       "      <td>0.796495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.455500</td>\n",
       "      <td>0.523639</td>\n",
       "      <td>0.794468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.454300</td>\n",
       "      <td>0.515352</td>\n",
       "      <td>0.793182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.453100</td>\n",
       "      <td>0.507740</td>\n",
       "      <td>0.796248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.457900</td>\n",
       "      <td>0.531490</td>\n",
       "      <td>0.783777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.527749</td>\n",
       "      <td>0.794406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.437300</td>\n",
       "      <td>0.507459</td>\n",
       "      <td>0.795864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.485400</td>\n",
       "      <td>0.499536</td>\n",
       "      <td>0.796336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.416800</td>\n",
       "      <td>0.535978</td>\n",
       "      <td>0.786210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.443800</td>\n",
       "      <td>0.515980</td>\n",
       "      <td>0.795480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.429000</td>\n",
       "      <td>0.514731</td>\n",
       "      <td>0.796647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.423900</td>\n",
       "      <td>0.515830</td>\n",
       "      <td>0.793123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.468900</td>\n",
       "      <td>0.501320</td>\n",
       "      <td>0.798429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.437600</td>\n",
       "      <td>0.516645</td>\n",
       "      <td>0.790491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.408700</td>\n",
       "      <td>0.531788</td>\n",
       "      <td>0.792876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.439600</td>\n",
       "      <td>0.516967</td>\n",
       "      <td>0.797967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.456100</td>\n",
       "      <td>0.505607</td>\n",
       "      <td>0.790757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.440900</td>\n",
       "      <td>0.512239</td>\n",
       "      <td>0.797602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.492100</td>\n",
       "      <td>0.501281</td>\n",
       "      <td>0.795211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.407600</td>\n",
       "      <td>0.509548</td>\n",
       "      <td>0.793504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.414800</td>\n",
       "      <td>0.506581</td>\n",
       "      <td>0.794022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.408500</td>\n",
       "      <td>0.512412</td>\n",
       "      <td>0.797906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.417700</td>\n",
       "      <td>0.508767</td>\n",
       "      <td>0.795156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.525400</td>\n",
       "      <td>0.489506</td>\n",
       "      <td>0.795973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.457700</td>\n",
       "      <td>0.505872</td>\n",
       "      <td>0.792177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.430700</td>\n",
       "      <td>0.512594</td>\n",
       "      <td>0.791405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.339900</td>\n",
       "      <td>0.530359</td>\n",
       "      <td>0.798355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.529613</td>\n",
       "      <td>0.791221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.397400</td>\n",
       "      <td>0.527614</td>\n",
       "      <td>0.794539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.396900</td>\n",
       "      <td>0.518210</td>\n",
       "      <td>0.797894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.331000</td>\n",
       "      <td>0.539828</td>\n",
       "      <td>0.794994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.355400</td>\n",
       "      <td>0.536194</td>\n",
       "      <td>0.789775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.350600</td>\n",
       "      <td>0.533204</td>\n",
       "      <td>0.796439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.539059</td>\n",
       "      <td>0.789617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.379200</td>\n",
       "      <td>0.531859</td>\n",
       "      <td>0.798662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.322800</td>\n",
       "      <td>0.538399</td>\n",
       "      <td>0.797119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.366400</td>\n",
       "      <td>0.543715</td>\n",
       "      <td>0.794638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.412900</td>\n",
       "      <td>0.533921</td>\n",
       "      <td>0.793156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.368600</td>\n",
       "      <td>0.529562</td>\n",
       "      <td>0.800287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.361500</td>\n",
       "      <td>0.534196</td>\n",
       "      <td>0.795870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.379600</td>\n",
       "      <td>0.527069</td>\n",
       "      <td>0.796721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.388300</td>\n",
       "      <td>0.521705</td>\n",
       "      <td>0.797619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.413800</td>\n",
       "      <td>0.534778</td>\n",
       "      <td>0.789206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.366200</td>\n",
       "      <td>0.532925</td>\n",
       "      <td>0.788543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.390900</td>\n",
       "      <td>0.520347</td>\n",
       "      <td>0.799614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.367100</td>\n",
       "      <td>0.521630</td>\n",
       "      <td>0.799334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.339600</td>\n",
       "      <td>0.519879</td>\n",
       "      <td>0.794326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.366700</td>\n",
       "      <td>0.520568</td>\n",
       "      <td>0.797475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.366300</td>\n",
       "      <td>0.528160</td>\n",
       "      <td>0.795829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.376500</td>\n",
       "      <td>0.531498</td>\n",
       "      <td>0.794878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.343600</td>\n",
       "      <td>0.532240</td>\n",
       "      <td>0.795622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.396200</td>\n",
       "      <td>0.531021</td>\n",
       "      <td>0.796214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>0.531587</td>\n",
       "      <td>0.795764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.342200</td>\n",
       "      <td>0.531494</td>\n",
       "      <td>0.797338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.378000</td>\n",
       "      <td>0.532884</td>\n",
       "      <td>0.797582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.533477</td>\n",
       "      <td>0.797595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.416500</td>\n",
       "      <td>0.533105</td>\n",
       "      <td>0.797496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.332000</td>\n",
       "      <td>0.532800</td>\n",
       "      <td>0.797496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-4000 (score: 0.7967213518555497).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='208' max='208' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [208/208 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▅▇▇▇███████████████████████████████████</td></tr><tr><td>eval/loss</td><td>█▆▃▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▂▂▂▁▁▂▁▁▂▁▂▁</td></tr><tr><td>eval/runtime</td><td>▁▂▂▅▃▃▄▄▄▃▃▄▃▄▄▃▃▃▄▃▃▃█▃▃▄▃▃▃▄▃▃▃▃▃▃▅▄▅▃</td></tr><tr><td>eval/samples_per_second</td><td>█▇▇▄▆▆▅▄▅▆▆▅▆▅▅▆▆▆▅▆▆▆▁▆▆▅▆▆▆▅▆▆▆▆▆▆▄▅▄▆</td></tr><tr><td>eval/steps_per_second</td><td>█▇▇▄▆▆▅▄▅▆▆▅▆▅▅▆▆▆▅▆▆▆▁▆▆▅▆▆▆▅▆▆▆▆▆▆▄▅▄▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▃▇███▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▅▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▂▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.79672</td></tr><tr><td>eval/loss</td><td>0.52707</td></tr><tr><td>eval/runtime</td><td>6.0973</td></tr><tr><td>eval/samples_per_second</td><td>579.602</td></tr><tr><td>eval/steps_per_second</td><td>34.114</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>4851</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.332</td></tr><tr><td>train/total_flos</td><td>2965575998095200.0</td></tr><tr><td>train/train_loss</td><td>0.48148</td></tr><tr><td>train/train_runtime</td><td>1125.5655</td></tr><tr><td>train/train_samples_per_second</td><td>73.243</td></tr><tr><td>train/train_steps_per_second</td><td>4.31</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">exalted-sweep-30</strong>: <a href=\"https://wandb.ai/eliottr/uncategorized/runs/amnuaim1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/runs/amnuaim1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_044516-amnuaim1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gx22sq7i with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.062797917397618e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tper_device_train_batch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05409031970679112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.017924287303923557\n",
      "2022-04-28 05:04:36.399039: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.15 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/wandb/run-20220428_050435-gx22sq7i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/eliottr/uncategorized/runs/gx22sq7i\" target=\"_blank\">treasured-sweep-31</a></strong> to <a href=\"https://wandb.ai/eliottr/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1\" target=\"_blank\">https://wandb.ai/eliottr/uncategorized/sweeps/tjsd3ws1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration tweet-sentiment-extraction-26a05391932e4e2c\n",
      "Reusing dataset csv (/home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb70c1187dba4b54a92c08afd4a896af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /.cache/huggingface/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /.cache/huggingface/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /.cache/huggingface/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /.cache/huggingface/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /.cache/huggingface/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-387b76863fd0f88a.arrow\n",
      "Loading cached processed dataset at /home/eliott.remmer/.cache/huggingface/datasets/csv/tweet-sentiment-extraction-26a05391932e4e2c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2e46877fd2f844c8.arrow\n",
      "using `logging_steps` to initialize `eval_steps` to 50\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running training *****\n",
      "  Num examples = 27480\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10305\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='559' max='10305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  559/10305 01:57 < 34:21, 4.73 it/s, Epoch 0.16/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.111200</td>\n",
       "      <td>1.105306</td>\n",
       "      <td>0.268833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.096800</td>\n",
       "      <td>1.090529</td>\n",
       "      <td>0.340385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.095600</td>\n",
       "      <td>1.075498</td>\n",
       "      <td>0.376286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.065400</td>\n",
       "      <td>1.043034</td>\n",
       "      <td>0.411776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.069900</td>\n",
       "      <td>1.019861</td>\n",
       "      <td>0.509984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.015200</td>\n",
       "      <td>0.971733</td>\n",
       "      <td>0.569244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.933900</td>\n",
       "      <td>0.880465</td>\n",
       "      <td>0.637439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.870200</td>\n",
       "      <td>0.822818</td>\n",
       "      <td>0.671032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.816900</td>\n",
       "      <td>0.754193</td>\n",
       "      <td>0.692009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.754500</td>\n",
       "      <td>0.724408</td>\n",
       "      <td>0.685376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.792900</td>\n",
       "      <td>0.663469</td>\n",
       "      <td>0.732066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500\n",
      "Configuration saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/config.json\n",
      "Model weights saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /workspace/models/bert-base-uncased-finetuned-tweets/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, textID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3534\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=training_loop, count=50)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b1f4913228ca8bf1c65a99910ee7505adbfcee7a59c64fb06da56487a51d674"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
